{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find the highest probobility of prediction for branchynet easy data.\n",
    "\n",
    "# Then use this data for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, RationalQuadratic, WhiteKernel, Matern\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#pandas\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import sqrt\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from enum import Enum\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000 where each sample is of size: (28, 28)\n",
      "Number of test samples: 10000 where each sample is of size: (28, 28)\n",
      "('x_train shape:', (60000, 1, 28, 28), 'y_train shape:', (60000,))\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/aakashnain/kmnist-mnist-replacement\n",
    "# Let us define some paths first\n",
    "input_path = \"datasets/data/kmnist/\"\n",
    "\n",
    "# Path to training images and corresponding labels provided as numpy arrays\n",
    "kmnist_train_images_path = input_path+\"kmnist-train-imgs.npz\"\n",
    "kmnist_train_labels_path = input_path+\"kmnist-train-labels.npz\"\n",
    "\n",
    "# Path to the test images and corresponding labels\n",
    "kmnist_test_images_path = input_path+\"kmnist-test-imgs.npz\"\n",
    "kmnist_test_labels_path = input_path+\"kmnist-test-labels.npz\"\n",
    "\n",
    "# Load the training data from the corresponding npz files\n",
    "kmnist_train_images = np.load(kmnist_train_images_path)['arr_0']\n",
    "kmnist_train_labels = np.load(kmnist_train_labels_path)['arr_0']\n",
    "\n",
    "# Load the test data from the corresponding npz files\n",
    "kmnist_test_images = np.load(kmnist_test_images_path)['arr_0']\n",
    "kmnist_test_labels = np.load(kmnist_test_labels_path)['arr_0']\n",
    "\n",
    "print(\"Number of training samples: {} where each sample is of size: {}\".format(\n",
    "    len(kmnist_train_images), kmnist_train_images.shape[1:] ))\n",
    "print(\"Number of test samples: {} where each sample is of size: {}\".format(\n",
    "    len(kmnist_test_images), kmnist_test_images.shape[1:]))\n",
    "\n",
    "X_train = kmnist_train_images.reshape(60000, 1, 28, 28)\n",
    "X_test = kmnist_test_images.reshape(10000, 1, 28, 28)\n",
    "Y_train = kmnist_train_labels\n",
    "Y_test = kmnist_test_labels\n",
    "print(\"x_train shape:\", X_train.shape, \"y_train shape:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-06a3d36f4757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install seaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the unique labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmnist_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peng/cooperating/venv/lib/python2.7/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peng/cooperating/venv/lib/python2.7/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m from .utils import (categorical_order, get_color_cycle, ci_to_errsize, sort_df,\n\u001b[1;32m     16\u001b[0m                     remove_na, locator_to_legend_entries)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name utils"
     ]
    }
   ],
   "source": [
    "#!pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the unique labels\n",
    "labels = np.unique(kmnist_train_labels)\n",
    "\n",
    "# Get the frequency count for each label\n",
    "frequency_count = np.bincount(kmnist_train_labels)\n",
    "\n",
    "# Visualize \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=labels, y=frequency_count);\n",
    "plt.title(\"Distribution of labels in KMNIST training data\", fontsize=16)\n",
    "plt.xlabel(\"Labels\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how the images for different labels look like\n",
    "random_samples = []\n",
    "for i in range(10):\n",
    "    samples = kmnist_train_images[np.where(kmnist_train_labels==i)][:3]\n",
    "    random_samples.append(samples)\n",
    "\n",
    "# Converting list into a numpy array\n",
    "random_samples = np.array(random_samples)\n",
    "\n",
    "# Visualize the samples\n",
    "f, ax = plt.subplots(10,3, figsize=(10,20))\n",
    "for i, j in enumerate(random_samples):\n",
    "    ax[i, 0].imshow(random_samples[i][0,:,:], cmap='gray')\n",
    "    ax[i, 1].imshow(random_samples[i][1,:,:], cmap='gray')\n",
    "    ax[i, 2].imshow(random_samples[i][2,:,:], cmap='gray')\n",
    "    \n",
    "    ax[i,0].set_title(str(i))\n",
    "    ax[i,0].axis('off')\n",
    "    ax[i,0].set_aspect('equal')\n",
    "    \n",
    "    ax[i,1].set_title(str(i))\n",
    "    ax[i,1].axis('off')\n",
    "    ax[i,1].set_aspect('equal')\n",
    "    \n",
    "    ax[i,2].set_title(str(i))\n",
    "    ax[i,2].axis('off')\n",
    "    ax[i,2].set_aspect('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 1, 28, 28), (60000, 1), (10000, 1, 28, 28), (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "# read training and test data from local disk\n",
    "data_dir = \"/home/peng/cooperating/project/data_model1/k_mnist/\"\n",
    "# read training data\n",
    "number_of_samples = 60000\n",
    "\n",
    "y_train_data = np.loadtxt(data_dir + \"y_train.txt\")\n",
    "y_train = y_train_data.reshape(number_of_samples, 1)\n",
    "y_train = pd.DataFrame(data=y_train, index=None, columns=None)\n",
    "\n",
    "# read testing data\n",
    "number_of_samples = 10000\n",
    "#x_test_all = np.loadtxt(data_dir + \"x_data_test.txt\").reshape(number_of_samples, 1, 28, 28)\n",
    "#x_data = [x.flatten() for x in x_data ]\n",
    "#x_test_all = pd.DataFrame(data=x_data, index=None, columns=None)   \n",
    "\n",
    "y_test_data = np.loadtxt(data_dir + \"y_test.txt\")\n",
    "y_test = y_test_data.reshape(number_of_samples, 1)\n",
    "y_test = pd.DataFrame(data=y_test, index=None, columns=None)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi41LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvSM8oowAAEQBJREFUeJzt3X1wVfWZB/Dvk5uQSAALgiG81PBqQWqhzWJrHYTRdsFthU4tymwt3TqNVZlq19nWZf9Y/uiuzFphndluNRVGbFmoO2plO8y2mtqyrpUSkNfSikJYyJKE9/Aa8vLsHznsRM157uXec+85yfP9zGRyc577y3m85su59/7OPT9RVRCRP0VxN0BE8WD4iZxi+ImcYviJnGL4iZxi+ImcYviJnGL4iZxi+ImcKi7kzgZIqZahvJC77BeKrrf/N40rPRVa23eowv7dJ89l1RMl00WcwyVtk0zum1P4RWQugKcApAA8q6rLrfuXoRw3yW257NKl8mdHmPXnx/9HaG3uww/bv/vFzVn1RMm0Wesyvm/WT/tFJAXghwDmAZgKYJGITM329xFRYeXymn8mgHdVdb+qXgKwHsD8aNoionzLJfyjARzq8fPhYNv7iEiNiNSLSH072nLYHRFFKe/v9qtqrapWq2p1CUrzvTsiylAu4W8EMLbHz2OCbUTUB+QS/i0AJonIOBEZAOAeABuiaYuI8i3rqT5V7RCRJQB+ie6pvtWquieyzhxJjbCn8pZ9NHwqDwBmbPpWaG38S7/Pqifq/3Ka51fVjQA2RtQLERUQT+8lcorhJ3KK4SdyiuEncorhJ3KK4SdyqqCf56fenZ4zwayPTHWa9apnjI9vc0UmCsEjP5FTDD+RUww/kVMMP5FTDD+RUww/kVOc6kuAppvt+k9Pf9ysF9e/E1rryqYhcoFHfiKnGH4ipxh+IqcYfiKnGH4ipxh+IqcYfiKnOM9fAEVlZWb9a3M2mfV/3TnLrE84t/2Ke0oEsVeSlk/dYNa1fneU3bjDIz+RUww/kVMMP5FTDD+RUww/kVMMP5FTDD+RUznN84tIA4AzADoBdKhqdRRN9Tc6baJZv6n838365mVTzLp9Ye++q+zJo2b9wt/Y1znA73dF2E3/E8VJPnNU9VgEv4eICohP+4mcyjX8CuBXIrJVRGqiaIiICiPXp/23qGqjiFwL4FUR+aOqvu9E9eAfhRoAKMPAHHdHRFHJ6civqo3B9xYALwOY2ct9alW1WlWrS1Cay+6IKEJZh19EykVk8OXbAD4PgB+zIuojcnnaXwHgZen+WGYxgH9T1f+MpCsiyrusw6+q+wF8IsJe+q2GOweb9W3nq8x614FDEXbzfqmpk816+4hys17027ez33ma5cMPnhpq1q99/KRZT90VPr7zpD3WA071ETnF8BM5xfATOcXwEznF8BM5xfATOcVLd0dAiu2H8asLXjfrq16bY9Yntr91xT1lquPqq8z68ueeMesP/+kes37ijZGhtbGvnTXHtu63Twd/++71Zv2GZ/8ytHZdjX3Z8M7jJ8x6f8AjP5FTDD+RUww/kVMMP5FTDD+RUww/kVMMP5FTnOePgEyzPxb79Y/UmvWNm2dH2M2Vkbd2mvW7frnErL/7xafNeurG8OPLsfvPmWMPd6T787SvDLXnM2tDazcuftAcW7nizTT77vt45CdyiuEncorhJ3KK4SdyiuEncorhJ3KK4SdyivP8EXhv4dVmffela8z6R36z36zndQnuNJfPnvK3+8z65PP2fPlnPv3H0FpFaas59tAF+9LdL4yvM+vjX/tGaG3KCwfNsR1mtX/gkZ/IKYafyCmGn8gphp/IKYafyCmGn8gphp/IqbTz/CKyGsAXALSo6rRg2zAAPwNQBaABwEJVdbvm8ajqI2b9gd/ca9YnN9dH2U6k0i1lPfGv7TUFjhalQmvHy8rMsYe+fb1Zx7ftef5UU/jn/TsON9q/24FMjvzPAZj7gW2PAahT1UkA6oKfiagPSRt+Vd0E4IPLl8wHsCa4vQbAgoj7IqI8y/Y1f4WqXn6u2wSgIqJ+iKhAcn7DT1UVQOgJ4iJSIyL1IlLfjrZcd0dEEck2/M0iUgkAwfeWsDuqaq2qVqtqdUmaCy4SUeFkG/4NABYHtxcDeCWadoioUNKGX0TWAfgdgOtF5LCI3AdgOYDPicg+ALcHPxNRH5J2nl9VF4WUbou4l0QrKi8PrY0qP22OPbVjVNTt9B1d4Vcj6Dp/3hw6fEd7TrsuGX8mp/H9Hc/wI3KK4SdyiuEncorhJ3KK4SdyiuEncoqX7s7Q8a/cGFr7wegV5thHtldF3I0PZS32VODZrotm/eKR8OlZ4pGfyC2Gn8gphp/IKYafyCmGn8gphp/IKYafyCnO82eo9Y6zobWmzkHm2JJ37MtE53UJ7j4s1XjMrJ/oshfS1hJ7+XHveOQncorhJ3KK4SdyiuEncorhJ3KK4SdyiuEncorz/IGiwYPN+vIZL4fW/vHAX5hji1sOZdUT2c512ceuseOOFqiTvolHfiKnGH4ipxh+IqcYfiKnGH4ipxh+IqcYfiKn0s7zi8hqAF8A0KKq04JtywB8E8DlidSlqroxX00WwvnZU8z6Fwf+NrS29PUx5tix+j9Z9eRd1zn7uv3/22mfm1E15HhorTmrjvqXTI78zwGY28v2lao6Pfjq08En8iht+FV1E4ATBeiFiAool9f8S0Rkp4isFpGhkXVERAWRbfh/BGACgOkAjgB4MuyOIlIjIvUiUt+Otix3R0RRyyr8qtqsqp2q2gXgxwBmGvetVdVqVa0uQWm2fRJRxLIKv4hU9vjxSwB2R9MOERVKJlN96wDMBjBcRA4D+HsAs0VkOgAF0ADg/jz2SER5kDb8qrqol82r8tBLrBpvtZ8EpSS8PrCJ14fPBykdYNZHpsLXUgCAcQOtef6SrHrqT3iGH5FTDD+RUww/kVMMP5FTDD+RUww/kVN+Lt1dlDLLU/+sway3a/hC2tfssqecKEua2xTql6/eGlrbMnSeObbz5Mmc9t0X8MhP5BTDT+QUw0/kFMNP5BTDT+QUw0/kFMNP5JSbef7222eY9VXj/tmsdxlXISpq60gztu9KDU1zecbiNH9C7ZdCS52nTptDu1rt8yd+ceZGsz7jqgaz7h2P/EROMfxETjH8RE4x/EROMfxETjH8RE4x/EROuZnnP7DI/mx4ZWqgWf/+sWmhNTnQmFVPSdB1y3Sz/udPbzLr8wbZ67VsufjR0NqawzebY8+02Ss83Vr+O7M+qvhCaK3p7o+ZYyvW2v9deiH8dwOAdtjnfiQBj/xETjH8RE4x/EROMfxETjH8RE4x/EROMfxETqWd5xeRsQCeB1ABQAHUqupTIjIMwM8AVAFoALBQVRN7sfOiU/aSzO912PO2ZzuNOeeS5J4uce6um8z6+hVPmvUxxYPMervac/H/dSF8me2VE18wx95QYi/R3ZHmSgmlEt77z5c+YY791t0Lzbo8ONKsd+7dZ9aTIJMjfweAR1V1KoBPA3hIRKYCeAxAnapOAlAX/ExEfUTa8KvqEVXdFtw+A2AvgNEA5gNYE9xtDYAF+WqSiKJ3Ra/5RaQKwAwAmwFUqOqRoNSE7pcFRNRHZBx+ERkE4EUAj6hqa8+aqiq63w/obVyNiNSLSH072nJqloiik1H4RaQE3cFfq6ovBZubRaQyqFcCaOltrKrWqmq1qlaXGBfBJKLCSht+EREAqwDsVdUVPUobACwObi8G8Er07RFRvmQyR/VZAPcC2CUi24NtSwEsB/CCiNwH4CAAe24kZl3l4UtsA8BAsT/y+/GBh0Jruy9clVVPhXD0E/a/7yNS9rOxugv20uZLfvKgWb/uH+pDay9Nm2OOnbUmfCwApMSe6hsz4ERorV2vNcem0zxruFkf3gem+tKGX1XfACAh5duibYeICoVn+BE5xfATOcXwEznF8BM5xfATOcXwEzmV3M+iRuy6qqNm/byGzWZ2O9YxJLwo9tg4jX98h1mfMnSJWb//1l+b9YrN7WZdO43zK97eY4797wVTzPrjv15v1r/y0++E1sqmnTLHPvqxV836L+61T1U//YxZTgQe+YmcYviJnGL4iZxi+ImcYviJnGL4iZxi+ImccjPPf+eonWZ9RJE9Vz8odTG8aM1lx6zr/HmzPvmRrWb96R/ONutvPrPSrD9w4MuhtaP/UmWOveqofQ7BqJT9uA/ZH167Zpn9eft1pdeb9Xe+P8msT8Rxs54EPPITOcXwEznF8BM5xfATOcXwEznF8BM5xfATOeVmnn94catZH1JUZtbbNfz69d2rlfVN2tFh1ic/sM2s3/6975r1Nx8KXwI8tdI+t+JUl93b4CJ7zYGBLeHnAaT7705Xn/idt8x6X8AjP5FTDD+RUww/kVMMP5FTDD+RUww/kVMMP5FTkm6OWkTGAngeQAUABVCrqk+JyDIA3wRw+YL4S1V1o/W7hsgwvUniWdW76ef2NeB3zFxn1u/cNze01nZrk73zdNf178PnCaTTcdunQmuffMI+h+CJkW+b9a8dnGXWm28+E17sp4/5Zq1Dq57IaCGJTE7y6QDwqKpuE5HBALaKyOUVDVaq6g+ybZSI4pM2/Kp6BMCR4PYZEdkLYHS+GyOi/Lqi1/wiUgVgBoDNwaYlIrJTRFaLyNCQMTUiUi8i9e2wlzgiosLJOPwiMgjAiwAeUdVWAD8CMAHAdHQ/M+j1JG5VrVXValWtLoF9LjYRFU5G4ReREnQHf62qvgQAqtqsqp2q2gXgxwBm5q9NIopa2vCLiABYBWCvqq7osb2yx92+BGB39O0RUb5k8m7/ZwHcC2CXiGwPti0FsEhEpqN7+q8BwP156TAio797yawfePWsWW9tC//Ib9oXM/10WikTxXXhlwbfM+9ac+yEJ//KrI9da//5luoWs+5dJu/2vwGgt3lDc06fiJKNZ/gROcXwEznF8BM5xfATOcXwEznF8BM55ebS3Z37jPWaAdy18xtm/XRreWhtAhqyacm9zuYWsz7xq3adcsMjP5FTDD+RUww/kVMMP5FTDD+RUww/kVMMP5FTaS/dHenORI4CONhj03AAxwrWwJVJam9J7Qtgb9mKsrfrVHVEJncsaPg/tHORelWtjq0BQ1J7S2pfAHvLVly98Wk/kVMMP5FTcYe/Nub9W5LaW1L7AthbtmLpLdbX/EQUn7iP/EQUk1jCLyJzReRPIvKuiDwWRw9hRKRBRHaJyHYRqY+5l9Ui0iIiu3tsGyYir4rIvuB7r8ukxdTbMhFpDB677SJyR0y9jRWR10XkDyKyR0QeDrbH+tgZfcXyuBX8ab+IpAC8A+BzAA4D2AJgkar+oaCNhBCRBgDVqhr7nLCIzAJwFsDzqjot2PZPAE6o6vLgH86hqvq9hPS2DMDZuFduDhaUqey5sjSABQC+jhgfO6OvhYjhcYvjyD8TwLuqul9VLwFYD2B+DH0knqpuAnDiA5vnA1gT3F6D7j+eggvpLRFU9YiqbgtunwFweWXpWB87o69YxBH+0QAO9fj5MJK15LcC+JWIbBWRmrib6UVFsGw6ADQBqIizmV6kXbm5kD6wsnRiHrtsVryOGt/w+7BbVPWTAOYBeCh4eptI2v2aLUnTNRmt3Fwovaws/f/ifOyyXfE6anGEvxHA2B4/jwm2JYKqNgbfWwC8jOStPtx8eZHU4HtiLnSXpJWbe1tZGgl47JK04nUc4d8CYJKIjBORAQDuAbAhhj4+RETKgzdiICLlAD6P5K0+vAHA4uD2YgCvxNjL+yRl5eawlaUR82OXuBWvVbXgXwDuQPc7/u8B+Ls4egjpazyAHcHXnrh7A7AO3U8D29H93sh9AK4BUAdgH4DXAAxLUG8/AbALwE50B60ypt5uQfdT+p0Atgdfd8T92Bl9xfK48Qw/Iqf4hh+RUww/kVMMP5FTDD+RUww/kVMMP5FTDD+RUww/kVP/B3eTFYxv4R+ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:]))) /255.0\n",
    "x_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:]))) /255.0\n",
    "\n",
    "x_train_all = []\n",
    "y_train_all = []\n",
    "\n",
    "x_test_all = []\n",
    "y_test_all = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    x_train_all.append(x_train[Y_train == i])\n",
    "    y_train_all.append(y_train[Y_train == i])\n",
    "    x_test_all.append(x_test[Y_test == i])\n",
    "    y_test_all.append(y_test[Y_test == i])\n",
    "    \n",
    "simplest_index = [579, 274, 547, 824, 273, 760, 569, 86, 505, 610]\n",
    "simplest_index = [396, 909, 471, 937, 799, 437, 645, 847, 772, 227]\n",
    "for i in range(10):\n",
    "    plt.imshow(X_train[Y_train == i][simplest_index[i]].reshape(28, 28))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (4904, 784), (1096, 784))\n",
      "(1, (4703, 784), (1297, 784))\n",
      "(2, (3819, 784), (2181, 784))\n",
      "(3, (4894, 784), (1106, 784))\n",
      "(4, (4321, 784), (1679, 784))\n",
      "(5, (4973, 784), (1027, 784))\n",
      "(6, (3731, 784), (2269, 784))\n",
      "(7, (4845, 784), (1155, 784))\n",
      "(8, (4873, 784), (1127, 784))\n",
      "(9, (5151, 784), (849, 784))\n",
      "((53764, 784), (53764, 784), (6308, 784), (6308, 784), 6308)\n",
      "(0, 4904)\n",
      "(0, 4904)\n",
      "(1, 4703)\n",
      "(1, 4703)\n",
      "(2, 7638)\n",
      "(2, 7638)\n",
      "(3, 4894)\n",
      "(3, 4894)\n",
      "(4, 4321)\n",
      "(4, 4321)\n"
     ]
    }
   ],
   "source": [
    "# training data for autoencoder..\n",
    "x_train_easy = []\n",
    "x_train_hard = []\n",
    "x_test_easy = []\n",
    "x_test_hard = []\n",
    "x_tmptest_easy = []\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# use only half of the output classes to train one autoencoder. \n",
    "# first half : range(int(num_classes/2))\n",
    "# second half : range(int(num_classes/2), num_classes)\n",
    "\n",
    "y_trndata_easy = []\n",
    "y_trndata_hard = []\n",
    "y_testdata_easy = []\n",
    "y_testdata_hard = []\n",
    "\n",
    "#for i in range(int(num_classes/2), num_classes):\n",
    "for i in range(int(num_classes)):\n",
    "\n",
    "    trn_easy = x_train_all[i][y_train_all[i][0]==0]\n",
    "    trn_hard = x_train_all[i][y_train_all[i][0]>0]\n",
    "    \n",
    "    print(i, trn_easy.shape, trn_hard.shape)\n",
    "\n",
    "    test_easy = x_test_all[i][y_test_all[i][0]==0]\n",
    "    test_hard = x_test_all[i][y_test_all[i][0]>0]\n",
    "    test_easy_tmp = x_test_all[i][y_test_all[i][0]==0]\n",
    "    \n",
    "    # there are more easy examples than hard examples\n",
    "    n1 = trn_hard.shape[0]\n",
    "    n2 = trn_easy.shape[0]\n",
    "    #trn_easy = trn_easy[0:n1]\n",
    "    #trn_hard = trn_hard[0:n1]\n",
    "    \n",
    "    while 2*n1 > n2:\n",
    "        trn_easy = np.concatenate((trn_easy, trn_easy), axis=0)\n",
    "        n2 = trn_easy.shape[0]\n",
    "        \n",
    "    trn_hard = np.concatenate((trn_hard[0:n1],  trn_easy[n1:n2]), axis=0)\n",
    "    \n",
    "    \n",
    "    n3 = test_hard.shape[0]\n",
    "    n4 = test_easy.shape[0]\n",
    "    test_hard = np.concatenate((test_hard[0:n3], test_easy[n3:n4]), axis=0)\n",
    "    #test_easy = test_easy[0:n2]\n",
    "    #test_hard = test_hard[0:n2]\n",
    "    #trn_easy = np.concatenate((trn_hard[0:n1], trn_easy), axis=0)\n",
    "    \n",
    "    for j in range(0,n2):\n",
    "        #trn_easy[j] = trn_easy[-1] \n",
    "        trn_easy[j] = x_train_all[i][simplest_index[i]]\n",
    "    #test_easy = np.concatenate((test_hard[0:n3], test_easy), axis=0)  \n",
    "    \n",
    "    for j in range(0,n4):\n",
    "        #test_easy[j] = trn_easy[-1]\n",
    "        test_easy[j] = x_train_all[i][simplest_index[i]]\n",
    "    x_train_easy.append(trn_easy)\n",
    "    x_train_hard.append(trn_hard)\n",
    "    \n",
    "    \n",
    "    # testing data\n",
    "    x_test_easy.append(test_easy)\n",
    "    x_test_hard.append(test_hard)\n",
    "    y_trndata_hard += [i for _ in (y_train_all[i][0]>0) if _ is True]\n",
    "    y_testdata_easy += [i for _ in (y_test_all[i][0]==0) if _ is True]\n",
    "    #y_testdata_easy = [i for _ in (y_test_all[i][0]>0) if _ is True] + y_testdata_easy\n",
    "\n",
    "    #y_testdata_hard += [i for _ in (y_test_all[i][0]>0) if _ is True]\n",
    "\n",
    "x_trndata_easy = np.concatenate(x_train_easy)\n",
    "x_trndata_hard = np.concatenate(x_train_hard)\n",
    "x_testdata_easy = np.concatenate(x_test_easy)\n",
    "x_testdata_hard = np.concatenate(x_test_hard)\n",
    "\n",
    "print (x_trndata_easy.shape, x_trndata_hard.shape, x_testdata_easy.shape, x_testdata_hard.shape, len(y_testdata_easy))\n",
    "\n",
    "for i in range(5):\n",
    "    print (i, x_train_easy[i].shape[0])\n",
    "    print (i, x_train_hard[i].shape[0])\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from branchynet.net import BranchyNet\n",
    "from branchynet.links import *\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from branchynet import utils, visualize\n",
    "from chainer import cuda\n",
    "import dill\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "def measure_performance_LeNet(X, Y):\n",
    "    # load branchynet\n",
    "    branchyNet = None\n",
    "    with open(\"_models/lenet_k_mnist.bn\", \"rb\") as f:\n",
    "        branchyNet = dill.load(f)\n",
    "    #set network to inference mode, this is fob_test_data_yr measuring baseline function. \n",
    "    branchyNet.testing()\n",
    "    branchyNet.verbose = False\n",
    "\n",
    "    #branchyNet.to_cpu()\n",
    "    TEST_BATCHSIZE = 10    \n",
    "    \n",
    "    \n",
    "    branchyNet.to_cpu()\n",
    "    c_baseacc, c_basediff, _, _ = utils.test(branchyNet, X, Y, main=True, batchsize=TEST_BATCHSIZE)\n",
    "    \n",
    "    print(\"LeNet accuracy is \", c_baseacc)\n",
    "    print(\"LeNet time is \", c_basediff)\n",
    "    print(\"\\n\")\n",
    "    return c_baseacc, c_basediff\n",
    "\n",
    "def measure_performance_branchynet(X, Y,threshold=0):\n",
    "    # load branchynet\n",
    "    branchyNet = None\n",
    "    with open(\"_models/lenet_k_mnist.bn\", \"rb\") as f:\n",
    "        branchyNet = dill.load(f)\n",
    "    #set network to inference mode, this is fob_test_data_yr measuring baseline function. \n",
    "    branchyNet.testing()\n",
    "    branchyNet.verbose = False\n",
    "\n",
    "    #branchyNet.to_cpu()\n",
    "    TEST_BATCHSIZE = 10    \n",
    "    thresholds = [0.025+threshold]\n",
    "    #print(decoded_imgs.shape)\n",
    "    \n",
    "    cpu_time_a = (time.time(), psutil.cpu_times())\n",
    "    \n",
    "    branchyNet.to_cpu()\n",
    "    c_ts, c_accs, c_diffs, c_exits  = utils.screen_branchy(branchyNet, X, Y, thresholds,\n",
    "                                                       batchsize=TEST_BATCHSIZE, verbose=False)\n",
    "    c_diffs *= len(Y)\n",
    "\n",
    "    print(\"accuracy is \", c_accs)\n",
    "    print(\"branchyNet time is \", c_diffs)\n",
    "    print(\"the distribution of exit number is \", c_exits)\n",
    "    \n",
    "    cpu_time_b = (time.time(), psutil.cpu_times())\n",
    "    print 'CPU used in %d seconds: %s' % (\n",
    "        cpu_time_b[0] - cpu_time_a[0],\n",
    "        calculate(cpu_time_a[1], cpu_time_b[1])\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    return c_accs, c_diffs\n",
    "# show some easy data and hard data\n",
    "\n",
    "\n",
    "def measure_perf_and_time(X_test_tmp, Y_test_tmp, data_tmp_shape, threshold=0.5):\n",
    "    \n",
    "    b_test_data_x = X_test_tmp.reshape(data_tmp_shape)/255.0\n",
    "\n",
    "\n",
    "    cpu_time_a = (time.time(), psutil.cpu_times())\n",
    "    \n",
    "    start = time.time()\n",
    "    decoded_imgs = autoencoder.predict(b_test_data_x)\n",
    "    decoded_imgs *= 255.0\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    cpu_time_b = (time.time(), psutil.cpu_times())\n",
    "    print 'CPU used in %d seconds: %s' % (\n",
    "        cpu_time_b[0] - cpu_time_a[0],\n",
    "        calculate(cpu_time_a[1], cpu_time_b[1])\n",
    "    )\n",
    "    \n",
    "    acc, c_diff = measure_performance_branchynet(decoded_imgs.reshape(-1, 1, 28,28), Y_test_tmp, threshold=threshold)\n",
    "\n",
    "    print(\"total time(s) is \", end - start + c_diff)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    return acc, end - start + c_diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# show some easy data and hard data\n",
    "\n",
    "def show_all(X_easy, X_hard, length = 100):\n",
    "    n = 40\n",
    "    for shift in range(0, length, 40):\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        for i in range(1, n + 1):\n",
    "            # Display original\n",
    "            ax = plt.subplot(n/20, 20, i)\n",
    "            plt.imshow(X_easy[shift+i-1].reshape(28, 28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(20, 4))\n",
    "        for i in range(1, n + 1):\n",
    "            # Display original\n",
    "            ax = plt.subplot(n/20, 20, i)\n",
    "            plt.imshow(X_hard[shift+i-1].reshape(28, 28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.show()\n",
    "        print(\"---------------------***********************************************--------------\")\n",
    "#Verify by branchynet\n",
    "\n",
    "\n",
    "\n",
    "def calculate(t1, t2):\n",
    "    # from psutil.cpu_percent()\n",
    "    # see: https://github.com/giampaolo/psutil/blob/master/psutil/__init__.py\n",
    "    t1_all = sum(t1)\n",
    "    t1_busy = t1_all - t1.idle\n",
    "    t2_all = sum(t2)\n",
    "    t2_busy = t2_all - t2.idle\n",
    "    if t2_busy <= t1_busy:\n",
    "        return 0.0\n",
    "    busy_delta = t2_busy - t1_busy\n",
    "    all_delta = t2_all - t1_all\n",
    "    busy_perc = (busy_delta / all_delta) * 100\n",
    "    return round(busy_perc, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this experiment for simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((53764, 784), (53764, 784), (6308, 784), (6308, 784))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, valid_X, train_ground,valid_ground = train_test_split(x_trndata_hard,\n",
    "                                                              x_trndata_easy,\n",
    "                                                              test_size=0.2,\n",
    "                                                              random_state=13)\n",
    "train_X, train_ground, valid_X, valid_ground = x_trndata_hard, x_trndata_easy, x_testdata_hard,  x_testdata_easy\n",
    "print(train_X.shape, train_ground.shape, valid_X.shape, valid_ground.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53764 samples, validate on 6308 samples\n",
      "WARNING:tensorflow:From /home/peng/cooperating/venv/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "53764/53764 [==============================] - 7s 130us/sample - loss: 0.3557 - val_loss: 0.2987\n",
      "Epoch 2/100\n",
      "53764/53764 [==============================] - 4s 74us/sample - loss: 0.1809 - val_loss: 0.2401\n",
      "Epoch 3/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1521 - val_loss: 0.2191\n",
      "Epoch 4/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1412 - val_loss: 0.2180\n",
      "Epoch 5/100\n",
      "53764/53764 [==============================] - 5s 101us/sample - loss: 0.1369 - val_loss: 0.2102\n",
      "Epoch 6/100\n",
      "53764/53764 [==============================] - 5s 100us/sample - loss: 0.1314 - val_loss: 0.1996\n",
      "Epoch 7/100\n",
      "53764/53764 [==============================] - 5s 99us/sample - loss: 0.1267 - val_loss: 0.1988\n",
      "Epoch 8/100\n",
      "53764/53764 [==============================] - 5s 94us/sample - loss: 0.1252 - val_loss: 0.1985\n",
      "Epoch 9/100\n",
      "53764/53764 [==============================] - 5s 97us/sample - loss: 0.1234 - val_loss: 0.1950\n",
      "Epoch 10/100\n",
      "53764/53764 [==============================] - 4s 83us/sample - loss: 0.1204 - val_loss: 0.1937\n",
      "Epoch 11/100\n",
      "53764/53764 [==============================] - 4s 74us/sample - loss: 0.1185 - val_loss: 0.1970\n",
      "Epoch 12/100\n",
      "53764/53764 [==============================] - 5s 96us/sample - loss: 0.1174 - val_loss: 0.1938\n",
      "Epoch 13/100\n",
      "53764/53764 [==============================] - 5s 91us/sample - loss: 0.1160 - val_loss: 0.1935\n",
      "Epoch 14/100\n",
      "53764/53764 [==============================] - 5s 88us/sample - loss: 0.1156 - val_loss: 0.1952\n",
      "Epoch 15/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1156 - val_loss: 0.1956\n",
      "Epoch 16/100\n",
      "53764/53764 [==============================] - 4s 79us/sample - loss: 0.1143 - val_loss: 0.1994\n",
      "Epoch 17/100\n",
      "53764/53764 [==============================] - 5s 89us/sample - loss: 0.1133 - val_loss: 0.1978\n",
      "Epoch 18/100\n",
      "53764/53764 [==============================] - 11s 200us/sample - loss: 0.1126 - val_loss: 0.1967\n",
      "Epoch 19/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1118 - val_loss: 0.1967\n",
      "Epoch 20/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1123 - val_loss: 0.1998\n",
      "Epoch 21/100\n",
      "53764/53764 [==============================] - 5s 98us/sample - loss: 0.1133 - val_loss: 0.1962\n",
      "Epoch 22/100\n",
      "53764/53764 [==============================] - 5s 85us/sample - loss: 0.1109 - val_loss: 0.2010\n",
      "Epoch 23/100\n",
      "53764/53764 [==============================] - 4s 79us/sample - loss: 0.1105 - val_loss: 0.1961\n",
      "Epoch 24/100\n",
      "53764/53764 [==============================] - 4s 73us/sample - loss: 0.1097 - val_loss: 0.1996\n",
      "Epoch 25/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1096 - val_loss: 0.1973\n",
      "Epoch 26/100\n",
      "53764/53764 [==============================] - 4s 71us/sample - loss: 0.1087 - val_loss: 0.1977\n",
      "Epoch 27/100\n",
      "53764/53764 [==============================] - 3s 54us/sample - loss: 0.1090 - val_loss: 0.1978\n",
      "Epoch 28/100\n",
      "53764/53764 [==============================] - 3s 60us/sample - loss: 0.1082 - val_loss: 0.2009\n",
      "Epoch 29/100\n",
      "53764/53764 [==============================] - 3s 62us/sample - loss: 0.1079 - val_loss: 0.1981\n",
      "Epoch 30/100\n",
      "53764/53764 [==============================] - 3s 61us/sample - loss: 0.1075 - val_loss: 0.1994\n",
      "Epoch 31/100\n",
      "53764/53764 [==============================] - 4s 70us/sample - loss: 0.1074 - val_loss: 0.2051\n",
      "Epoch 32/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1073 - val_loss: 0.2005\n",
      "Epoch 33/100\n",
      "53764/53764 [==============================] - 5s 100us/sample - loss: 0.1067 - val_loss: 0.2043\n",
      "Epoch 34/100\n",
      "53764/53764 [==============================] - 5s 102us/sample - loss: 0.1067 - val_loss: 0.2022\n",
      "Epoch 35/100\n",
      "53764/53764 [==============================] - 5s 102us/sample - loss: 0.1062 - val_loss: 0.2046\n",
      "Epoch 36/100\n",
      "53764/53764 [==============================] - 5s 101us/sample - loss: 0.1062 - val_loss: 0.2049\n",
      "Epoch 37/100\n",
      "53764/53764 [==============================] - 4s 78us/sample - loss: 0.1074 - val_loss: 0.2040\n",
      "Epoch 38/100\n",
      "53764/53764 [==============================] - 6s 103us/sample - loss: 0.1062 - val_loss: 0.2034\n",
      "Epoch 39/100\n",
      "53764/53764 [==============================] - 6s 105us/sample - loss: 0.1066 - val_loss: 0.2049\n",
      "Epoch 40/100\n",
      "53764/53764 [==============================] - 5s 100us/sample - loss: 0.1056 - val_loss: 0.2092\n",
      "Epoch 41/100\n",
      "53764/53764 [==============================] - 5s 88us/sample - loss: 0.1055 - val_loss: 0.2077\n",
      "Epoch 42/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1051 - val_loss: 0.2087\n",
      "Epoch 43/100\n",
      "53764/53764 [==============================] - 3s 57us/sample - loss: 0.1050 - val_loss: 0.2090\n",
      "Epoch 44/100\n",
      "53764/53764 [==============================] - 4s 77us/sample - loss: 0.1055 - val_loss: 0.2099\n",
      "Epoch 45/100\n",
      "53764/53764 [==============================] - 5s 92us/sample - loss: 0.1046 - val_loss: 0.2091\n",
      "Epoch 46/100\n",
      "53764/53764 [==============================] - 5s 89us/sample - loss: 0.1046 - val_loss: 0.2091\n",
      "Epoch 47/100\n",
      "53764/53764 [==============================] - 5s 98us/sample - loss: 0.1046 - val_loss: 0.2129\n",
      "Epoch 48/100\n",
      "53764/53764 [==============================] - 5s 86us/sample - loss: 0.1052 - val_loss: 0.2094\n",
      "Epoch 49/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1047 - val_loss: 0.2086\n",
      "Epoch 50/100\n",
      "53764/53764 [==============================] - 4s 78us/sample - loss: 0.1044 - val_loss: 0.2094\n",
      "Epoch 51/100\n",
      "53764/53764 [==============================] - 4s 76us/sample - loss: 0.1043 - val_loss: 0.2111\n",
      "Epoch 52/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1050 - val_loss: 0.2083\n",
      "Epoch 53/100\n",
      "53764/53764 [==============================] - 4s 81us/sample - loss: 0.1038 - val_loss: 0.2126\n",
      "Epoch 54/100\n",
      "53764/53764 [==============================] - 4s 83us/sample - loss: 0.1036 - val_loss: 0.2177\n",
      "Epoch 55/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1037 - val_loss: 0.2164\n",
      "Epoch 56/100\n",
      "53764/53764 [==============================] - 5s 91us/sample - loss: 0.1038 - val_loss: 0.2150\n",
      "Epoch 57/100\n",
      "53764/53764 [==============================] - 4s 79us/sample - loss: 0.1040 - val_loss: 0.2161\n",
      "Epoch 58/100\n",
      "53764/53764 [==============================] - 4s 70us/sample - loss: 0.1033 - val_loss: 0.2169\n",
      "Epoch 59/100\n",
      "53764/53764 [==============================] - 4s 73us/sample - loss: 0.1032 - val_loss: 0.2182\n",
      "Epoch 60/100\n",
      "53764/53764 [==============================] - 4s 75us/sample - loss: 0.1033 - val_loss: 0.2146\n",
      "Epoch 61/100\n",
      "53764/53764 [==============================] - 5s 90us/sample - loss: 0.1032 - val_loss: 0.2174\n",
      "Epoch 62/100\n",
      "53764/53764 [==============================] - 5s 86us/sample - loss: 0.1034 - val_loss: 0.2211\n",
      "Epoch 63/100\n",
      "53764/53764 [==============================] - 5s 84us/sample - loss: 0.1033 - val_loss: 0.2189\n",
      "Epoch 64/100\n",
      "53764/53764 [==============================] - 4s 76us/sample - loss: 0.1047 - val_loss: 0.2175\n",
      "Epoch 65/100\n",
      "53764/53764 [==============================] - 4s 75us/sample - loss: 0.1065 - val_loss: 0.2128\n",
      "Epoch 66/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1036 - val_loss: 0.2198\n",
      "Epoch 67/100\n",
      "53764/53764 [==============================] - 4s 77us/sample - loss: 0.1033 - val_loss: 0.2184\n",
      "Epoch 68/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1031 - val_loss: 0.2190\n",
      "Epoch 69/100\n",
      "53764/53764 [==============================] - 4s 70us/sample - loss: 0.1029 - val_loss: 0.2208\n",
      "Epoch 70/100\n",
      "53764/53764 [==============================] - 4s 76us/sample - loss: 0.1030 - val_loss: 0.2232\n",
      "Epoch 71/100\n",
      "53764/53764 [==============================] - 4s 78us/sample - loss: 0.1028 - val_loss: 0.2261\n",
      "Epoch 72/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1028 - val_loss: 0.2202\n",
      "Epoch 73/100\n",
      "53764/53764 [==============================] - 4s 77us/sample - loss: 0.1024 - val_loss: 0.2212\n",
      "Epoch 74/100\n",
      "53764/53764 [==============================] - 4s 72us/sample - loss: 0.1025 - val_loss: 0.2262\n",
      "Epoch 75/100\n",
      "53764/53764 [==============================] - 4s 80us/sample - loss: 0.1027 - val_loss: 0.2282\n",
      "Epoch 76/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1027 - val_loss: 0.2236\n",
      "Epoch 77/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1026 - val_loss: 0.2229\n",
      "Epoch 78/100\n",
      "53764/53764 [==============================] - 5s 96us/sample - loss: 0.1023 - val_loss: 0.2258\n",
      "Epoch 79/100\n",
      "53764/53764 [==============================] - 4s 72us/sample - loss: 0.1027 - val_loss: 0.2225\n",
      "Epoch 80/100\n",
      "53764/53764 [==============================] - 4s 66us/sample - loss: 0.1025 - val_loss: 0.2267\n",
      "Epoch 81/100\n",
      "53764/53764 [==============================] - 5s 84us/sample - loss: 0.1021 - val_loss: 0.2234\n",
      "Epoch 82/100\n",
      "53764/53764 [==============================] - 4s 76us/sample - loss: 0.1020 - val_loss: 0.2270\n",
      "Epoch 83/100\n",
      "53764/53764 [==============================] - 4s 81us/sample - loss: 0.1020 - val_loss: 0.2276\n",
      "Epoch 84/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1019 - val_loss: 0.2281\n",
      "Epoch 85/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1022 - val_loss: 0.2247\n",
      "Epoch 86/100\n",
      "53764/53764 [==============================] - 4s 82us/sample - loss: 0.1019 - val_loss: 0.2247\n",
      "Epoch 87/100\n",
      "53764/53764 [==============================] - 4s 78us/sample - loss: 0.1016 - val_loss: 0.2323\n",
      "Epoch 88/100\n",
      "53764/53764 [==============================] - 5s 91us/sample - loss: 0.1022 - val_loss: 0.2270\n",
      "Epoch 89/100\n",
      "53764/53764 [==============================] - 5s 88us/sample - loss: 0.1017 - val_loss: 0.2332\n",
      "Epoch 90/100\n",
      "53764/53764 [==============================] - 5s 92us/sample - loss: 0.1021 - val_loss: 0.2336\n",
      "Epoch 91/100\n",
      "53764/53764 [==============================] - 4s 81us/sample - loss: 0.1020 - val_loss: 0.2297\n",
      "Epoch 92/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1018 - val_loss: 0.2268\n",
      "Epoch 93/100\n",
      "53764/53764 [==============================] - 5s 92us/sample - loss: 0.1016 - val_loss: 0.2285\n",
      "Epoch 94/100\n",
      "53764/53764 [==============================] - 6s 103us/sample - loss: 0.1019 - val_loss: 0.2296\n",
      "Epoch 95/100\n",
      "53764/53764 [==============================] - 5s 87us/sample - loss: 0.1015 - val_loss: 0.2300\n",
      "Epoch 96/100\n",
      "53764/53764 [==============================] - 5s 102us/sample - loss: 0.1013 - val_loss: 0.2328\n",
      "Epoch 97/100\n",
      "53764/53764 [==============================] - 6s 110us/sample - loss: 0.1013 - val_loss: 0.2302\n",
      "Epoch 98/100\n",
      "53764/53764 [==============================] - 6s 105us/sample - loss: 0.1011 - val_loss: 0.2324\n",
      "Epoch 99/100\n",
      "53764/53764 [==============================] - 5s 92us/sample - loss: 0.1014 - val_loss: 0.2340\n",
      "Epoch 100/100\n",
      "53764/53764 [==============================] - 5s 85us/sample - loss: 0.1011 - val_loss: 0.2337\n"
     ]
    }
   ],
   "source": [
    "#autoencoder \n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "#encoded = layers.Dense(512, activation='relu',activity_regularizer=regularizers.l1(10e-8))(input_img)\n",
    "#encoded = layers.Dense(384, activation='relu',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "#encoded = layers.Dense(256, activation='relu',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "#encoded = layers.Dense(64, activation='linear',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "#decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "encoded = layers.Dense(512, activation='relu',activity_regularizer=regularizers.l1(10e-8))(input_img)\n",
    "encoded = layers.Dense(384, activation='linear',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "encoded = layers.Dense(32, activation='linear',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(train_X, train_ground,\n",
    "                epochs=100,\n",
    "                batch_size=512,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X,valid_ground))\n",
    "\n",
    "data_shape = (X_test.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)  # fontsize of the figure title\n",
    "plt.rc('axes',edgecolor='black')\n",
    "\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 0.5])\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Peng/peng/paper_figures/{}.pdf'.format('Autoencode-loss-kmnist')) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.2463 - acc: 0.7168 - val_loss: 0.3840 - val_acc: 0.6884\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1284 - acc: 0.7507 - val_loss: 0.3822 - val_acc: 0.6901\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1135 - acc: 0.7533 - val_loss: 0.3929 - val_acc: 0.6943\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1065 - acc: 0.7544 - val_loss: 0.4178 - val_acc: 0.6902\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1024 - acc: 0.7550 - val_loss: 0.4276 - val_acc: 0.6926\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0998 - acc: 0.7554 - val_loss: 0.4510 - val_acc: 0.6933\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0984 - acc: 0.7555 - val_loss: 0.4557 - val_acc: 0.6938\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4804 - val_acc: 0.6947\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.5078 - val_acc: 0.6945\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5215 - val_acc: 0.6950\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5266 - val_acc: 0.6956\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5277 - val_acc: 0.6951\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5384 - val_acc: 0.6957\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5481 - val_acc: 0.6959\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5580 - val_acc: 0.6959\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5604 - val_acc: 0.6962\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5681 - val_acc: 0.6960\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5598 - val_acc: 0.6962\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5766 - val_acc: 0.6963\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5745 - val_acc: 0.6963\n",
      "17922/17922 [==============================] - 1s 43us/sample - loss: 1.4792 - acc: 0.6016\n",
      "35842/35842 [==============================] - 1s 38us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.2558 - acc: 0.7253 - val_loss: 0.3705 - val_acc: 0.6907\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1335 - acc: 0.7569 - val_loss: 0.3496 - val_acc: 0.7035\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1172 - acc: 0.7603 - val_loss: 0.3258 - val_acc: 0.7091\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1067 - acc: 0.7622 - val_loss: 0.3347 - val_acc: 0.7099\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1018 - acc: 0.7628 - val_loss: 0.3386 - val_acc: 0.7104\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0993 - acc: 0.7632 - val_loss: 0.3631 - val_acc: 0.7099\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.0969 - acc: 0.7634 - val_loss: 0.3740 - val_acc: 0.7109\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0960 - acc: 0.7635 - val_loss: 0.3921 - val_acc: 0.7113\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3806 - val_acc: 0.7103\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0953 - acc: 0.7635 - val_loss: 0.4048 - val_acc: 0.7120\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4087 - val_acc: 0.7116\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4125 - val_acc: 0.7122\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4207 - val_acc: 0.7124\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4229 - val_acc: 0.7124\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4293 - val_acc: 0.7126\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4280 - val_acc: 0.7126\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 79us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4362 - val_acc: 0.7129\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4409 - val_acc: 0.7129\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4407 - val_acc: 0.7129\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4451 - val_acc: 0.7129\n",
      "17921/17921 [==============================] - 1s 37us/sample - loss: 0.9019 - acc: 0.6581\n",
      "35843/35843 [==============================] - 1s 33us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.2739 - acc: 0.6934 - val_loss: 0.3890 - val_acc: 0.6741\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 79us/sample - loss: 0.1491 - acc: 0.7290 - val_loss: 0.3562 - val_acc: 0.6852\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.1296 - acc: 0.7327 - val_loss: 0.3677 - val_acc: 0.6854\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.1201 - acc: 0.7343 - val_loss: 0.3892 - val_acc: 0.6808\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 79us/sample - loss: 0.1166 - acc: 0.7348 - val_loss: 0.3953 - val_acc: 0.6872\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.1115 - acc: 0.7356 - val_loss: 0.3999 - val_acc: 0.6866\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.1095 - acc: 0.7359 - val_loss: 0.4157 - val_acc: 0.6867\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.1084 - acc: 0.7359 - val_loss: 0.4386 - val_acc: 0.6883\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 79us/sample - loss: 0.1095 - acc: 0.7358 - val_loss: 0.4494 - val_acc: 0.6842\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 79us/sample - loss: 0.1083 - acc: 0.7359 - val_loss: 0.4345 - val_acc: 0.6904\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.1078 - acc: 0.7360 - val_loss: 0.4472 - val_acc: 0.6864\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.1093 - acc: 0.7357 - val_loss: 0.4934 - val_acc: 0.6783\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.1086 - acc: 0.7359 - val_loss: 0.4651 - val_acc: 0.6838\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.1087 - acc: 0.7358 - val_loss: 0.4844 - val_acc: 0.6825\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.1074 - acc: 0.7360 - val_loss: 0.4779 - val_acc: 0.6846\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4985 - val_acc: 0.6879\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.1070 - acc: 0.7360 - val_loss: 0.4910 - val_acc: 0.6872\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.5070 - val_acc: 0.6864\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5116 - val_acc: 0.6869\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5134 - val_acc: 0.6872\n",
      "17921/17921 [==============================] - 1s 38us/sample - loss: 0.9385 - acc: 0.6585\n",
      "35843/35843 [==============================] - 1s 34us/sample - loss: 0.1063 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 6s 164us/sample - loss: 0.2610 - acc: 0.7119 - val_loss: 0.3953 - val_acc: 0.6883\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1304 - acc: 0.7506 - val_loss: 0.3783 - val_acc: 0.6937\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1151 - acc: 0.7534 - val_loss: 0.3832 - val_acc: 0.6948\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1073 - acc: 0.7546 - val_loss: 0.3974 - val_acc: 0.6944\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1029 - acc: 0.7551 - val_loss: 0.4217 - val_acc: 0.6941\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1002 - acc: 0.7554 - val_loss: 0.4231 - val_acc: 0.6959\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0987 - acc: 0.7555 - val_loss: 0.4475 - val_acc: 0.6951\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4704 - val_acc: 0.6952\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4941 - val_acc: 0.6962\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5041 - val_acc: 0.6964\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5163 - val_acc: 0.6964\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5177 - val_acc: 0.6968\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5252 - val_acc: 0.6969\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5342 - val_acc: 0.6968\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5341 - val_acc: 0.6970\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5463 - val_acc: 0.6970\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5428 - val_acc: 0.6968\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5554 - val_acc: 0.6973\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5435 - val_acc: 0.6973\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5535 - val_acc: 0.6973\n",
      "17922/17922 [==============================] - 1s 45us/sample - loss: 1.4045 - acc: 0.6052\n",
      "35842/35842 [==============================] - 2s 45us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 167us/sample - loss: 0.2844 - acc: 0.7144 - val_loss: 0.3705 - val_acc: 0.6889\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1378 - acc: 0.7563 - val_loss: 0.3382 - val_acc: 0.7063\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1178 - acc: 0.7604 - val_loss: 0.3231 - val_acc: 0.7079\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1084 - acc: 0.7620 - val_loss: 0.3375 - val_acc: 0.7101\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1026 - acc: 0.7629 - val_loss: 0.3415 - val_acc: 0.7099\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0994 - acc: 0.7633 - val_loss: 0.3491 - val_acc: 0.7100\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0976 - acc: 0.7634 - val_loss: 0.3612 - val_acc: 0.7122\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3704 - val_acc: 0.7120\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0958 - acc: 0.7635 - val_loss: 0.3793 - val_acc: 0.7116\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0955 - acc: 0.7635 - val_loss: 0.3901 - val_acc: 0.7119\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3990 - val_acc: 0.7127\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4123 - val_acc: 0.7127\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4285 - val_acc: 0.7129\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4253 - val_acc: 0.7129\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4372 - val_acc: 0.7130\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4359 - val_acc: 0.7130\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4393 - val_acc: 0.7130\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4371 - val_acc: 0.7129\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4192 - val_acc: 0.7120\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4446 - val_acc: 0.7128\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.9020 - acc: 0.6584\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 162us/sample - loss: 0.2905 - acc: 0.6884 - val_loss: 0.3769 - val_acc: 0.6753\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1519 - acc: 0.7288 - val_loss: 0.3591 - val_acc: 0.6861\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1319 - acc: 0.7327 - val_loss: 0.3642 - val_acc: 0.6904\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1218 - acc: 0.7343 - val_loss: 0.3671 - val_acc: 0.6883\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1163 - acc: 0.7352 - val_loss: 0.3917 - val_acc: 0.6868\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1128 - acc: 0.7356 - val_loss: 0.4103 - val_acc: 0.6871\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1102 - acc: 0.7359 - val_loss: 0.4217 - val_acc: 0.6865\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1088 - acc: 0.7360 - val_loss: 0.4303 - val_acc: 0.6873\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1079 - acc: 0.7360 - val_loss: 0.4512 - val_acc: 0.6867\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1081 - acc: 0.7360 - val_loss: 0.4417 - val_acc: 0.6876\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4533 - val_acc: 0.6896\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4661 - val_acc: 0.6902\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4853 - val_acc: 0.6896\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4849 - val_acc: 0.6909\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4864 - val_acc: 0.6911\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4921 - val_acc: 0.6912\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4947 - val_acc: 0.6914\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4923 - val_acc: 0.6913\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5004 - val_acc: 0.6917\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4969 - val_acc: 0.6923\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.8995 - acc: 0.6686\n",
      "35843/35843 [==============================] - 2s 43us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 6s 162us/sample - loss: 0.3141 - acc: 0.6908 - val_loss: 0.4044 - val_acc: 0.6799\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1348 - acc: 0.7502 - val_loss: 0.3731 - val_acc: 0.6936\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1180 - acc: 0.7531 - val_loss: 0.3845 - val_acc: 0.6935\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1098 - acc: 0.7544 - val_loss: 0.3999 - val_acc: 0.6940\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1053 - acc: 0.7550 - val_loss: 0.4084 - val_acc: 0.6942\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1017 - acc: 0.7554 - val_loss: 0.4314 - val_acc: 0.6953\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.0998 - acc: 0.7555 - val_loss: 0.4393 - val_acc: 0.6966\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0986 - acc: 0.7556 - val_loss: 0.4535 - val_acc: 0.6977\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4773 - val_acc: 0.6976\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4811 - val_acc: 0.6976\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5000 - val_acc: 0.6991\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5062 - val_acc: 0.6979\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5185 - val_acc: 0.6983\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5271 - val_acc: 0.6989\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5282 - val_acc: 0.6988\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5341 - val_acc: 0.6990\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5478 - val_acc: 0.6990\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5487 - val_acc: 0.6991\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5569 - val_acc: 0.6993\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5633 - val_acc: 0.6992\n",
      "17922/17922 [==============================] - 1s 45us/sample - loss: 1.4194 - acc: 0.6093\n",
      "35842/35842 [==============================] - 2s 45us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 167us/sample - loss: 0.3030 - acc: 0.7084 - val_loss: 0.4037 - val_acc: 0.6905\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1449 - acc: 0.7553 - val_loss: 0.3580 - val_acc: 0.6924\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1255 - acc: 0.7588 - val_loss: 0.3488 - val_acc: 0.7056\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1135 - acc: 0.7614 - val_loss: 0.3306 - val_acc: 0.7106\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1065 - acc: 0.7627 - val_loss: 0.3304 - val_acc: 0.7121\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1022 - acc: 0.7632 - val_loss: 0.3356 - val_acc: 0.7129\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0996 - acc: 0.7634 - val_loss: 0.3430 - val_acc: 0.7127\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0981 - acc: 0.7635 - val_loss: 0.3502 - val_acc: 0.7132\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0971 - acc: 0.7635 - val_loss: 0.3674 - val_acc: 0.7139\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3720 - val_acc: 0.7147\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0961 - acc: 0.7636 - val_loss: 0.3865 - val_acc: 0.7145\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3898 - val_acc: 0.7149\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.4028 - val_acc: 0.7152\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4070 - val_acc: 0.7152\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4164 - val_acc: 0.7153\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4240 - val_acc: 0.7153\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4287 - val_acc: 0.7157\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4271 - val_acc: 0.7156\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4357 - val_acc: 0.7156\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4380 - val_acc: 0.7157\n",
      "17921/17921 [==============================] - 1s 47us/sample - loss: 0.8788 - acc: 0.6613\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 175us/sample - loss: 0.3331 - acc: 0.6733 - val_loss: 0.3842 - val_acc: 0.6765\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1597 - acc: 0.7276 - val_loss: 0.3528 - val_acc: 0.6882\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1365 - acc: 0.7320 - val_loss: 0.3696 - val_acc: 0.6842\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1259 - acc: 0.7339 - val_loss: 0.3723 - val_acc: 0.6842\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1185 - acc: 0.7351 - val_loss: 0.3677 - val_acc: 0.6893\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1143 - acc: 0.7356 - val_loss: 0.3861 - val_acc: 0.6857\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1114 - acc: 0.7359 - val_loss: 0.3998 - val_acc: 0.6868\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1098 - acc: 0.7360 - val_loss: 0.3960 - val_acc: 0.6904\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1089 - acc: 0.7360 - val_loss: 0.4167 - val_acc: 0.6891\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1082 - acc: 0.7361 - val_loss: 0.4290 - val_acc: 0.6905\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1077 - acc: 0.7361 - val_loss: 0.4307 - val_acc: 0.6907\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4410 - val_acc: 0.6914\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4461 - val_acc: 0.6931\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4605 - val_acc: 0.6928\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4633 - val_acc: 0.6935\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4695 - val_acc: 0.6938\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4773 - val_acc: 0.6940\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4828 - val_acc: 0.6941\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4964 - val_acc: 0.6941\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4913 - val_acc: 0.6947\n",
      "17921/17921 [==============================] - 1s 47us/sample - loss: 0.8825 - acc: 0.6738\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 6s 179us/sample - loss: 0.2484 - acc: 0.7167 - val_loss: 0.3870 - val_acc: 0.6872\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1300 - acc: 0.7504 - val_loss: 0.3798 - val_acc: 0.6929\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1144 - acc: 0.7532 - val_loss: 0.3909 - val_acc: 0.6927\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1070 - acc: 0.7544 - val_loss: 0.4001 - val_acc: 0.6927\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1027 - acc: 0.7550 - val_loss: 0.4291 - val_acc: 0.6927\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1005 - acc: 0.7553 - val_loss: 0.4201 - val_acc: 0.6957 - loss: 0.1006 - a\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0985 - acc: 0.7555 - val_loss: 0.4546 - val_acc: 0.6943\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4639 - val_acc: 0.6950\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4825 - val_acc: 0.6957\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5074 - val_acc: 0.6949\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5175 - val_acc: 0.6949\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5262 - val_acc: 0.6952\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5311 - val_acc: 0.6956\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5607 - val_acc: 0.6960\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1019 - acc: 0.7547 - val_loss: 0.4826 - val_acc: 0.6849\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1022 - acc: 0.7546 - val_loss: 0.4550 - val_acc: 0.6911\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 108us/sample - loss: 0.0991 - acc: 0.7552 - val_loss: 0.4759 - val_acc: 0.6915\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0978 - acc: 0.7555 - val_loss: 0.4988 - val_acc: 0.6927\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5296 - val_acc: 0.6930\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5358 - val_acc: 0.6935\n",
      "17922/17922 [==============================] - 1s 45us/sample - loss: 1.3563 - acc: 0.5954\n",
      "35842/35842 [==============================] - 2s 45us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 176us/sample - loss: 0.2573 - acc: 0.7247 - val_loss: 0.3709 - val_acc: 0.6911\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1355 - acc: 0.7566 - val_loss: 0.3269 - val_acc: 0.7054\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1173 - acc: 0.7603 - val_loss: 0.3249 - val_acc: 0.7093\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1085 - acc: 0.7619 - val_loss: 0.3370 - val_acc: 0.7081\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1032 - acc: 0.7627 - val_loss: 0.3405 - val_acc: 0.7097\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0997 - acc: 0.7632 - val_loss: 0.3490 - val_acc: 0.7113\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0980 - acc: 0.7633 - val_loss: 0.3701 - val_acc: 0.7103\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.0968 - acc: 0.7635 - val_loss: 0.3704 - val_acc: 0.7103\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3908 - val_acc: 0.7113\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0954 - acc: 0.7635 - val_loss: 0.3937 - val_acc: 0.7119\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4069 - val_acc: 0.7121\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4124 - val_acc: 0.7120\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4197 - val_acc: 0.7123\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4242 - val_acc: 0.7123\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4288 - val_acc: 0.7125\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4335 - val_acc: 0.7125\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4395 - val_acc: 0.7125\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4439 - val_acc: 0.7127\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4404 - val_acc: 0.7126\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4473 - val_acc: 0.7128\n",
      "17921/17921 [==============================] - ETA: 0s - loss: 0.9373 - acc: 0.653 - 1s 46us/sample - loss: 0.9019 - acc: 0.6584\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 171us/sample - loss: 0.2803 - acc: 0.6927 - val_loss: 0.3708 - val_acc: 0.6796\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1517 - acc: 0.7287 - val_loss: 0.3524 - val_acc: 0.6887\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1315 - acc: 0.7324 - val_loss: 0.3585 - val_acc: 0.6849s: 0.13\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1214 - acc: 0.7341 - val_loss: 0.3701 - val_acc: 0.6863\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1156 - acc: 0.7350 - val_loss: 0.3853 - val_acc: 0.6866\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1117 - acc: 0.7356 - val_loss: 0.4139 - val_acc: 0.6834\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1097 - acc: 0.7358 - val_loss: 0.4133 - val_acc: 0.6871\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1088 - acc: 0.7359 - val_loss: 0.4386 - val_acc: 0.6824\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1077 - acc: 0.7360 - val_loss: 0.4347 - val_acc: 0.6864\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4620 - val_acc: 0.6859\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4502 - val_acc: 0.6887\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4652 - val_acc: 0.6875\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1076 - acc: 0.7360 - val_loss: 0.4640 - val_acc: 0.6815\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1109 - acc: 0.7353 - val_loss: 0.4165 - val_acc: 0.6834\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1134 - acc: 0.7347 - val_loss: 0.4580 - val_acc: 0.6799\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1086 - acc: 0.7358 - val_loss: 0.4815 - val_acc: 0.6821\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1077 - acc: 0.7359 - val_loss: 0.4985 - val_acc: 0.6800\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.5009 - val_acc: 0.6836\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5012 - val_acc: 0.6847\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5122 - val_acc: 0.6846\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.9687 - acc: 0.6510\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 5s 143us/sample - loss: 0.2713 - acc: 0.7087 - val_loss: 0.3968 - val_acc: 0.6864\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 74us/sample - loss: 0.1337 - acc: 0.7499 - val_loss: 0.3829 - val_acc: 0.6910\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 72us/sample - loss: 0.1182 - acc: 0.7526 - val_loss: 0.3806 - val_acc: 0.6932\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 73us/sample - loss: 0.1087 - acc: 0.7543 - val_loss: 0.3857 - val_acc: 0.6935\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 76us/sample - loss: 0.1039 - acc: 0.7550 - val_loss: 0.4139 - val_acc: 0.6929\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 76us/sample - loss: 0.1009 - acc: 0.7553 - val_loss: 0.4348 - val_acc: 0.6935\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 74us/sample - loss: 0.0991 - acc: 0.7555 - val_loss: 0.4473 - val_acc: 0.6931\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 77us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.4700 - val_acc: 0.6942\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 73us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4901 - val_acc: 0.6928\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 73us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4877 - val_acc: 0.6942\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 73us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5095 - val_acc: 0.6945\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5180 - val_acc: 0.6947\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 74us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5231 - val_acc: 0.6946\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 76us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5296 - val_acc: 0.6948\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 77us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5376 - val_acc: 0.6953\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 77us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5412 - val_acc: 0.6951\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5515 - val_acc: 0.6952\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5549 - val_acc: 0.6951\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 76us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5509 - val_acc: 0.6952\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 73us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5586 - val_acc: 0.6951\n",
      "17922/17922 [==============================] - 1s 34us/sample - loss: 1.4095 - acc: 0.5984\n",
      "35842/35842 [==============================] - 1s 36us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 140us/sample - loss: 0.2739 - acc: 0.7188 - val_loss: 0.3659 - val_acc: 0.6898\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1397 - acc: 0.7560 - val_loss: 0.3512 - val_acc: 0.7011\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1201 - acc: 0.7599 - val_loss: 0.3309 - val_acc: 0.7092\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1094 - acc: 0.7618 - val_loss: 0.3237 - val_acc: 0.7082\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1045 - acc: 0.7626 - val_loss: 0.3498 - val_acc: 0.7109\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1002 - acc: 0.7632 - val_loss: 0.3555 - val_acc: 0.7102\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0981 - acc: 0.7634 - val_loss: 0.3675 - val_acc: 0.7060\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1039 - acc: 0.7624 - val_loss: 0.3623 - val_acc: 0.7107\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0973 - acc: 0.7634 - val_loss: 0.3876 - val_acc: 0.7107\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0962 - acc: 0.7635 - val_loss: 0.3919 - val_acc: 0.7117\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.3968 - val_acc: 0.7112\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0955 - acc: 0.7635 - val_loss: 0.4100 - val_acc: 0.7117\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4131 - val_acc: 0.7122\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4165 - val_acc: 0.7121\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4262 - val_acc: 0.7123\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5116 - val_acc: 0.6911\n",
      "17921/17921 [==============================] - 1s 40us/sample - loss: 0.9429 - acc: 0.6643\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.1063 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 5s 141us/sample - loss: 0.2967 - acc: 0.6995 - val_loss: 0.4160 - val_acc: 0.6877\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.1372 - acc: 0.7496 - val_loss: 0.3769 - val_acc: 0.6918\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1192 - acc: 0.7528 - val_loss: 0.3770 - val_acc: 0.6940\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1101 - acc: 0.7543 - val_loss: 0.3917 - val_acc: 0.6936\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1048 - acc: 0.7550 - val_loss: 0.3956 - val_acc: 0.6934\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1016 - acc: 0.7554 - val_loss: 0.4086 - val_acc: 0.6965\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0998 - acc: 0.7555 - val_loss: 0.4377 - val_acc: 0.6955\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4535 - val_acc: 0.6958\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4670 - val_acc: 0.6969\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4784 - val_acc: 0.6965\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4907 - val_acc: 0.6976\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5073 - val_acc: 0.6977\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5138 - val_acc: 0.6978\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5175 - val_acc: 0.6978\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5248 - val_acc: 0.6979\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5396 - val_acc: 0.6985\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5237 - val_acc: 0.6976\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5394 - val_acc: 0.6976\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5520 - val_acc: 0.6978\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5558 - val_acc: 0.6982\n",
      "17922/17922 [==============================] - 1s 37us/sample - loss: 1.4013 - acc: 0.6055\n",
      "35842/35842 [==============================] - 1s 37us/sample - loss: 0.0966 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 146us/sample - loss: 0.2943 - acc: 0.7110 - val_loss: 0.3903 - val_acc: 0.6874\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1427 - acc: 0.7555 - val_loss: 0.3684 - val_acc: 0.6966\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1239 - acc: 0.7590 - val_loss: 0.3465 - val_acc: 0.7029\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1133 - acc: 0.7613 - val_loss: 0.3374 - val_acc: 0.7100\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1062 - acc: 0.7626 - val_loss: 0.3359 - val_acc: 0.7111\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1019 - acc: 0.7632 - val_loss: 0.3350 - val_acc: 0.7118\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0994 - acc: 0.7634 - val_loss: 0.3527 - val_acc: 0.7123\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0979 - acc: 0.7635 - val_loss: 0.3632 - val_acc: 0.7134\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0969 - acc: 0.7635 - val_loss: 0.3737 - val_acc: 0.7137\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0963 - acc: 0.7635 - val_loss: 0.3839 - val_acc: 0.7137\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3940 - val_acc: 0.7138\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.4044 - val_acc: 0.7141\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4122 - val_acc: 0.7142\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4235 - val_acc: 0.7144\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4230 - val_acc: 0.7142\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4351 - val_acc: 0.7146\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4434 - val_acc: 0.7146\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4472 - val_acc: 0.7145\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4537 - val_acc: 0.7146\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4555 - val_acc: 0.7146\n",
      "17921/17921 [==============================] - 1s 41us/sample - loss: 0.9129 - acc: 0.6609\n",
      "35843/35843 [==============================] - 1s 39us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 143us/sample - loss: 0.3315 - acc: 0.6730 - val_loss: 0.3872 - val_acc: 0.6778\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1586 - acc: 0.7277 - val_loss: 0.3719 - val_acc: 0.6823\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1357 - acc: 0.7322 - val_loss: 0.3555 - val_acc: 0.6864\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1243 - acc: 0.7341 - val_loss: 0.3713 - val_acc: 0.6872\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1177 - acc: 0.7352 - val_loss: 0.3630 - val_acc: 0.6932\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1157 - acc: 0.7353 - val_loss: 0.3735 - val_acc: 0.6876\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1132 - acc: 0.7357 - val_loss: 0.3924 - val_acc: 0.6897\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1102 - acc: 0.7359 - val_loss: 0.4101 - val_acc: 0.6899\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1090 - acc: 0.7360 - val_loss: 0.4170 - val_acc: 0.6922\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1083 - acc: 0.7360 - val_loss: 0.4228 - val_acc: 0.6909\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1077 - acc: 0.7361 - val_loss: 0.4309 - val_acc: 0.6908\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4478 - val_acc: 0.6905\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4544 - val_acc: 0.6918\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4565 - val_acc: 0.6911\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4685 - val_acc: 0.6914\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4789 - val_acc: 0.6920\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4755 - val_acc: 0.6916\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4810 - val_acc: 0.6919\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4780 - val_acc: 0.6923\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4911 - val_acc: 0.6922\n",
      "17921/17921 [==============================] - 1s 41us/sample - loss: 0.9051 - acc: 0.6684\n",
      "35843/35843 [==============================] - 1s 40us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 6s 154us/sample - loss: 0.2502 - acc: 0.7163 - val_loss: 0.3879 - val_acc: 0.6867\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1308 - acc: 0.7501 - val_loss: 0.3790 - val_acc: 0.6930\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1156 - acc: 0.7528 - val_loss: 0.3883 - val_acc: 0.6948\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1089 - acc: 0.7540 - val_loss: 0.3920 - val_acc: 0.6926\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.1032 - acc: 0.7549 - val_loss: 0.4262 - val_acc: 0.6916\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1007 - acc: 0.7553 - val_loss: 0.4442 - val_acc: 0.6918\n",
      "Epoch 7/20\n",
      "30208/35842 [========================>.....] - ETA: 0s - loss: 0.0989 - acc: 0.7555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0960 - acc: 0.7635 - val_loss: 0.3884 - val_acc: 0.7111\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0956 - acc: 0.7635 - val_loss: 0.4014 - val_acc: 0.7117\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4082 - val_acc: 0.7119\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4224 - val_acc: 0.7119\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4245 - val_acc: 0.7121\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4299 - val_acc: 0.7123\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4344 - val_acc: 0.7124\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4360 - val_acc: 0.7125\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4461 - val_acc: 0.7125\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4438 - val_acc: 0.7125\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4462 - val_acc: 0.7127\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4437 - val_acc: 0.7126\n",
      "17921/17921 [==============================] - 1s 39us/sample - loss: 0.8830 - acc: 0.6576\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 148us/sample - loss: 0.2788 - acc: 0.6929 - val_loss: 0.3922 - val_acc: 0.6731\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1527 - acc: 0.7282 - val_loss: 0.3667 - val_acc: 0.6828\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1324 - acc: 0.7322 - val_loss: 0.3762 - val_acc: 0.6795\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1242 - acc: 0.7335 - val_loss: 0.3706 - val_acc: 0.6898\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1185 - acc: 0.7346 - val_loss: 0.3854 - val_acc: 0.6867\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1130 - acc: 0.7354 - val_loss: 0.3992 - val_acc: 0.6878\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1103 - acc: 0.7358 - val_loss: 0.4178 - val_acc: 0.6876\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1090 - acc: 0.7359 - val_loss: 0.4372 - val_acc: 0.6848\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4338 - val_acc: 0.6883\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1076 - acc: 0.7360 - val_loss: 0.4439 - val_acc: 0.6881\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4657 - val_acc: 0.6873\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4780 - val_acc: 0.6878\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4799 - val_acc: 0.6887\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4911 - val_acc: 0.6891\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4937 - val_acc: 0.6890\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4930 - val_acc: 0.6897\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5006 - val_acc: 0.6897\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4989 - val_acc: 0.6901\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5010 - val_acc: 0.6897\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5064 - val_acc: 0.6897\n",
      "17921/17921 [==============================] - 1s 40us/sample - loss: 0.9333 - acc: 0.6629\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.1063 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 5s 148us/sample - loss: 0.2771 - acc: 0.7057 - val_loss: 0.4013 - val_acc: 0.6871\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1351 - acc: 0.7498 - val_loss: 0.3767 - val_acc: 0.6925\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.1180 - acc: 0.7527 - val_loss: 0.3767 - val_acc: 0.6913\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1102 - acc: 0.7540 - val_loss: 0.3947 - val_acc: 0.6946\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.1052 - acc: 0.7548 - val_loss: 0.4121 - val_acc: 0.6940\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1020 - acc: 0.7552 - val_loss: 0.4299 - val_acc: 0.6948\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1003 - acc: 0.7554 - val_loss: 0.4490 - val_acc: 0.6939\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0987 - acc: 0.7556 - val_loss: 0.4640 - val_acc: 0.6947\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4772 - val_acc: 0.6960\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.5009 - val_acc: 0.6955\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4967 - val_acc: 0.6955\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5085 - val_acc: 0.6958\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5188 - val_acc: 0.6962\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5281 - val_acc: 0.6963\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5329 - val_acc: 0.6963\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5423 - val_acc: 0.6967\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5493 - val_acc: 0.6963\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5564 - val_acc: 0.6966\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5564 - val_acc: 0.6969\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5662 - val_acc: 0.6969\n",
      "17922/17922 [==============================] - 1s 41us/sample - loss: 1.4201 - acc: 0.6042\n",
      "35842/35842 [==============================] - 1s 41us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 150us/sample - loss: 0.2811 - acc: 0.7169 - val_loss: 0.3862 - val_acc: 0.6884\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1415 - acc: 0.7554 - val_loss: 0.3462 - val_acc: 0.6984\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.1212 - acc: 0.7597 - val_loss: 0.3272 - val_acc: 0.7088\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1110 - acc: 0.7616 - val_loss: 0.3272 - val_acc: 0.7098\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1049 - acc: 0.7626 - val_loss: 0.3460 - val_acc: 0.7107\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1008 - acc: 0.7631 - val_loss: 0.3482 - val_acc: 0.7110\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0984 - acc: 0.7634 - val_loss: 0.3594 - val_acc: 0.7105\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0971 - acc: 0.7635 - val_loss: 0.3793 - val_acc: 0.7110\n",
      "Epoch 9/20\n",
      "23296/35843 [==================>...........] - ETA: 0s - loss: 0.0964 - acc: 0.7632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4799 - val_acc: 0.6867\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4854 - val_acc: 0.6874\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4944 - val_acc: 0.6876\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.5028 - val_acc: 0.6869\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5057 - val_acc: 0.6874\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5095 - val_acc: 0.6865\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5171 - val_acc: 0.6873\n",
      "17921/17921 [==============================] - 1s 38us/sample - loss: 0.9506 - acc: 0.6577\n",
      "35843/35843 [==============================] - 2s 42us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 5s 152us/sample - loss: 0.3202 - acc: 0.6890 - val_loss: 0.4137 - val_acc: 0.6834\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1388 - acc: 0.7493 - val_loss: 0.3890 - val_acc: 0.6919\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.1209 - acc: 0.7525 - val_loss: 0.3895 - val_acc: 0.6924\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1119 - acc: 0.7539 - val_loss: 0.3913 - val_acc: 0.6952\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.1063 - acc: 0.7548 - val_loss: 0.4134 - val_acc: 0.6941\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1031 - acc: 0.7552 - val_loss: 0.4379 - val_acc: 0.6945\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1009 - acc: 0.7554 - val_loss: 0.4456 - val_acc: 0.6960\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0999 - acc: 0.7555 - val_loss: 0.4625 - val_acc: 0.6946\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0984 - acc: 0.7556 - val_loss: 0.4789 - val_acc: 0.6953\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4979 - val_acc: 0.6961\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.5017 - val_acc: 0.6965\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5221 - val_acc: 0.6964\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5295 - val_acc: 0.6969\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5439 - val_acc: 0.6968\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5483 - val_acc: 0.6968\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5569 - val_acc: 0.6973\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5653 - val_acc: 0.6972\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5722 - val_acc: 0.6972\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5673 - val_acc: 0.6976\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5785 - val_acc: 0.6971\n",
      "17922/17922 [==============================] - 1s 41us/sample - loss: 1.5029 - acc: 0.6038\n",
      "35842/35842 [==============================] - 1s 39us/sample - loss: 0.0966 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 5s 151us/sample - loss: 0.3129 - acc: 0.7047 - val_loss: 0.3925 - val_acc: 0.6881\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1491 - acc: 0.7548 - val_loss: 0.3525 - val_acc: 0.6978\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1255 - acc: 0.7593 - val_loss: 0.3308 - val_acc: 0.7076\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1136 - acc: 0.7615 - val_loss: 0.3326 - val_acc: 0.7105\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1072 - acc: 0.7625 - val_loss: 0.3554 - val_acc: 0.7116\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1040 - acc: 0.7630 - val_loss: 0.3400 - val_acc: 0.7108\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1004 - acc: 0.7633 - val_loss: 0.3515 - val_acc: 0.7114\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0985 - acc: 0.7634 - val_loss: 0.3601 - val_acc: 0.7117\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 74us/sample - loss: 0.0974 - acc: 0.7635 - val_loss: 0.3751 - val_acc: 0.7115\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 71us/sample - loss: 0.0979 - acc: 0.7634 - val_loss: 0.3886 - val_acc: 0.7110\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 71us/sample - loss: 0.0965 - acc: 0.7635 - val_loss: 0.3890 - val_acc: 0.7113\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 76us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3938 - val_acc: 0.7119\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.4067 - val_acc: 0.7122\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4107 - val_acc: 0.7121\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4196 - val_acc: 0.7125\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4239 - val_acc: 0.7126\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4329 - val_acc: 0.7125\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4313 - val_acc: 0.7127\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4391 - val_acc: 0.7128\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4409 - val_acc: 0.7129\n",
      "17921/17921 [==============================] - 1s 40us/sample - loss: 0.8682 - acc: 0.6583\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1014 - acc: 0.7552 - val_loss: 0.4324 - val_acc: 0.6932\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0995 - acc: 0.7554 - val_loss: 0.4510 - val_acc: 0.6926\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0982 - acc: 0.7555 - val_loss: 0.4603 - val_acc: 0.6948\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4916 - val_acc: 0.6945\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.5014 - val_acc: 0.6947\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5162 - val_acc: 0.6941\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5232 - val_acc: 0.6951\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5348 - val_acc: 0.6953\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4956 - val_acc: 0.6937\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5074 - val_acc: 0.6938\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5236 - val_acc: 0.6903\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1020 - acc: 0.7546 - val_loss: 0.4579 - val_acc: 0.6891\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1017 - acc: 0.7547 - val_loss: 0.4675 - val_acc: 0.6918\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0982 - acc: 0.7554 - val_loss: 0.5195 - val_acc: 0.6926\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0974 - acc: 0.7555 - val_loss: 0.5184 - val_acc: 0.6929\n",
      "17922/17922 [==============================] - 1s 39us/sample - loss: 1.3119 - acc: 0.5951\n",
      "35842/35842 [==============================] - 1s 40us/sample - loss: 0.0971 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 167us/sample - loss: 0.2702 - acc: 0.7219 - val_loss: 0.3826 - val_acc: 0.6884\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1394 - acc: 0.7556 - val_loss: 0.3402 - val_acc: 0.7035\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1189 - acc: 0.7599 - val_loss: 0.3318 - val_acc: 0.7070\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1096 - acc: 0.7616 - val_loss: 0.3387 - val_acc: 0.7090\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1035 - acc: 0.7626 - val_loss: 0.3430 - val_acc: 0.7075\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1003 - acc: 0.7631 - val_loss: 0.3598 - val_acc: 0.7101\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0981 - acc: 0.7633 - val_loss: 0.3626 - val_acc: 0.7094\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0968 - acc: 0.7635 - val_loss: 0.3780 - val_acc: 0.7105\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0960 - acc: 0.7635 - val_loss: 0.3905 - val_acc: 0.7106\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.4088 - val_acc: 0.7109\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0972 - acc: 0.7633 - val_loss: 0.3940 - val_acc: 0.7053\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0993 - acc: 0.7629 - val_loss: 0.3807 - val_acc: 0.7051\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0972 - acc: 0.7633 - val_loss: 0.4034 - val_acc: 0.7080\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0972 - acc: 0.7633 - val_loss: 0.4171 - val_acc: 0.7090\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0955 - acc: 0.7635 - val_loss: 0.4246 - val_acc: 0.7088\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4274 - val_acc: 0.7101\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4353 - val_acc: 0.7106\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4383 - val_acc: 0.7107\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4418 - val_acc: 0.7107\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4465 - val_acc: 0.7109\n",
      "17921/17921 [==============================] - 1s 40us/sample - loss: 0.9159 - acc: 0.6560\n",
      "35843/35843 [==============================] - 1s 40us/sample - loss: 0.0948 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 161us/sample - loss: 0.2853 - acc: 0.6911 - val_loss: 0.3795 - val_acc: 0.6815\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1549 - acc: 0.7276 - val_loss: 0.3812 - val_acc: 0.6811\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1344 - acc: 0.7317 - val_loss: 0.3767 - val_acc: 0.6804\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1239 - acc: 0.7336 - val_loss: 0.3828 - val_acc: 0.6837\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1176 - acc: 0.7347 - val_loss: 0.3910 - val_acc: 0.6864\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1143 - acc: 0.7352 - val_loss: 0.4060 - val_acc: 0.6856\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1113 - acc: 0.7356 - val_loss: 0.4133 - val_acc: 0.6869\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1094 - acc: 0.7358 - val_loss: 0.4288 - val_acc: 0.6858\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1087 - acc: 0.7359 - val_loss: 0.4538 - val_acc: 0.6829\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 79us/sample - loss: 0.1078 - acc: 0.7360 - val_loss: 0.4638 - val_acc: 0.6847\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1075 - acc: 0.7360 - val_loss: 0.4496 - val_acc: 0.6875\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4613 - val_acc: 0.6870\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4828 - val_acc: 0.6869\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4874 - val_acc: 0.6875\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4938 - val_acc: 0.6880\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4984 - val_acc: 0.6872\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5045 - val_acc: 0.6876\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5058 - val_acc: 0.6878\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5055 - val_acc: 0.6883\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1166 - acc: 0.7340 - val_loss: 0.4459 - val_acc: 0.6860\n",
      "17921/17921 [==============================] - 1s 40us/sample - loss: 0.7513 - acc: 0.6643\n",
      "35843/35843 [==============================] - 1s 42us/sample - loss: 0.1190 - acc: 0.7337\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 6s 160us/sample - loss: 0.2800 - acc: 0.7066 - val_loss: 0.4012 - val_acc: 0.6877\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.1377 - acc: 0.7492 - val_loss: 0.3723 - val_acc: 0.6924\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1206 - acc: 0.7522 - val_loss: 0.3844 - val_acc: 0.6902\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.1128 - acc: 0.7535 - val_loss: 0.3900 - val_acc: 0.6918\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.1064 - acc: 0.7546 - val_loss: 0.4012 - val_acc: 0.6930\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.1030 - acc: 0.7551 - val_loss: 0.4075 - val_acc: 0.6939\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.1006 - acc: 0.7554 - val_loss: 0.4200 - val_acc: 0.6934\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0994 - acc: 0.7555 - val_loss: 0.4443 - val_acc: 0.6939\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 79us/sample - loss: 0.0989 - acc: 0.7555 - val_loss: 0.4515 - val_acc: 0.6949\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4634 - val_acc: 0.6948\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4888 - val_acc: 0.6944\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4912 - val_acc: 0.6960\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5064 - val_acc: 0.6958\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5140 - val_acc: 0.6958\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5284 - val_acc: 0.6963\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5325 - val_acc: 0.6965\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5388 - val_acc: 0.6963\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5489 - val_acc: 0.6966\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5487 - val_acc: 0.6968\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5517 - val_acc: 0.6967\n",
      "17922/17922 [==============================] - 1s 40us/sample - loss: 1.3596 - acc: 0.6046\n",
      "35842/35842 [==============================] - 1s 41us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 156us/sample - loss: 0.2936 - acc: 0.7123 - val_loss: 0.3867 - val_acc: 0.6840\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1465 - acc: 0.7545 - val_loss: 0.3423 - val_acc: 0.6978\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1235 - acc: 0.7592 - val_loss: 0.3257 - val_acc: 0.7081\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1125 - acc: 0.7613 - val_loss: 0.3208 - val_acc: 0.7099\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1060 - acc: 0.7624 - val_loss: 0.3261 - val_acc: 0.7101\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1019 - acc: 0.7630 - val_loss: 0.3396 - val_acc: 0.7100\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.0993 - acc: 0.7633 - val_loss: 0.3468 - val_acc: 0.7106\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0976 - acc: 0.7634 - val_loss: 0.3569 - val_acc: 0.7118\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3647 - val_acc: 0.7117\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0961 - acc: 0.7635 - val_loss: 0.3741 - val_acc: 0.7112\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.3891 - val_acc: 0.7121\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.3906 - val_acc: 0.7117\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0954 - acc: 0.7635 - val_loss: 0.3893 - val_acc: 0.7119\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4058 - val_acc: 0.7122\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4116 - val_acc: 0.7123\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4204 - val_acc: 0.7124\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4258 - val_acc: 0.7125\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4277 - val_acc: 0.7126\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4353 - val_acc: 0.7126\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4425 - val_acc: 0.7128\n",
      "17921/17921 [==============================] - 1s 43us/sample - loss: 0.9020 - acc: 0.6569\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 170us/sample - loss: 0.3137 - acc: 0.6805 - val_loss: 0.3784 - val_acc: 0.6836\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.1649 - acc: 0.7260 - val_loss: 0.3767 - val_acc: 0.6820\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1415 - acc: 0.7307 - val_loss: 0.3746 - val_acc: 0.6855\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1288 - acc: 0.7331 - val_loss: 0.3755 - val_acc: 0.6841\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1214 - acc: 0.7344 - val_loss: 0.4080 - val_acc: 0.6834\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1177 - acc: 0.7349 - val_loss: 0.3847 - val_acc: 0.6871\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1136 - acc: 0.7355 - val_loss: 0.4094 - val_acc: 0.6856\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1111 - acc: 0.7357 - val_loss: 0.4291 - val_acc: 0.6898\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1124 - acc: 0.7354 - val_loss: 0.4252 - val_acc: 0.6844\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1094 - acc: 0.7358 - val_loss: 0.4671 - val_acc: 0.6836\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4603 - val_acc: 0.6870\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1077 - acc: 0.7360 - val_loss: 0.4793 - val_acc: 0.6854\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4887 - val_acc: 0.6867\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4912 - val_acc: 0.6866\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.5009 - val_acc: 0.6872\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.5142 - val_acc: 0.6860\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.5159 - val_acc: 0.6862\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.5250 - val_acc: 0.6863\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.5234 - val_acc: 0.6867\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5331 - val_acc: 0.6858\n",
      "17921/17921 [==============================] - 1s 40us/sample - loss: 0.9976 - acc: 0.6553\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 6s 168us/sample - loss: 0.3153 - acc: 0.6949 - val_loss: 0.4099 - val_acc: 0.6831\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.1422 - acc: 0.7487 - val_loss: 0.3773 - val_acc: 0.6917\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1224 - acc: 0.7522 - val_loss: 0.3728 - val_acc: 0.6945\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1133 - acc: 0.7537 - val_loss: 0.3748 - val_acc: 0.6953\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.1075 - acc: 0.7546 - val_loss: 0.3939 - val_acc: 0.6945\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1035 - acc: 0.7552 - val_loss: 0.4067 - val_acc: 0.6939\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1012 - acc: 0.7554 - val_loss: 0.4246 - val_acc: 0.6964\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.1002 - acc: 0.7555 - val_loss: 0.4315 - val_acc: 0.6963\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0987 - acc: 0.7556 - val_loss: 0.4475 - val_acc: 0.6963\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.4647 - val_acc: 0.6962\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4764 - val_acc: 0.6972\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4905 - val_acc: 0.6965\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4970 - val_acc: 0.6972\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5091 - val_acc: 0.6972\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5224 - val_acc: 0.6966\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5265 - val_acc: 0.6972\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5287 - val_acc: 0.6971\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5482 - val_acc: 0.6974\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 84us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5461 - val_acc: 0.6975\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 83us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5660 - val_acc: 0.6974\n",
      "17922/17922 [==============================] - 1s 40us/sample - loss: 1.3999 - acc: 0.6086\n",
      "35842/35842 [==============================] - 2s 43us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 171us/sample - loss: 0.3105 - acc: 0.7067 - val_loss: 0.3943 - val_acc: 0.6863\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1471 - acc: 0.7549 - val_loss: 0.3601 - val_acc: 0.6973\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1265 - acc: 0.7587 - val_loss: 0.3332 - val_acc: 0.7064\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1143 - acc: 0.7613 - val_loss: 0.3306 - val_acc: 0.7075\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1090 - acc: 0.7622 - val_loss: 0.3338 - val_acc: 0.7091\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1046 - acc: 0.7628 - val_loss: 0.3405 - val_acc: 0.7100\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1011 - acc: 0.7632 - val_loss: 0.3438 - val_acc: 0.7105\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0998 - acc: 0.7633 - val_loss: 0.3573 - val_acc: 0.7117\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0982 - acc: 0.7634 - val_loss: 0.3639 - val_acc: 0.7108\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.0973 - acc: 0.7635 - val_loss: 0.3745 - val_acc: 0.7118\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3810 - val_acc: 0.7116\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0965 - acc: 0.7635 - val_loss: 0.3916 - val_acc: 0.7122\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0961 - acc: 0.7635 - val_loss: 0.3852 - val_acc: 0.7118\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0958 - acc: 0.7635 - val_loss: 0.4064 - val_acc: 0.7117\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0956 - acc: 0.7635 - val_loss: 0.4102 - val_acc: 0.7123\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4204 - val_acc: 0.7124\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4272 - val_acc: 0.7124\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4291 - val_acc: 0.7124\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4397 - val_acc: 0.7127\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4448 - val_acc: 0.7123\n",
      "17921/17921 [==============================] - 1s 41us/sample - loss: 0.8974 - acc: 0.6578\n",
      "35843/35843 [==============================] - 2s 43us/sample - loss: 0.0951 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 6s 167us/sample - loss: 0.3276 - acc: 0.6732 - val_loss: 0.3829 - val_acc: 0.6830\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1600 - acc: 0.7275 - val_loss: 0.3771 - val_acc: 0.6868\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1393 - acc: 0.7315 - val_loss: 0.3595 - val_acc: 0.6897\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1272 - acc: 0.7337 - val_loss: 0.3814 - val_acc: 0.6841\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1200 - acc: 0.7348 - val_loss: 0.3914 - val_acc: 0.6832\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1158 - acc: 0.7354 - val_loss: 0.3978 - val_acc: 0.6861\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1128 - acc: 0.7357 - val_loss: 0.4077 - val_acc: 0.6872\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1116 - acc: 0.7358 - val_loss: 0.4225 - val_acc: 0.6859\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1097 - acc: 0.7359 - val_loss: 0.4273 - val_acc: 0.6883\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1089 - acc: 0.7360 - val_loss: 0.4362 - val_acc: 0.6883\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1083 - acc: 0.7360 - val_loss: 0.4421 - val_acc: 0.6900\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1079 - acc: 0.7361 - val_loss: 0.4410 - val_acc: 0.6907\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4501 - val_acc: 0.6896\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4668 - val_acc: 0.6904\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4620 - val_acc: 0.6910\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4767 - val_acc: 0.6906\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4918 - val_acc: 0.6904\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4947 - val_acc: 0.6912\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4993 - val_acc: 0.6908\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.5094 - val_acc: 0.6917\n",
      "17921/17921 [==============================] - 1s 42us/sample - loss: 0.9386 - acc: 0.6676\n",
      "35843/35843 [==============================] - 2s 43us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 187us/sample - loss: 0.2308 - acc: 0.7225 - val_loss: 0.3950 - val_acc: 0.6895\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1286 - acc: 0.7513 - val_loss: 0.3692 - val_acc: 0.6905\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1148 - acc: 0.7538 - val_loss: 0.3835 - val_acc: 0.6925\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1084 - acc: 0.7547 - val_loss: 0.3825 - val_acc: 0.6951\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 98us/sample - loss: 0.1033 - acc: 0.7553 - val_loss: 0.3917 - val_acc: 0.6959\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1005 - acc: 0.7555 - val_loss: 0.3964 - val_acc: 0.6959\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0991 - acc: 0.7556 - val_loss: 0.4181 - val_acc: 0.6963\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0982 - acc: 0.7556 - val_loss: 0.4253 - val_acc: 0.6964\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0994 - acc: 0.7556 - val_loss: 0.4265 - val_acc: 0.6958\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4485 - val_acc: 0.6978\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4511 - val_acc: 0.6964\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4597 - val_acc: 0.6968\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4490 - val_acc: 0.6969\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4724 - val_acc: 0.6966\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4738 - val_acc: 0.6970\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4736 - val_acc: 0.6976\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4758 - val_acc: 0.6970\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4802 - val_acc: 0.6974\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.4907 - val_acc: 0.6976\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.4935 - val_acc: 0.6974\n",
      "17922/17922 [==============================] - 1s 48us/sample - loss: 1.2127 - acc: 0.6058\n",
      "35842/35842 [==============================] - 2s 46us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 184us/sample - loss: 0.2432 - acc: 0.7296 - val_loss: 0.3699 - val_acc: 0.6919\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1374 - acc: 0.7567 - val_loss: 0.3302 - val_acc: 0.7062\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1185 - acc: 0.7606 - val_loss: 0.3171 - val_acc: 0.7106\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1087 - acc: 0.7623 - val_loss: 0.3227 - val_acc: 0.7126\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1055 - acc: 0.7628 - val_loss: 0.3249 - val_acc: 0.7125\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1003 - acc: 0.7633 - val_loss: 0.3329 - val_acc: 0.7132\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0983 - acc: 0.7634 - val_loss: 0.3367 - val_acc: 0.7130\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0971 - acc: 0.7635 - val_loss: 0.3520 - val_acc: 0.7129\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3562 - val_acc: 0.7138\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3604 - val_acc: 0.7136\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3605 - val_acc: 0.7140\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.3704 - val_acc: 0.7143\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3749 - val_acc: 0.7143\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0958 - acc: 0.7635 - val_loss: 0.3592 - val_acc: 0.7134\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3683 - val_acc: 0.7137\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3778 - val_acc: 0.7139\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3850 - val_acc: 0.7139\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3877 - val_acc: 0.7139\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.3870 - val_acc: 0.7140\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.3867 - val_acc: 0.7140\n",
      "17921/17921 [==============================] - 1s 46us/sample - loss: 0.7598 - acc: 0.6596\n",
      "35843/35843 [==============================] - 2s 50us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 191us/sample - loss: 0.2574 - acc: 0.6998 - val_loss: 0.3561 - val_acc: 0.6819\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1491 - acc: 0.7299 - val_loss: 0.3584 - val_acc: 0.6854\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1309 - acc: 0.7334 - val_loss: 0.3670 - val_acc: 0.6847\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1215 - acc: 0.7348 - val_loss: 0.3615 - val_acc: 0.6926\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1162 - acc: 0.7355 - val_loss: 0.3797 - val_acc: 0.6882\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1161 - acc: 0.7354 - val_loss: 0.3704 - val_acc: 0.6894\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1112 - acc: 0.7358 - val_loss: 0.3774 - val_acc: 0.6931\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1097 - acc: 0.7360 - val_loss: 0.3882 - val_acc: 0.6914\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1086 - acc: 0.7360 - val_loss: 0.4005 - val_acc: 0.6906\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1080 - acc: 0.7361 - val_loss: 0.4061 - val_acc: 0.6916\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4087 - val_acc: 0.6912\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4138 - val_acc: 0.6916\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4262 - val_acc: 0.6910\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4295 - val_acc: 0.6925\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4343 - val_acc: 0.6924\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4374 - val_acc: 0.6925\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4381 - val_acc: 0.6912\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4396 - val_acc: 0.6914\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4445 - val_acc: 0.6907\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4404 - val_acc: 0.6923\n",
      "17921/17921 [==============================] - 1s 48us/sample - loss: 0.7895 - acc: 0.6662\n",
      "35843/35843 [==============================] - 2s 49us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 190us/sample - loss: 0.2509 - acc: 0.7158 - val_loss: 0.3799 - val_acc: 0.6899\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1318 - acc: 0.7508 - val_loss: 0.3623 - val_acc: 0.6942\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1168 - acc: 0.7534 - val_loss: 0.3718 - val_acc: 0.6928\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1093 - acc: 0.7546 - val_loss: 0.3773 - val_acc: 0.6941\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1046 - acc: 0.7552 - val_loss: 0.3903 - val_acc: 0.6963\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1018 - acc: 0.7554 - val_loss: 0.3934 - val_acc: 0.6970\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0998 - acc: 0.7555 - val_loss: 0.4143 - val_acc: 0.6972\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0987 - acc: 0.7556 - val_loss: 0.4254 - val_acc: 0.6973\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0981 - acc: 0.7556 - val_loss: 0.4324 - val_acc: 0.6979\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4458 - val_acc: 0.6970\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4595 - val_acc: 0.6973\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4657 - val_acc: 0.6979\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4644 - val_acc: 0.6978\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4713 - val_acc: 0.6980\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4776 - val_acc: 0.6968\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4743 - val_acc: 0.6980\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4895 - val_acc: 0.6977\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4974 - val_acc: 0.6975\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5005 - val_acc: 0.6977\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5047 - val_acc: 0.6975\n",
      "17922/17922 [==============================] - 1s 46us/sample - loss: 1.2562 - acc: 0.6046\n",
      "35842/35842 [==============================] - 2s 47us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 195us/sample - loss: 0.2537 - acc: 0.7261 - val_loss: 0.3723 - val_acc: 0.6916\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1384 - acc: 0.7564 - val_loss: 0.3374 - val_acc: 0.7024\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1203 - acc: 0.7604 - val_loss: 0.3214 - val_acc: 0.7085\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1113 - acc: 0.7620 - val_loss: 0.3142 - val_acc: 0.7097\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1048 - acc: 0.7630 - val_loss: 0.3164 - val_acc: 0.7124\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1011 - acc: 0.7633 - val_loss: 0.3243 - val_acc: 0.7131\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0990 - acc: 0.7635 - val_loss: 0.3339 - val_acc: 0.7135\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0976 - acc: 0.7635 - val_loss: 0.3409 - val_acc: 0.7137\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3489 - val_acc: 0.7139\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0962 - acc: 0.7636 - val_loss: 0.3535 - val_acc: 0.7143\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3562 - val_acc: 0.7142\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3599 - val_acc: 0.7140\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.3621 - val_acc: 0.7144\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3710 - val_acc: 0.7146\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3740 - val_acc: 0.7143\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3774 - val_acc: 0.7143\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3764 - val_acc: 0.7144\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3854 - val_acc: 0.7144\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3862 - val_acc: 0.7144\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3833 - val_acc: 0.7144\n",
      "17921/17921 [==============================] - 1s 46us/sample - loss: 0.7539 - acc: 0.6608\n",
      "35843/35843 [==============================] - 2s 46us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 192us/sample - loss: 0.2730 - acc: 0.6936 - val_loss: 0.3714 - val_acc: 0.6859\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1510 - acc: 0.7295 - val_loss: 0.3611 - val_acc: 0.6872\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1332 - acc: 0.7330 - val_loss: 0.3459 - val_acc: 0.6901\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1237 - acc: 0.7346 - val_loss: 0.3530 - val_acc: 0.6890\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1172 - acc: 0.7355 - val_loss: 0.3653 - val_acc: 0.6932\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1136 - acc: 0.7358 - val_loss: 0.3802 - val_acc: 0.6906\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1111 - acc: 0.7359 - val_loss: 0.3709 - val_acc: 0.6903\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1097 - acc: 0.7360 - val_loss: 0.3897 - val_acc: 0.6921\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1088 - acc: 0.7360 - val_loss: 0.3995 - val_acc: 0.6898\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1082 - acc: 0.7361 - val_loss: 0.4063 - val_acc: 0.6911\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1077 - acc: 0.7361 - val_loss: 0.4099 - val_acc: 0.6911\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4206 - val_acc: 0.6917\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4280 - val_acc: 0.6915\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4335 - val_acc: 0.6905\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4231 - val_acc: 0.6913\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4320 - val_acc: 0.6910\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4403 - val_acc: 0.6925\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4441 - val_acc: 0.6917\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4483 - val_acc: 0.6919\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4558 - val_acc: 0.6919\n",
      "17921/17921 [==============================] - 1s 48us/sample - loss: 0.8123 - acc: 0.6677\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 198us/sample - loss: 0.2715 - acc: 0.7083 - val_loss: 0.3919 - val_acc: 0.6877\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1329 - acc: 0.7506 - val_loss: 0.3795 - val_acc: 0.6953\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1175 - acc: 0.7532 - val_loss: 0.3708 - val_acc: 0.6963\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1112 - acc: 0.7544 - val_loss: 0.3815 - val_acc: 0.6979\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1055 - acc: 0.7551 - val_loss: 0.3898 - val_acc: 0.6978\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1023 - acc: 0.7554 - val_loss: 0.3928 - val_acc: 0.6978\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1004 - acc: 0.7556 - val_loss: 0.4101 - val_acc: 0.6971\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0993 - acc: 0.7556 - val_loss: 0.4187 - val_acc: 0.6984\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4348 - val_acc: 0.6980\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4518 - val_acc: 0.6968\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4504 - val_acc: 0.6981\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4644 - val_acc: 0.6983\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4661 - val_acc: 0.6979\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4711 - val_acc: 0.6989\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4767 - val_acc: 0.6988\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4799 - val_acc: 0.6990\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4895 - val_acc: 0.6984\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4923 - val_acc: 0.6988\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5011 - val_acc: 0.6987\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5060 - val_acc: 0.6987\n",
      "17922/17922 [==============================] - 1s 47us/sample - loss: 1.2617 - acc: 0.6084\n",
      "35842/35842 [==============================] - 2s 47us/sample - loss: 0.0966 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 203us/sample - loss: 0.2789 - acc: 0.7166 - val_loss: 0.3731 - val_acc: 0.6899\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1402 - acc: 0.7563 - val_loss: 0.3423 - val_acc: 0.6980\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1223 - acc: 0.7600 - val_loss: 0.3200 - val_acc: 0.7089\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1116 - acc: 0.7620 - val_loss: 0.3241 - val_acc: 0.7126\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1067 - acc: 0.7628 - val_loss: 0.3214 - val_acc: 0.7127\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1021 - acc: 0.7633 - val_loss: 0.3267 - val_acc: 0.7127\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1006 - acc: 0.7634 - val_loss: 0.3235 - val_acc: 0.7140\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0984 - acc: 0.7635 - val_loss: 0.3369 - val_acc: 0.7147\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0974 - acc: 0.7635 - val_loss: 0.3501 - val_acc: 0.7144\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3488 - val_acc: 0.7145\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0963 - acc: 0.7636 - val_loss: 0.3565 - val_acc: 0.7145\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3682 - val_acc: 0.7147\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0958 - acc: 0.7636 - val_loss: 0.3648 - val_acc: 0.7145\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.3720 - val_acc: 0.7150\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3790 - val_acc: 0.7150\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3845 - val_acc: 0.7148\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3866 - val_acc: 0.7149\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3935 - val_acc: 0.7149\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3904 - val_acc: 0.7150\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3973 - val_acc: 0.7150\n",
      "17921/17921 [==============================] - 1s 49us/sample - loss: 0.7805 - acc: 0.6620\n",
      "35843/35843 [==============================] - 2s 48us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 202us/sample - loss: 0.3021 - acc: 0.6839 - val_loss: 0.3698 - val_acc: 0.6815\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1539 - acc: 0.7291 - val_loss: 0.3537 - val_acc: 0.6878\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1342 - acc: 0.7329 - val_loss: 0.3620 - val_acc: 0.6825\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1247 - acc: 0.7346 - val_loss: 0.3576 - val_acc: 0.6884\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1181 - acc: 0.7354 - val_loss: 0.3526 - val_acc: 0.6904\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1142 - acc: 0.7358 - val_loss: 0.3657 - val_acc: 0.6904\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1117 - acc: 0.7359 - val_loss: 0.3682 - val_acc: 0.6902\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1103 - acc: 0.7360 - val_loss: 0.3813 - val_acc: 0.6925\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1093 - acc: 0.7361 - val_loss: 0.3772 - val_acc: 0.6936\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1086 - acc: 0.7361 - val_loss: 0.3951 - val_acc: 0.6917\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1080 - acc: 0.7361 - val_loss: 0.4017 - val_acc: 0.6907\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1077 - acc: 0.7361 - val_loss: 0.4065 - val_acc: 0.6932\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4114 - val_acc: 0.6925\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4123 - val_acc: 0.6931\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4250 - val_acc: 0.6934\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4257 - val_acc: 0.6929\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4307 - val_acc: 0.6930\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4274 - val_acc: 0.6930\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4378 - val_acc: 0.6924\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4428 - val_acc: 0.6926\n",
      "17921/17921 [==============================] - 1s 49us/sample - loss: 0.7947 - acc: 0.6672\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 201us/sample - loss: 0.2394 - acc: 0.7210 - val_loss: 0.3934 - val_acc: 0.6865\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1323 - acc: 0.7506 - val_loss: 0.3643 - val_acc: 0.6951\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1168 - acc: 0.7534 - val_loss: 0.3737 - val_acc: 0.6968\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1090 - acc: 0.7546 - val_loss: 0.3847 - val_acc: 0.6950\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1040 - acc: 0.7552 - val_loss: 0.3864 - val_acc: 0.6963\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1011 - acc: 0.7555 - val_loss: 0.4034 - val_acc: 0.6959\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0995 - acc: 0.7556 - val_loss: 0.4239 - val_acc: 0.6959\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4312 - val_acc: 0.6974\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4367 - val_acc: 0.6969\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4461 - val_acc: 0.6969\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4559 - val_acc: 0.6968\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 98us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4617 - val_acc: 0.6972\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4724 - val_acc: 0.6976\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4734 - val_acc: 0.6968\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4836 - val_acc: 0.6975\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4878 - val_acc: 0.6976\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4798 - val_acc: 0.6974\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.4932 - val_acc: 0.6973\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.4928 - val_acc: 0.6976\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.4949 - val_acc: 0.6976\n",
      "17922/17922 [==============================] - 1s 49us/sample - loss: 1.2092 - acc: 0.6061\n",
      "35842/35842 [==============================] - 2s 48us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 8s 209us/sample - loss: 0.2484 - acc: 0.7280 - val_loss: 0.3697 - val_acc: 0.6919\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1377 - acc: 0.7568 - val_loss: 0.3275 - val_acc: 0.7068\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1191 - acc: 0.7606 - val_loss: 0.3079 - val_acc: 0.7097\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1100 - acc: 0.7622 - val_loss: 0.3225 - val_acc: 0.7122\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1047 - acc: 0.7630 - val_loss: 0.3249 - val_acc: 0.7128\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1008 - acc: 0.7633 - val_loss: 0.3250 - val_acc: 0.7126\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0987 - acc: 0.7635 - val_loss: 0.3319 - val_acc: 0.7134\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0975 - acc: 0.7635 - val_loss: 0.3420 - val_acc: 0.7138\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3506 - val_acc: 0.7144\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0962 - acc: 0.7636 - val_loss: 0.3542 - val_acc: 0.7138\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3600 - val_acc: 0.7139\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.3717 - val_acc: 0.7143\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3697 - val_acc: 0.7141\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3784 - val_acc: 0.7142\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3820 - val_acc: 0.7139\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3834 - val_acc: 0.7145\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3853 - val_acc: 0.7143\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3841 - val_acc: 0.7143\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3900 - val_acc: 0.7142\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.3915 - val_acc: 0.7143\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.7825 - acc: 0.6596\n",
      "35843/35843 [==============================] - 1s 41us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 185us/sample - loss: 0.2630 - acc: 0.6989 - val_loss: 0.3838 - val_acc: 0.6761\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1533 - acc: 0.7290 - val_loss: 0.3624 - val_acc: 0.6819\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1351 - acc: 0.7326 - val_loss: 0.3438 - val_acc: 0.6920\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1238 - acc: 0.7345 - val_loss: 0.3562 - val_acc: 0.6928\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1206 - acc: 0.7349 - val_loss: 0.3558 - val_acc: 0.6901\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1146 - acc: 0.7356 - val_loss: 0.3653 - val_acc: 0.6912\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1115 - acc: 0.7359 - val_loss: 0.3752 - val_acc: 0.6912\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1100 - acc: 0.7360 - val_loss: 0.3866 - val_acc: 0.6917\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1089 - acc: 0.7360 - val_loss: 0.3945 - val_acc: 0.6917\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1083 - acc: 0.7360 - val_loss: 0.3995 - val_acc: 0.6920\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1078 - acc: 0.7361 - val_loss: 0.4013 - val_acc: 0.6919\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4114 - val_acc: 0.6918\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4156 - val_acc: 0.6912\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4275 - val_acc: 0.6919\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4247 - val_acc: 0.6917\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4304 - val_acc: 0.6925\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4360 - val_acc: 0.6919\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4346 - val_acc: 0.6928\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4387 - val_acc: 0.6919\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4391 - val_acc: 0.6919\n",
      "17921/17921 [==============================] - 1s 48us/sample - loss: 0.7717 - acc: 0.6663\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 194us/sample - loss: 0.2513 - acc: 0.7160 - val_loss: 0.3882 - val_acc: 0.6908\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1336 - acc: 0.7505 - val_loss: 0.3782 - val_acc: 0.6927\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1183 - acc: 0.7531 - val_loss: 0.3716 - val_acc: 0.6948\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1101 - acc: 0.7545 - val_loss: 0.3898 - val_acc: 0.6966\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.1053 - acc: 0.7551 - val_loss: 0.3849 - val_acc: 0.6974\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1021 - acc: 0.7554 - val_loss: 0.4055 - val_acc: 0.6958\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1001 - acc: 0.7556 - val_loss: 0.4132 - val_acc: 0.6977\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0989 - acc: 0.7556 - val_loss: 0.4275 - val_acc: 0.6981\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4304 - val_acc: 0.6974\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4441 - val_acc: 0.6979\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4550 - val_acc: 0.6980\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4670 - val_acc: 0.6978\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4700 - val_acc: 0.6977\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4798 - val_acc: 0.6977\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4805 - val_acc: 0.6982\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4846 - val_acc: 0.6976\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4937 - val_acc: 0.6982\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4938 - val_acc: 0.6983\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5002 - val_acc: 0.6982\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.4996 - val_acc: 0.6987\n",
      "17922/17922 [==============================] - 1s 45us/sample - loss: 1.2216 - acc: 0.6079\n",
      "35842/35842 [==============================] - 2s 45us/sample - loss: 0.0968 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 204us/sample - loss: 0.2574 - acc: 0.7248 - val_loss: 0.3702 - val_acc: 0.6915\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1397 - acc: 0.7564 - val_loss: 0.3336 - val_acc: 0.7035\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1214 - acc: 0.7602 - val_loss: 0.3157 - val_acc: 0.7095\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1139 - acc: 0.7617 - val_loss: 0.3195 - val_acc: 0.7122\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1062 - acc: 0.7628 - val_loss: 0.3180 - val_acc: 0.7132\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1023 - acc: 0.7633 - val_loss: 0.3222 - val_acc: 0.7142\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0999 - acc: 0.7634 - val_loss: 0.3274 - val_acc: 0.7139\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0973 - acc: 0.7635 - val_loss: 0.3336 - val_acc: 0.7143\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0973 - acc: 0.7635 - val_loss: 0.3406 - val_acc: 0.7137\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0966 - acc: 0.7635 - val_loss: 0.3523 - val_acc: 0.7142\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3510 - val_acc: 0.7142\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3556 - val_acc: 0.7144\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 197us/sample - loss: 0.2882 - acc: 0.7149 - val_loss: 0.3857 - val_acc: 0.6908\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1454 - acc: 0.7555 - val_loss: 0.3565 - val_acc: 0.6956\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1261 - acc: 0.7590 - val_loss: 0.3430 - val_acc: 0.7058\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1150 - acc: 0.7615 - val_loss: 0.3205 - val_acc: 0.7115\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1078 - acc: 0.7627 - val_loss: 0.3238 - val_acc: 0.7128\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1036 - acc: 0.7632 - val_loss: 0.3249 - val_acc: 0.7131\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1007 - acc: 0.7634 - val_loss: 0.3284 - val_acc: 0.7138\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0990 - acc: 0.7635 - val_loss: 0.3324 - val_acc: 0.7141\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0978 - acc: 0.7635 - val_loss: 0.3388 - val_acc: 0.7144\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0971 - acc: 0.7635 - val_loss: 0.3440 - val_acc: 0.7147\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0965 - acc: 0.7636 - val_loss: 0.3553 - val_acc: 0.7146\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0961 - acc: 0.7636 - val_loss: 0.3591 - val_acc: 0.7149\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0958 - acc: 0.7636 - val_loss: 0.3631 - val_acc: 0.7149\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3645 - val_acc: 0.7147\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3629 - val_acc: 0.7148\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3754 - val_acc: 0.7152\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3750 - val_acc: 0.7150\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3802 - val_acc: 0.7151\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3837 - val_acc: 0.7150\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3855 - val_acc: 0.7152\n",
      "17921/17921 [==============================] - 1s 46us/sample - loss: 0.7570 - acc: 0.6612\n",
      "35843/35843 [==============================] - 2s 46us/sample - loss: 0.0950 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 208us/sample - loss: 0.3048 - acc: 0.6850 - val_loss: 0.3824 - val_acc: 0.6812\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1594 - acc: 0.7280 - val_loss: 0.3562 - val_acc: 0.6881\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1383 - acc: 0.7322 - val_loss: 0.3520 - val_acc: 0.6879\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1273 - acc: 0.7342 - val_loss: 0.3447 - val_acc: 0.6939\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1209 - acc: 0.7351 - val_loss: 0.3574 - val_acc: 0.6902\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1166 - acc: 0.7356 - val_loss: 0.3551 - val_acc: 0.6936\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1134 - acc: 0.7359 - val_loss: 0.3651 - val_acc: 0.6907\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1115 - acc: 0.7360 - val_loss: 0.3650 - val_acc: 0.6921\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1103 - acc: 0.7360 - val_loss: 0.3718 - val_acc: 0.6935\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1094 - acc: 0.7361 - val_loss: 0.3835 - val_acc: 0.6923\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1088 - acc: 0.7361 - val_loss: 0.3900 - val_acc: 0.6934\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1084 - acc: 0.7361 - val_loss: 0.3986 - val_acc: 0.6912\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1078 - acc: 0.7361 - val_loss: 0.3976 - val_acc: 0.6943\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4120 - val_acc: 0.6951\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1079 - acc: 0.7361 - val_loss: 0.4117 - val_acc: 0.6923\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4118 - val_acc: 0.6927\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4254 - val_acc: 0.6927\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4293 - val_acc: 0.6929\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4195 - val_acc: 0.6934\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4331 - val_acc: 0.6926\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.7743 - acc: 0.6659\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.1067 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 200us/sample - loss: 0.2424 - acc: 0.7195 - val_loss: 0.3942 - val_acc: 0.6904\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1344 - acc: 0.7502 - val_loss: 0.3657 - val_acc: 0.6940\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1185 - acc: 0.7530 - val_loss: 0.3613 - val_acc: 0.6961\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1114 - acc: 0.7542 - val_loss: 0.3764 - val_acc: 0.6956\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1058 - acc: 0.7550 - val_loss: 0.3776 - val_acc: 0.6964\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1025 - acc: 0.7554 - val_loss: 0.3989 - val_acc: 0.6931\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1015 - acc: 0.7554 - val_loss: 0.3994 - val_acc: 0.6960\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0995 - acc: 0.7556 - val_loss: 0.4128 - val_acc: 0.6963\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0986 - acc: 0.7556 - val_loss: 0.4279 - val_acc: 0.6967\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.4375 - val_acc: 0.6957\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4410 - val_acc: 0.6968\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4499 - val_acc: 0.6965\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4562 - val_acc: 0.6966\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4573 - val_acc: 0.6965\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4651 - val_acc: 0.6966\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4691 - val_acc: 0.6964\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4754 - val_acc: 0.6969\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4760 - val_acc: 0.6965\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4780 - val_acc: 0.6968\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4813 - val_acc: 0.6967\n",
      "17922/17922 [==============================] - 1s 45us/sample - loss: 1.2112 - acc: 0.6025\n",
      "35842/35842 [==============================] - 2s 46us/sample - loss: 0.0966 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 202us/sample - loss: 0.2507 - acc: 0.7280 - val_loss: 0.3692 - val_acc: 0.6915\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1391 - acc: 0.7565 - val_loss: 0.3335 - val_acc: 0.7046\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1202 - acc: 0.7604 - val_loss: 0.3258 - val_acc: 0.7105\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1115 - acc: 0.7619 - val_loss: 0.3182 - val_acc: 0.7112\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1051 - acc: 0.7629 - val_loss: 0.3296 - val_acc: 0.7122\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1015 - acc: 0.7633 - val_loss: 0.3309 - val_acc: 0.7124\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0992 - acc: 0.7634 - val_loss: 0.3399 - val_acc: 0.7130\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0978 - acc: 0.7635 - val_loss: 0.3477 - val_acc: 0.7133\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0969 - acc: 0.7635 - val_loss: 0.3504 - val_acc: 0.7131\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3545 - val_acc: 0.7133\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3609 - val_acc: 0.7135\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3705 - val_acc: 0.7133\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.3726 - val_acc: 0.7133\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3785 - val_acc: 0.7135\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3815 - val_acc: 0.7133\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3838 - val_acc: 0.7136\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3845 - val_acc: 0.7136\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3920 - val_acc: 0.7137\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3893 - val_acc: 0.7137\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.3963 - val_acc: 0.7137\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.7556 - acc: 0.6598\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.0950 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 208us/sample - loss: 0.2662 - acc: 0.6975 - val_loss: 0.3704 - val_acc: 0.6803\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1541 - acc: 0.7288 - val_loss: 0.3551 - val_acc: 0.6863\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1349 - acc: 0.7326 - val_loss: 0.3476 - val_acc: 0.6914\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1249 - acc: 0.7343 - val_loss: 0.3746 - val_acc: 0.6816\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1202 - acc: 0.7350 - val_loss: 0.3590 - val_acc: 0.6910\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1146 - acc: 0.7357 - val_loss: 0.3646 - val_acc: 0.6910\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1121 - acc: 0.7358 - val_loss: 0.3724 - val_acc: 0.6907\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1106 - acc: 0.7359 - val_loss: 0.3940 - val_acc: 0.6891\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1100 - acc: 0.7360 - val_loss: 0.3943 - val_acc: 0.6872\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1093 - acc: 0.7360 - val_loss: 0.3981 - val_acc: 0.6893\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1083 - acc: 0.7361 - val_loss: 0.4067 - val_acc: 0.6891\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1077 - acc: 0.7361 - val_loss: 0.4182 - val_acc: 0.6899\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4224 - val_acc: 0.6910\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4211 - val_acc: 0.6897\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4254 - val_acc: 0.6907\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4299 - val_acc: 0.6908\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4356 - val_acc: 0.6901\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4381 - val_acc: 0.6908\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4349 - val_acc: 0.6899\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4436 - val_acc: 0.6907\n",
      "17921/17921 [==============================] - 1s 44us/sample - loss: 0.7753 - acc: 0.6645\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.1066 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 7s 207us/sample - loss: 0.2573 - acc: 0.7141 - val_loss: 0.3897 - val_acc: 0.6849\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1366 - acc: 0.7500 - val_loss: 0.3702 - val_acc: 0.6929\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1198 - acc: 0.7529 - val_loss: 0.3661 - val_acc: 0.6961\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1117 - acc: 0.7543 - val_loss: 0.3674 - val_acc: 0.6971\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1067 - acc: 0.7550 - val_loss: 0.3789 - val_acc: 0.6966\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1035 - acc: 0.7553 - val_loss: 0.3950 - val_acc: 0.6968\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1012 - acc: 0.7555 - val_loss: 0.4074 - val_acc: 0.6961\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0998 - acc: 0.7556 - val_loss: 0.4173 - val_acc: 0.6964\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0989 - acc: 0.7556 - val_loss: 0.4318 - val_acc: 0.6963\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0982 - acc: 0.7556 - val_loss: 0.4374 - val_acc: 0.6968\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4429 - val_acc: 0.6968\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4604 - val_acc: 0.6970\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4628 - val_acc: 0.6971\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4665 - val_acc: 0.6974\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4735 - val_acc: 0.6971\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4834 - val_acc: 0.6965\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4730 - val_acc: 0.6971\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4767 - val_acc: 0.6975\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4869 - val_acc: 0.6974\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4935 - val_acc: 0.6976\n",
      "17922/17922 [==============================] - 1s 48us/sample - loss: 1.1966 - acc: 0.6082\n",
      "35842/35842 [==============================] - 2s 48us/sample - loss: 0.0969 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 7s 207us/sample - loss: 0.2658 - acc: 0.7223 - val_loss: 0.3803 - val_acc: 0.6897\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1436 - acc: 0.7556 - val_loss: 0.3464 - val_acc: 0.7002\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1248 - acc: 0.7594 - val_loss: 0.3272 - val_acc: 0.7090\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1139 - acc: 0.7616 - val_loss: 0.3287 - val_acc: 0.7110\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1113 - acc: 0.7622 - val_loss: 0.3215 - val_acc: 0.7125\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1042 - acc: 0.7631 - val_loss: 0.3186 - val_acc: 0.7128\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1012 - acc: 0.7633 - val_loss: 0.3299 - val_acc: 0.7129\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0998 - acc: 0.7634 - val_loss: 0.3316 - val_acc: 0.7124\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0982 - acc: 0.7635 - val_loss: 0.3356 - val_acc: 0.7134\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0973 - acc: 0.7635 - val_loss: 0.3383 - val_acc: 0.7137\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3497 - val_acc: 0.7135\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0963 - acc: 0.7636 - val_loss: 0.3591 - val_acc: 0.7138\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0961 - acc: 0.7636 - val_loss: 0.3535 - val_acc: 0.7136\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3642 - val_acc: 0.7137\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3653 - val_acc: 0.7135\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3716 - val_acc: 0.7132\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3727 - val_acc: 0.7137\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3798 - val_acc: 0.7131\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3729 - val_acc: 0.7133\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3803 - val_acc: 0.7135\n",
      "17921/17921 [==============================] - 1s 46us/sample - loss: 0.7429 - acc: 0.6598\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.0951 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 271us/sample - loss: 0.2825 - acc: 0.6914 - val_loss: 0.3732 - val_acc: 0.6783\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1557 - acc: 0.7286 - val_loss: 0.3669 - val_acc: 0.6863\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1370 - acc: 0.7324 - val_loss: 0.3522 - val_acc: 0.6906\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1275 - acc: 0.7341 - val_loss: 0.3548 - val_acc: 0.6914\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1204 - acc: 0.7351 - val_loss: 0.3554 - val_acc: 0.6877\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1160 - acc: 0.7356 - val_loss: 0.3616 - val_acc: 0.6910\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1136 - acc: 0.7358 - val_loss: 0.3698 - val_acc: 0.6888\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1112 - acc: 0.7359 - val_loss: 0.3880 - val_acc: 0.6909\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1102 - acc: 0.7360 - val_loss: 0.3887 - val_acc: 0.6902\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1091 - acc: 0.7360 - val_loss: 0.3973 - val_acc: 0.6893\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1084 - acc: 0.7361 - val_loss: 0.4033 - val_acc: 0.6899\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1080 - acc: 0.7361 - val_loss: 0.4057 - val_acc: 0.6903\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4133 - val_acc: 0.6912\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4172 - val_acc: 0.6905\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4272 - val_acc: 0.6913\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4290 - val_acc: 0.6905\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4337 - val_acc: 0.6906\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4362 - val_acc: 0.6901\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4416 - val_acc: 0.6907\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4515 - val_acc: 0.6917\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.7929 - acc: 0.6676\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.1067 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 271us/sample - loss: 0.2803 - acc: 0.7065 - val_loss: 0.3978 - val_acc: 0.6839\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1379 - acc: 0.7498 - val_loss: 0.3783 - val_acc: 0.6893\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1209 - acc: 0.7528 - val_loss: 0.3653 - val_acc: 0.6964\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1130 - acc: 0.7542 - val_loss: 0.3629 - val_acc: 0.6964\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1077 - acc: 0.7549 - val_loss: 0.3873 - val_acc: 0.6970\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1044 - acc: 0.7553 - val_loss: 0.3983 - val_acc: 0.6964\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1026 - acc: 0.7554 - val_loss: 0.4001 - val_acc: 0.6967\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1005 - acc: 0.7556 - val_loss: 0.4075 - val_acc: 0.6970\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.0994 - acc: 0.7556 - val_loss: 0.4175 - val_acc: 0.6980\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0988 - acc: 0.7556 - val_loss: 0.4261 - val_acc: 0.6979\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.0983 - acc: 0.7556 - val_loss: 0.4373 - val_acc: 0.6981\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4433 - val_acc: 0.6976\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4476 - val_acc: 0.6982\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4595 - val_acc: 0.6980\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4627 - val_acc: 0.6983\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4752 - val_acc: 0.6984\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 108us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4793 - val_acc: 0.6984\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4829 - val_acc: 0.6985\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4867 - val_acc: 0.6985\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4938 - val_acc: 0.6985\n",
      "17922/17922 [==============================] - 1s 53us/sample - loss: 1.2071 - acc: 0.6072\n",
      "35842/35842 [==============================] - 2s 53us/sample - loss: 0.0968 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 286us/sample - loss: 0.2847 - acc: 0.7140 - val_loss: 0.3790 - val_acc: 0.6882\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1436 - acc: 0.7558 - val_loss: 0.3459 - val_acc: 0.6988\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1251 - acc: 0.7594 - val_loss: 0.3298 - val_acc: 0.7083\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1145 - acc: 0.7616 - val_loss: 0.3164 - val_acc: 0.7121\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1078 - acc: 0.7627 - val_loss: 0.3158 - val_acc: 0.7129\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1035 - acc: 0.7632 - val_loss: 0.3288 - val_acc: 0.7135\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1010 - acc: 0.7634 - val_loss: 0.3306 - val_acc: 0.7134\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0993 - acc: 0.7635 - val_loss: 0.3315 - val_acc: 0.7136\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0982 - acc: 0.7635 - val_loss: 0.3370 - val_acc: 0.7136\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0975 - acc: 0.7635 - val_loss: 0.3498 - val_acc: 0.7139\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3541 - val_acc: 0.7144\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0963 - acc: 0.7636 - val_loss: 0.3529 - val_acc: 0.7143\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.0960 - acc: 0.7636 - val_loss: 0.3581 - val_acc: 0.7146\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3643 - val_acc: 0.7144\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3711 - val_acc: 0.7144\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.0958 - acc: 0.7636 - val_loss: 0.3652 - val_acc: 0.7140\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3695 - val_acc: 0.7145\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3769 - val_acc: 0.7146\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3840 - val_acc: 0.7146\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3842 - val_acc: 0.7148\n",
      "17921/17921 [==============================] - 1s 53us/sample - loss: 0.7443 - acc: 0.6617\n",
      "35843/35843 [==============================] - 2s 52us/sample - loss: 0.0952 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 282us/sample - loss: 0.3080 - acc: 0.6821 - val_loss: 0.3620 - val_acc: 0.6852\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1590 - acc: 0.7280 - val_loss: 0.3548 - val_acc: 0.6849\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1380 - acc: 0.7322 - val_loss: 0.3445 - val_acc: 0.6899\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1275 - acc: 0.7341 - val_loss: 0.3549 - val_acc: 0.6907\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1210 - acc: 0.7350 - val_loss: 0.3618 - val_acc: 0.6908\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1173 - acc: 0.7355 - val_loss: 0.3699 - val_acc: 0.6887\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1139 - acc: 0.7358 - val_loss: 0.3708 - val_acc: 0.6913\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1120 - acc: 0.7359 - val_loss: 0.3874 - val_acc: 0.6868\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1111 - acc: 0.7360 - val_loss: 0.3872 - val_acc: 0.6903\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1098 - acc: 0.7360 - val_loss: 0.3996 - val_acc: 0.6924\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1095 - acc: 0.7360 - val_loss: 0.3969 - val_acc: 0.6911\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1085 - acc: 0.7361 - val_loss: 0.4080 - val_acc: 0.6907\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1082 - acc: 0.7361 - val_loss: 0.4115 - val_acc: 0.6912\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1079 - acc: 0.7361 - val_loss: 0.4113 - val_acc: 0.6915\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4232 - val_acc: 0.6912\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4266 - val_acc: 0.6910\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 6s 155us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4317 - val_acc: 0.6916\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 6s 170us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4383 - val_acc: 0.6913\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 6s 167us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4419 - val_acc: 0.6905\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 7s 190us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4430 - val_acc: 0.6911\n",
      "17921/17921 [==============================] - 1s 82us/sample - loss: 0.7789 - acc: 0.6661\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1068 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 8s 218us/sample - loss: 0.2480 - acc: 0.7186 - val_loss: 0.3893 - val_acc: 0.6867\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1348 - acc: 0.7502 - val_loss: 0.3698 - val_acc: 0.6928\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1189 - acc: 0.7531 - val_loss: 0.3655 - val_acc: 0.6959\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1107 - acc: 0.7544 - val_loss: 0.3831 - val_acc: 0.6961\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1056 - acc: 0.7550 - val_loss: 0.4072 - val_acc: 0.6919\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1059 - acc: 0.7551 - val_loss: 0.3917 - val_acc: 0.6958\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1023 - acc: 0.7554 - val_loss: 0.4093 - val_acc: 0.6950\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1000 - acc: 0.7555 - val_loss: 0.4141 - val_acc: 0.6966\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0990 - acc: 0.7556 - val_loss: 0.4305 - val_acc: 0.6959\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0991 - acc: 0.7556 - val_loss: 0.4298 - val_acc: 0.6958\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0983 - acc: 0.7556 - val_loss: 0.4400 - val_acc: 0.6959\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4484 - val_acc: 0.6960\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4561 - val_acc: 0.6961\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4605 - val_acc: 0.6966\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4614 - val_acc: 0.6964\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4689 - val_acc: 0.6967\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4687 - val_acc: 0.6966\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4727 - val_acc: 0.6964\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4800 - val_acc: 0.6964\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4818 - val_acc: 0.6970\n",
      "17922/17922 [==============================] - 1s 46us/sample - loss: 1.2085 - acc: 0.6043\n",
      "35842/35842 [==============================] - 2s 47us/sample - loss: 0.0968 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 8s 219us/sample - loss: 0.2544 - acc: 0.7279 - val_loss: 0.3752 - val_acc: 0.6861\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1417 - acc: 0.7560 - val_loss: 0.3305 - val_acc: 0.7040\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1227 - acc: 0.7599 - val_loss: 0.3205 - val_acc: 0.7096\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1125 - acc: 0.7618 - val_loss: 0.3179 - val_acc: 0.7116\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1067 - acc: 0.7627 - val_loss: 0.3253 - val_acc: 0.7109\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1027 - acc: 0.7632 - val_loss: 0.3256 - val_acc: 0.7124\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1001 - acc: 0.7634 - val_loss: 0.3307 - val_acc: 0.7125\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0985 - acc: 0.7635 - val_loss: 0.3360 - val_acc: 0.7131\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0974 - acc: 0.7635 - val_loss: 0.3364 - val_acc: 0.7128\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3497 - val_acc: 0.7130\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0963 - acc: 0.7635 - val_loss: 0.3496 - val_acc: 0.7128\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3575 - val_acc: 0.7130\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3620 - val_acc: 0.7131\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.3697 - val_acc: 0.7134\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3669 - val_acc: 0.7130\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3723 - val_acc: 0.7133\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3680 - val_acc: 0.7132\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3752 - val_acc: 0.7135\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3798 - val_acc: 0.7133\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3731 - val_acc: 0.7130\n",
      "17921/17921 [==============================] - 1s 47us/sample - loss: 0.7141 - acc: 0.6582\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.0950 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 8s 230us/sample - loss: 0.2723 - acc: 0.6962 - val_loss: 0.3654 - val_acc: 0.6789\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1562 - acc: 0.7284 - val_loss: 0.3543 - val_acc: 0.6893\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1370 - acc: 0.7322 - val_loss: 0.3439 - val_acc: 0.6884\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1262 - acc: 0.7341 - val_loss: 0.3458 - val_acc: 0.6915\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1202 - acc: 0.7350 - val_loss: 0.3522 - val_acc: 0.6906\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1155 - acc: 0.7355 - val_loss: 0.3663 - val_acc: 0.6876\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1155 - acc: 0.7355 - val_loss: 0.3734 - val_acc: 0.6886\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1117 - acc: 0.7359 - val_loss: 0.3836 - val_acc: 0.6890\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1100 - acc: 0.7360 - val_loss: 0.3845 - val_acc: 0.6898\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1091 - acc: 0.7360 - val_loss: 0.3973 - val_acc: 0.6895\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1084 - acc: 0.7360 - val_loss: 0.4047 - val_acc: 0.6894\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1079 - acc: 0.7361 - val_loss: 0.4029 - val_acc: 0.6900\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4143 - val_acc: 0.6897\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4196 - val_acc: 0.6910\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4251 - val_acc: 0.6906\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4277 - val_acc: 0.6906\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4330 - val_acc: 0.6904\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4347 - val_acc: 0.6900\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4404 - val_acc: 0.6899\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4492 - val_acc: 0.6894\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.8113 - acc: 0.6600\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.1067 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 8s 230us/sample - loss: 0.2595 - acc: 0.7144 - val_loss: 0.3948 - val_acc: 0.6886\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1374 - acc: 0.7499 - val_loss: 0.3819 - val_acc: 0.6911\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.1216 - acc: 0.7527 - val_loss: 0.3713 - val_acc: 0.6957\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.1138 - acc: 0.7540 - val_loss: 0.3854 - val_acc: 0.6957\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.1078 - acc: 0.7548 - val_loss: 0.3911 - val_acc: 0.6958\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.1041 - acc: 0.7553 - val_loss: 0.3982 - val_acc: 0.6950\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.1019 - acc: 0.7555 - val_loss: 0.4159 - val_acc: 0.6944\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.1006 - acc: 0.7555 - val_loss: 0.4066 - val_acc: 0.6967\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0992 - acc: 0.7556 - val_loss: 0.4219 - val_acc: 0.6971\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 89us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4343 - val_acc: 0.6972\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0982 - acc: 0.7556 - val_loss: 0.4327 - val_acc: 0.6956\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4477 - val_acc: 0.6962\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4487 - val_acc: 0.6968\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4591 - val_acc: 0.6966\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4629 - val_acc: 0.6968\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4727 - val_acc: 0.6968\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4763 - val_acc: 0.6965\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4775 - val_acc: 0.6966\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.4846 - val_acc: 0.6969\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 88us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.4831 - val_acc: 0.6967\n",
      "17922/17922 [==============================] - 1s 46us/sample - loss: 1.1908 - acc: 0.6038\n",
      "35842/35842 [==============================] - 2s 48us/sample - loss: 0.0967 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 8s 225us/sample - loss: 0.2662 - acc: 0.7225 - val_loss: 0.3843 - val_acc: 0.6899\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.1432 - acc: 0.7557 - val_loss: 0.3521 - val_acc: 0.6998\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1245 - acc: 0.7595 - val_loss: 0.3295 - val_acc: 0.7087\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1140 - acc: 0.7616 - val_loss: 0.3226 - val_acc: 0.7111\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1083 - acc: 0.7626 - val_loss: 0.3266 - val_acc: 0.7128\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.1036 - acc: 0.7632 - val_loss: 0.3193 - val_acc: 0.7125\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1012 - acc: 0.7634 - val_loss: 0.3271 - val_acc: 0.7132\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0991 - acc: 0.7635 - val_loss: 0.3322 - val_acc: 0.7138\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0979 - acc: 0.7635 - val_loss: 0.3348 - val_acc: 0.7139\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0974 - acc: 0.7635 - val_loss: 0.3384 - val_acc: 0.7138\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0966 - acc: 0.7635 - val_loss: 0.3482 - val_acc: 0.7142\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0962 - acc: 0.7636 - val_loss: 0.3513 - val_acc: 0.7144\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3568 - val_acc: 0.7143\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3532 - val_acc: 0.7142\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.3564 - val_acc: 0.7143\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.3682 - val_acc: 0.7145\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3692 - val_acc: 0.7142\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3734 - val_acc: 0.7143\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3757 - val_acc: 0.7145\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3834 - val_acc: 0.7145\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.7452 - acc: 0.6604\n",
      "35843/35843 [==============================] - 2s 48us/sample - loss: 0.0950 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 8s 219us/sample - loss: 0.2900 - acc: 0.6896 - val_loss: 0.3697 - val_acc: 0.6811\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1596 - acc: 0.7279 - val_loss: 0.3550 - val_acc: 0.6863\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1393 - acc: 0.7320 - val_loss: 0.3555 - val_acc: 0.6887\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1282 - acc: 0.7340 - val_loss: 0.3661 - val_acc: 0.6848\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1218 - acc: 0.7349 - val_loss: 0.3539 - val_acc: 0.6905\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1177 - acc: 0.7354 - val_loss: 0.3634 - val_acc: 0.6896\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1141 - acc: 0.7358 - val_loss: 0.3699 - val_acc: 0.6910\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1135 - acc: 0.7358 - val_loss: 0.3779 - val_acc: 0.6919\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1107 - acc: 0.7360 - val_loss: 0.3899 - val_acc: 0.6905\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1096 - acc: 0.7360 - val_loss: 0.3952 - val_acc: 0.6897\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1101 - acc: 0.7359 - val_loss: 0.4019 - val_acc: 0.6911\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1085 - acc: 0.7361 - val_loss: 0.4116 - val_acc: 0.6908\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1080 - acc: 0.7361 - val_loss: 0.4185 - val_acc: 0.6918\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4210 - val_acc: 0.6910\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1079 - acc: 0.7361 - val_loss: 0.4236 - val_acc: 0.6919\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4269 - val_acc: 0.6915\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4340 - val_acc: 0.6919\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4384 - val_acc: 0.6919\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4447 - val_acc: 0.6924\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4395 - val_acc: 0.6921\n",
      "17921/17921 [==============================] - 1s 48us/sample - loss: 0.7727 - acc: 0.6678\n",
      "35843/35843 [==============================] - 2s 45us/sample - loss: 0.1068 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 8s 227us/sample - loss: 0.2852 - acc: 0.7050 - val_loss: 0.3989 - val_acc: 0.6872\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.1393 - acc: 0.7495 - val_loss: 0.3792 - val_acc: 0.6928\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.1223 - acc: 0.7526 - val_loss: 0.3721 - val_acc: 0.6937\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1137 - acc: 0.7540 - val_loss: 0.3784 - val_acc: 0.6951\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.1083 - acc: 0.7549 - val_loss: 0.3849 - val_acc: 0.6963\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 85us/sample - loss: 0.1047 - acc: 0.7553 - val_loss: 0.3991 - val_acc: 0.6973\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.1025 - acc: 0.7554 - val_loss: 0.4026 - val_acc: 0.6962\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.1008 - acc: 0.7556 - val_loss: 0.4147 - val_acc: 0.6970\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0996 - acc: 0.7556 - val_loss: 0.4222 - val_acc: 0.6964\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0989 - acc: 0.7556 - val_loss: 0.4373 - val_acc: 0.6952\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.0988 - acc: 0.7556 - val_loss: 0.4432 - val_acc: 0.6967\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.4421 - val_acc: 0.6969\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4561 - val_acc: 0.6969\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4693 - val_acc: 0.6971\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4638 - val_acc: 0.6968\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 86us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4771 - val_acc: 0.6962\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.4734 - val_acc: 0.6973\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4842 - val_acc: 0.6971\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4844 - val_acc: 0.6971\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 87us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4953 - val_acc: 0.6972\n",
      "17922/17922 [==============================] - 1s 46us/sample - loss: 1.2029 - acc: 0.6075\n",
      "35842/35842 [==============================] - 2s 46us/sample - loss: 0.0968 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 8s 229us/sample - loss: 0.2937 - acc: 0.7131 - val_loss: 0.3749 - val_acc: 0.6897\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1489 - acc: 0.7548 - val_loss: 0.3546 - val_acc: 0.6976\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1291 - acc: 0.7585 - val_loss: 0.3392 - val_acc: 0.7047\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1178 - acc: 0.7610 - val_loss: 0.3202 - val_acc: 0.7107\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1105 - acc: 0.7623 - val_loss: 0.3210 - val_acc: 0.7115\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1057 - acc: 0.7630 - val_loss: 0.3234 - val_acc: 0.7128\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1027 - acc: 0.7633 - val_loss: 0.3298 - val_acc: 0.7128\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1009 - acc: 0.7634 - val_loss: 0.3265 - val_acc: 0.7133\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0991 - acc: 0.7635 - val_loss: 0.3394 - val_acc: 0.7134\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0981 - acc: 0.7635 - val_loss: 0.3446 - val_acc: 0.7141\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0974 - acc: 0.7635 - val_loss: 0.3499 - val_acc: 0.7137\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0968 - acc: 0.7636 - val_loss: 0.3579 - val_acc: 0.7139\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0965 - acc: 0.7636 - val_loss: 0.3584 - val_acc: 0.7142\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0962 - acc: 0.7636 - val_loss: 0.3605 - val_acc: 0.7140\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0959 - acc: 0.7636 - val_loss: 0.3708 - val_acc: 0.7137\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3700 - val_acc: 0.7142\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3737 - val_acc: 0.7142\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.3817 - val_acc: 0.7143\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.3826 - val_acc: 0.7142\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.3914 - val_acc: 0.7144\n",
      "17921/17921 [==============================] - 1s 46us/sample - loss: 0.7704 - acc: 0.6616\n",
      "35843/35843 [==============================] - 2s 46us/sample - loss: 0.0952 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 238us/sample - loss: 0.3115 - acc: 0.6826 - val_loss: 0.3715 - val_acc: 0.6793\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 85us/sample - loss: 0.1636 - acc: 0.7271 - val_loss: 0.3566 - val_acc: 0.6882\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1426 - acc: 0.7314 - val_loss: 0.3496 - val_acc: 0.6918\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1312 - acc: 0.7336 - val_loss: 0.3550 - val_acc: 0.6911\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1248 - acc: 0.7347 - val_loss: 0.3523 - val_acc: 0.6919\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1194 - acc: 0.7354 - val_loss: 0.3543 - val_acc: 0.6911\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1160 - acc: 0.7357 - val_loss: 0.3573 - val_acc: 0.6907\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1138 - acc: 0.7358 - val_loss: 0.3652 - val_acc: 0.6933\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1120 - acc: 0.7359 - val_loss: 0.3702 - val_acc: 0.6920\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 86us/sample - loss: 0.1112 - acc: 0.7360 - val_loss: 0.3808 - val_acc: 0.6894\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1100 - acc: 0.7360 - val_loss: 0.3914 - val_acc: 0.6941\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1104 - acc: 0.7360 - val_loss: 0.3899 - val_acc: 0.6907\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1087 - acc: 0.7361 - val_loss: 0.3935 - val_acc: 0.6928\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1083 - acc: 0.7361 - val_loss: 0.3999 - val_acc: 0.6918\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1079 - acc: 0.7361 - val_loss: 0.4093 - val_acc: 0.6914\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 89us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4134 - val_acc: 0.6907\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4146 - val_acc: 0.6923\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4267 - val_acc: 0.6922\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 88us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4287 - val_acc: 0.6924\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 87us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4283 - val_acc: 0.6929\n",
      "17921/17921 [==============================] - 1s 45us/sample - loss: 0.7727 - acc: 0.6674\n",
      "35843/35843 [==============================] - 2s 47us/sample - loss: 0.1073 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 9s 245us/sample - loss: 0.2341 - acc: 0.7206 - val_loss: 0.3835 - val_acc: 0.6914\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1247 - acc: 0.7514 - val_loss: 0.3774 - val_acc: 0.6943\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1112 - acc: 0.7535 - val_loss: 0.3904 - val_acc: 0.6936\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1050 - acc: 0.7546 - val_loss: 0.4178 - val_acc: 0.6938\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1010 - acc: 0.7552 - val_loss: 0.4509 - val_acc: 0.6939\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1004 - acc: 0.7553 - val_loss: 0.4579 - val_acc: 0.6929\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.4674 - val_acc: 0.6930\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 98us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4845 - val_acc: 0.6940\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4934 - val_acc: 0.6942\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5055 - val_acc: 0.6942\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5130 - val_acc: 0.6944\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 98us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5174 - val_acc: 0.6945\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5187 - val_acc: 0.6945\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5218 - val_acc: 0.6946\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5310 - val_acc: 0.6947\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5289 - val_acc: 0.6949\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5336 - val_acc: 0.6948\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5363 - val_acc: 0.6949\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5362 - val_acc: 0.6948\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5385 - val_acc: 0.6950\n",
      "17922/17922 [==============================] - 1s 50us/sample - loss: 1.3890 - acc: 0.5993\n",
      "35842/35842 [==============================] - 2s 52us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 255us/sample - loss: 0.2424 - acc: 0.7276 - val_loss: 0.3683 - val_acc: 0.6931\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1311 - acc: 0.7575 - val_loss: 0.3317 - val_acc: 0.7037\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1142 - acc: 0.7609 - val_loss: 0.3259 - val_acc: 0.7080\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1055 - acc: 0.7624 - val_loss: 0.3456 - val_acc: 0.7091\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1010 - acc: 0.7631 - val_loss: 0.3688 - val_acc: 0.7093\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0984 - acc: 0.7633 - val_loss: 0.3713 - val_acc: 0.7091\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0997 - acc: 0.7631 - val_loss: 0.3788 - val_acc: 0.7095\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0971 - acc: 0.7634 - val_loss: 0.3850 - val_acc: 0.7104\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.0963 - acc: 0.7635 - val_loss: 0.3984 - val_acc: 0.7111\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.3990 - val_acc: 0.7117\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0953 - acc: 0.7635 - val_loss: 0.4040 - val_acc: 0.7119\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4129 - val_acc: 0.7119\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4150 - val_acc: 0.7115\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4115 - val_acc: 0.7118\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4171 - val_acc: 0.7119\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4196 - val_acc: 0.7121\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4215 - val_acc: 0.7120\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4222 - val_acc: 0.7121\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4258 - val_acc: 0.7122\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4276 - val_acc: 0.7123\n",
      "17921/17921 [==============================] - 1s 57us/sample - loss: 0.8843 - acc: 0.6578\n",
      "35843/35843 [==============================] - 2s 59us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 488us/sample - loss: 0.2581 - acc: 0.6980 - val_loss: 0.3624 - val_acc: 0.6815\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 146us/sample - loss: 0.1457 - acc: 0.7297 - val_loss: 0.3477 - val_acc: 0.6907\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1274 - acc: 0.7329 - val_loss: 0.3785 - val_acc: 0.6853\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1183 - acc: 0.7345 - val_loss: 0.3745 - val_acc: 0.6868\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1132 - acc: 0.7354 - val_loss: 0.4069 - val_acc: 0.6848\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1111 - acc: 0.7356 - val_loss: 0.4344 - val_acc: 0.6844\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1096 - acc: 0.7358 - val_loss: 0.4337 - val_acc: 0.6857\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1081 - acc: 0.7360 - val_loss: 0.4354 - val_acc: 0.6883\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4622 - val_acc: 0.6877\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4571 - val_acc: 0.6870\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4660 - val_acc: 0.6886\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4695 - val_acc: 0.6872\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4649 - val_acc: 0.6881\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4800 - val_acc: 0.6886\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.4891 - val_acc: 0.6889\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.4890 - val_acc: 0.6893\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.4910 - val_acc: 0.6890\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.4920 - val_acc: 0.6889\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.4933 - val_acc: 0.6896\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5145 - val_acc: 0.6964\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5170 - val_acc: 0.6965\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5246 - val_acc: 0.6966\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5276 - val_acc: 0.6964\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5267 - val_acc: 0.6965\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5360 - val_acc: 0.6966\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5344 - val_acc: 0.6963\n",
      "17922/17922 [==============================] - 1s 50us/sample - loss: 1.3686 - acc: 0.6017\n",
      "35842/35842 [==============================] - 2s 50us/sample - loss: 0.0966 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 243us/sample - loss: 0.2614 - acc: 0.7197 - val_loss: 0.3629 - val_acc: 0.6924\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1323 - acc: 0.7576 - val_loss: 0.3345 - val_acc: 0.7070\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1145 - acc: 0.7611 - val_loss: 0.3131 - val_acc: 0.7101\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1061 - acc: 0.7625 - val_loss: 0.3343 - val_acc: 0.7105\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1010 - acc: 0.7631 - val_loss: 0.3411 - val_acc: 0.7100\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0987 - acc: 0.7633 - val_loss: 0.3511 - val_acc: 0.7106\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0972 - acc: 0.7635 - val_loss: 0.3762 - val_acc: 0.7115\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0963 - acc: 0.7635 - val_loss: 0.3808 - val_acc: 0.7113\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3849 - val_acc: 0.7118\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.3974 - val_acc: 0.7120\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4022 - val_acc: 0.7122\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4087 - val_acc: 0.7122\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4141 - val_acc: 0.7123\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4137 - val_acc: 0.7126\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4173 - val_acc: 0.7126\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4199 - val_acc: 0.7125\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4228 - val_acc: 0.7126\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4258 - val_acc: 0.7126\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4225 - val_acc: 0.7128\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4242 - val_acc: 0.7125\n",
      "17921/17921 [==============================] - 1s 50us/sample - loss: 0.8667 - acc: 0.6590\n",
      "35843/35843 [==============================] - 2s 50us/sample - loss: 0.0948 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 244us/sample - loss: 0.2806 - acc: 0.6904 - val_loss: 0.3649 - val_acc: 0.6804\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1469 - acc: 0.7299 - val_loss: 0.3501 - val_acc: 0.6888\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1283 - acc: 0.7332 - val_loss: 0.3760 - val_acc: 0.6826\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1220 - acc: 0.7341 - val_loss: 0.3796 - val_acc: 0.6853\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1145 - acc: 0.7353 - val_loss: 0.3838 - val_acc: 0.6848\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1111 - acc: 0.7358 - val_loss: 0.3955 - val_acc: 0.6879\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1093 - acc: 0.7359 - val_loss: 0.3969 - val_acc: 0.6884\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4188 - val_acc: 0.6871\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4328 - val_acc: 0.6856\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4303 - val_acc: 0.6874\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4422 - val_acc: 0.6878\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4484 - val_acc: 0.6875\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4461 - val_acc: 0.6880\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4440 - val_acc: 0.6863\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4748 - val_acc: 0.6839\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1110 - acc: 0.7355 - val_loss: 0.4778 - val_acc: 0.6778\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1154 - acc: 0.7346 - val_loss: 0.4612 - val_acc: 0.6845\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1094 - acc: 0.7358 - val_loss: 0.4359 - val_acc: 0.6847\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1083 - acc: 0.7359 - val_loss: 0.4524 - val_acc: 0.6863\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4667 - val_acc: 0.6856\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.8256 - acc: 0.6539\n",
      "35843/35843 [==============================] - 2s 52us/sample - loss: 0.1067 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 9s 249us/sample - loss: 0.2870 - acc: 0.7000 - val_loss: 0.3888 - val_acc: 0.6858\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1298 - acc: 0.7510 - val_loss: 0.3745 - val_acc: 0.6946\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1141 - acc: 0.7536 - val_loss: 0.3825 - val_acc: 0.6955\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1086 - acc: 0.7545 - val_loss: 0.3999 - val_acc: 0.6946\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1030 - acc: 0.7552 - val_loss: 0.4166 - val_acc: 0.6952\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1006 - acc: 0.7554 - val_loss: 0.4386 - val_acc: 0.6947\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0989 - acc: 0.7556 - val_loss: 0.4445 - val_acc: 0.6957\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0981 - acc: 0.7556 - val_loss: 0.4621 - val_acc: 0.6956\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4696 - val_acc: 0.6956\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4793 - val_acc: 0.6954\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4894 - val_acc: 0.6956\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4982 - val_acc: 0.6962\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.4962 - val_acc: 0.6963\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5052 - val_acc: 0.6961\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5119 - val_acc: 0.6969\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5145 - val_acc: 0.6968\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5210 - val_acc: 0.6969\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5246 - val_acc: 0.6971\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5361 - val_acc: 0.6974\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5192 - val_acc: 0.6965\n",
      "17922/17922 [==============================] - 1s 50us/sample - loss: 1.3083 - acc: 0.6021\n",
      "35842/35842 [==============================] - 2s 51us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 258us/sample - loss: 0.2938 - acc: 0.7091 - val_loss: 0.3763 - val_acc: 0.6889\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1384 - acc: 0.7566 - val_loss: 0.3499 - val_acc: 0.6982\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1213 - acc: 0.7596 - val_loss: 0.3442 - val_acc: 0.7052\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1124 - acc: 0.7616 - val_loss: 0.3482 - val_acc: 0.7104\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1043 - acc: 0.7629 - val_loss: 0.3478 - val_acc: 0.7111\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1008 - acc: 0.7633 - val_loss: 0.3513 - val_acc: 0.7111\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0986 - acc: 0.7635 - val_loss: 0.3586 - val_acc: 0.7117\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0975 - acc: 0.7635 - val_loss: 0.3744 - val_acc: 0.7118\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0968 - acc: 0.7635 - val_loss: 0.3795 - val_acc: 0.7121\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0962 - acc: 0.7636 - val_loss: 0.3872 - val_acc: 0.7125\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0960 - acc: 0.7636 - val_loss: 0.3898 - val_acc: 0.7128\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0958 - acc: 0.7636 - val_loss: 0.3977 - val_acc: 0.7126\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.4042 - val_acc: 0.7128\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4064 - val_acc: 0.7130\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4153 - val_acc: 0.7132\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4130 - val_acc: 0.7131\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4250 - val_acc: 0.7131\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4180 - val_acc: 0.7131\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4181 - val_acc: 0.7133\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0975 - acc: 0.7632 - val_loss: 0.4146 - val_acc: 0.7002\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.7513 - acc: 0.6516\n",
      "35843/35843 [==============================] - 2s 52us/sample - loss: 0.1109 - acc: 0.7608\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 252us/sample - loss: 0.3093 - acc: 0.6786 - val_loss: 0.3657 - val_acc: 0.6838\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1511 - acc: 0.7297 - val_loss: 0.3572 - val_acc: 0.6863\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1322 - acc: 0.7331 - val_loss: 0.3876 - val_acc: 0.6802\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1232 - acc: 0.7346 - val_loss: 0.3892 - val_acc: 0.6842\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1167 - acc: 0.7355 - val_loss: 0.3838 - val_acc: 0.6912\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1133 - acc: 0.7358 - val_loss: 0.3944 - val_acc: 0.6866\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1117 - acc: 0.7359 - val_loss: 0.3972 - val_acc: 0.6872\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1097 - acc: 0.7360 - val_loss: 0.4091 - val_acc: 0.6875\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1086 - acc: 0.7361 - val_loss: 0.4235 - val_acc: 0.6883\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1080 - acc: 0.7361 - val_loss: 0.4290 - val_acc: 0.6862\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1076 - acc: 0.7361 - val_loss: 0.4373 - val_acc: 0.6878\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4456 - val_acc: 0.6865\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4443 - val_acc: 0.6874\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4451 - val_acc: 0.6886\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4512 - val_acc: 0.6882\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4676 - val_acc: 0.6865\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4532 - val_acc: 0.6875\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4564 - val_acc: 0.6868\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4533 - val_acc: 0.6861\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4583 - val_acc: 0.6874\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.8287 - acc: 0.6601\n",
      "35843/35843 [==============================] - 2s 51us/sample - loss: 0.1066 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 9s 253us/sample - loss: 0.2348 - acc: 0.7202 - val_loss: 0.3920 - val_acc: 0.6923\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1270 - acc: 0.7510 - val_loss: 0.3712 - val_acc: 0.6926\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1121 - acc: 0.7535 - val_loss: 0.3944 - val_acc: 0.6946\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1067 - acc: 0.7544 - val_loss: 0.4200 - val_acc: 0.6938\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 98us/sample - loss: 0.1014 - acc: 0.7552 - val_loss: 0.4374 - val_acc: 0.6921\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0994 - acc: 0.7554 - val_loss: 0.4539 - val_acc: 0.6951\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0981 - acc: 0.7555 - val_loss: 0.4706 - val_acc: 0.6943\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4783 - val_acc: 0.6939\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4946 - val_acc: 0.6946\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 98us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5072 - val_acc: 0.6944\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5115 - val_acc: 0.6947\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5145 - val_acc: 0.6945\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5224 - val_acc: 0.6946\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5283 - val_acc: 0.6949\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5283 - val_acc: 0.6948\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5340 - val_acc: 0.6951\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5358 - val_acc: 0.6951\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5417 - val_acc: 0.6951\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5417 - val_acc: 0.6953\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.0963 - acc: 0.7556 - val_loss: 0.5424 - val_acc: 0.6954\n",
      "17922/17922 [==============================] - 1s 52us/sample - loss: 1.4237 - acc: 0.5974\n",
      "35842/35842 [==============================] - 2s 51us/sample - loss: 0.0963 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 258us/sample - loss: 0.2461 - acc: 0.7271 - val_loss: 0.4010 - val_acc: 0.6934\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1355 - acc: 0.7566 - val_loss: 0.3232 - val_acc: 0.7067\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1160 - acc: 0.7605 - val_loss: 0.3339 - val_acc: 0.7086\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1072 - acc: 0.7621 - val_loss: 0.3395 - val_acc: 0.7093\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1019 - acc: 0.7629 - val_loss: 0.3491 - val_acc: 0.7106\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0997 - acc: 0.7632 - val_loss: 0.3522 - val_acc: 0.7110\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0974 - acc: 0.7634 - val_loss: 0.3634 - val_acc: 0.7118\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0963 - acc: 0.7635 - val_loss: 0.3748 - val_acc: 0.7122\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.3823 - val_acc: 0.7122\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0953 - acc: 0.7635 - val_loss: 0.3894 - val_acc: 0.7124\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.3946 - val_acc: 0.7127\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.3953 - val_acc: 0.7125\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4037 - val_acc: 0.7124\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4068 - val_acc: 0.7127\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4074 - val_acc: 0.7125\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4124 - val_acc: 0.7128\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4149 - val_acc: 0.7129\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4156 - val_acc: 0.7128\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4200 - val_acc: 0.7129\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4210 - val_acc: 0.7129\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.8938 - acc: 0.6577\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 255us/sample - loss: 0.2627 - acc: 0.6971 - val_loss: 0.3812 - val_acc: 0.6764\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1469 - acc: 0.7296 - val_loss: 0.3622 - val_acc: 0.6846\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1285 - acc: 0.7329 - val_loss: 0.3845 - val_acc: 0.6819\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1199 - acc: 0.7343 - val_loss: 0.4061 - val_acc: 0.6779\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1207 - acc: 0.7339 - val_loss: 0.4068 - val_acc: 0.6840\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1128 - acc: 0.7354 - val_loss: 0.4187 - val_acc: 0.6852\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1101 - acc: 0.7357 - val_loss: 0.4483 - val_acc: 0.6832\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1092 - acc: 0.7358 - val_loss: 0.4462 - val_acc: 0.6878\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1079 - acc: 0.7360 - val_loss: 0.4487 - val_acc: 0.6863\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1074 - acc: 0.7360 - val_loss: 0.4653 - val_acc: 0.6852\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4726 - val_acc: 0.6865\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4808 - val_acc: 0.6855\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4824 - val_acc: 0.6866\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4923 - val_acc: 0.6868\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4953 - val_acc: 0.6868\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4957 - val_acc: 0.6871\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5039 - val_acc: 0.6864\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5044 - val_acc: 0.6866\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5063 - val_acc: 0.6871\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5093 - val_acc: 0.6870\n",
      "17921/17921 [==============================] - 1s 53us/sample - loss: 0.9171 - acc: 0.6572\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 9s 263us/sample - loss: 0.2628 - acc: 0.7095 - val_loss: 0.3913 - val_acc: 0.6882\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1292 - acc: 0.7510 - val_loss: 0.3848 - val_acc: 0.6902\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1139 - acc: 0.7535 - val_loss: 0.3833 - val_acc: 0.6927\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1061 - acc: 0.7547 - val_loss: 0.4263 - val_acc: 0.6926\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1019 - acc: 0.7552 - val_loss: 0.4217 - val_acc: 0.6958\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0996 - acc: 0.7554 - val_loss: 0.4431 - val_acc: 0.6949\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0982 - acc: 0.7556 - val_loss: 0.4623 - val_acc: 0.6948\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4675 - val_acc: 0.6951\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4787 - val_acc: 0.6953\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4886 - val_acc: 0.6956\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.4952 - val_acc: 0.6959\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5024 - val_acc: 0.6954\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5077 - val_acc: 0.6956\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5087 - val_acc: 0.6955\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5199 - val_acc: 0.6955\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5237 - val_acc: 0.6958\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5283 - val_acc: 0.6956\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5275 - val_acc: 0.6955\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5308 - val_acc: 0.6958\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5306 - val_acc: 0.6957\n",
      "17922/17922 [==============================] - 1s 51us/sample - loss: 1.3668 - acc: 0.6001\n",
      "35842/35842 [==============================] - 2s 51us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 254us/sample - loss: 0.2642 - acc: 0.7211 - val_loss: 0.3663 - val_acc: 0.6918\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1360 - acc: 0.7570 - val_loss: 0.3339 - val_acc: 0.7041\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1175 - acc: 0.7606 - val_loss: 0.3277 - val_acc: 0.7088\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1077 - acc: 0.7623 - val_loss: 0.3517 - val_acc: 0.7091\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1026 - acc: 0.7630 - val_loss: 0.3485 - val_acc: 0.7105\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0994 - acc: 0.7633 - val_loss: 0.3559 - val_acc: 0.7117\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0977 - acc: 0.7634 - val_loss: 0.3811 - val_acc: 0.7114\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3918 - val_acc: 0.7112\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0976 - acc: 0.7634 - val_loss: 0.3921 - val_acc: 0.7113\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3996 - val_acc: 0.7118\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0956 - acc: 0.7635 - val_loss: 0.4034 - val_acc: 0.7122\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0954 - acc: 0.7635 - val_loss: 0.4071 - val_acc: 0.7121\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4116 - val_acc: 0.7119\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4116 - val_acc: 0.7120\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4162 - val_acc: 0.7120\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4184 - val_acc: 0.7122\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4238 - val_acc: 0.7121\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4272 - val_acc: 0.7124\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4258 - val_acc: 0.7124\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4294 - val_acc: 0.7123\n",
      "17921/17921 [==============================] - 1s 50us/sample - loss: 0.8651 - acc: 0.6584\n",
      "35843/35843 [==============================] - 2s 51us/sample - loss: 0.0948 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 260us/sample - loss: 0.2818 - acc: 0.6902 - val_loss: 0.3677 - val_acc: 0.6829\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1502 - acc: 0.7295 - val_loss: 0.3556 - val_acc: 0.6868\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1307 - acc: 0.7329 - val_loss: 0.3575 - val_acc: 0.6910\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1213 - acc: 0.7344 - val_loss: 0.3681 - val_acc: 0.6895\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1152 - acc: 0.7353 - val_loss: 0.4020 - val_acc: 0.6876\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1120 - acc: 0.7357 - val_loss: 0.4225 - val_acc: 0.6864\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1097 - acc: 0.7359 - val_loss: 0.4305 - val_acc: 0.6894\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1101 - acc: 0.7358 - val_loss: 0.4418 - val_acc: 0.6856\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4331 - val_acc: 0.6877\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1077 - acc: 0.7360 - val_loss: 0.4339 - val_acc: 0.6891\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4374 - val_acc: 0.6899\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4543 - val_acc: 0.6887\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4543 - val_acc: 0.6891\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4561 - val_acc: 0.6892\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4597 - val_acc: 0.6897\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4604 - val_acc: 0.6899\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4659 - val_acc: 0.6901\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4666 - val_acc: 0.6897\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4697 - val_acc: 0.6896\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4707 - val_acc: 0.6902\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.8402 - acc: 0.6669\n",
      "35843/35843 [==============================] - 2s 52us/sample - loss: 0.1064 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 9s 257us/sample - loss: 0.2902 - acc: 0.7008 - val_loss: 0.3954 - val_acc: 0.6874\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1336 - acc: 0.7507 - val_loss: 0.3811 - val_acc: 0.6940\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1173 - acc: 0.7534 - val_loss: 0.3953 - val_acc: 0.6934\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1096 - acc: 0.7546 - val_loss: 0.4156 - val_acc: 0.6963\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1049 - acc: 0.7552 - val_loss: 0.4131 - val_acc: 0.6971\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1021 - acc: 0.7554 - val_loss: 0.4321 - val_acc: 0.6951\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1005 - acc: 0.7555 - val_loss: 0.4476 - val_acc: 0.6963\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0994 - acc: 0.7556 - val_loss: 0.4631 - val_acc: 0.6966\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4632 - val_acc: 0.6967\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.4741 - val_acc: 0.6978\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4824 - val_acc: 0.6972\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4914 - val_acc: 0.6968\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4904 - val_acc: 0.6977\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.5016 - val_acc: 0.6977\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5090 - val_acc: 0.6975\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5096 - val_acc: 0.6978\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5164 - val_acc: 0.6978\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5170 - val_acc: 0.6975\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5209 - val_acc: 0.6982\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5264 - val_acc: 0.6986\n",
      "17922/17922 [==============================] - 1s 49us/sample - loss: 1.3507 - acc: 0.6089\n",
      "35842/35842 [==============================] - 2s 51us/sample - loss: 0.0969 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 264us/sample - loss: 0.2998 - acc: 0.7080 - val_loss: 0.3744 - val_acc: 0.6931\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1395 - acc: 0.7563 - val_loss: 0.3472 - val_acc: 0.6992\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1221 - acc: 0.7597 - val_loss: 0.3246 - val_acc: 0.7085\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1116 - acc: 0.7619 - val_loss: 0.3272 - val_acc: 0.7108\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1048 - acc: 0.7629 - val_loss: 0.3363 - val_acc: 0.7117\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1013 - acc: 0.7633 - val_loss: 0.3385 - val_acc: 0.7118\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0992 - acc: 0.7634 - val_loss: 0.3496 - val_acc: 0.7119\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0981 - acc: 0.7635 - val_loss: 0.3564 - val_acc: 0.7124\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0973 - acc: 0.7635 - val_loss: 0.3723 - val_acc: 0.7127\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0968 - acc: 0.7635 - val_loss: 0.3632 - val_acc: 0.7124\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3774 - val_acc: 0.7122\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0960 - acc: 0.7636 - val_loss: 0.3863 - val_acc: 0.7127\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0958 - acc: 0.7636 - val_loss: 0.3884 - val_acc: 0.7126\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0960 - acc: 0.7636 - val_loss: 0.3962 - val_acc: 0.7124\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0956 - acc: 0.7636 - val_loss: 0.3996 - val_acc: 0.7124\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.3988 - val_acc: 0.7122\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4029 - val_acc: 0.7123\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4071 - val_acc: 0.7131\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4146 - val_acc: 0.7127\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4081 - val_acc: 0.7130\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.8174 - acc: 0.6574\n",
      "35843/35843 [==============================] - 2s 51us/sample - loss: 0.0951 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 9s 261us/sample - loss: 0.3088 - acc: 0.6818 - val_loss: 0.3663 - val_acc: 0.6823\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1543 - acc: 0.7292 - val_loss: 0.3619 - val_acc: 0.6849\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1351 - acc: 0.7327 - val_loss: 0.3623 - val_acc: 0.6894\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1245 - acc: 0.7345 - val_loss: 0.3805 - val_acc: 0.6818\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1193 - acc: 0.7352 - val_loss: 0.3808 - val_acc: 0.6885\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1141 - acc: 0.7358 - val_loss: 0.3895 - val_acc: 0.6861\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1116 - acc: 0.7359 - val_loss: 0.4039 - val_acc: 0.6881\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1105 - acc: 0.7360 - val_loss: 0.4165 - val_acc: 0.6871\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1094 - acc: 0.7360 - val_loss: 0.4139 - val_acc: 0.6872\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1086 - acc: 0.7361 - val_loss: 0.4206 - val_acc: 0.6875\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1082 - acc: 0.7361 - val_loss: 0.4326 - val_acc: 0.6881\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1078 - acc: 0.7361 - val_loss: 0.4332 - val_acc: 0.6883\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4408 - val_acc: 0.6891\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4391 - val_acc: 0.6891\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4468 - val_acc: 0.6895\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4542 - val_acc: 0.6880\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1080 - acc: 0.7360 - val_loss: 0.4383 - val_acc: 0.6865\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1138 - acc: 0.7350 - val_loss: 0.4368 - val_acc: 0.6807\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1174 - acc: 0.7344 - val_loss: 0.4464 - val_acc: 0.6741\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1110 - acc: 0.7357 - val_loss: 0.4393 - val_acc: 0.6847\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.7782 - acc: 0.6520\n",
      "35843/35843 [==============================] - 2s 51us/sample - loss: 0.1083 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 274us/sample - loss: 0.2386 - acc: 0.7194 - val_loss: 0.3932 - val_acc: 0.6858\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1282 - acc: 0.7508 - val_loss: 0.3782 - val_acc: 0.6940\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1142 - acc: 0.7531 - val_loss: 0.3971 - val_acc: 0.6896\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1081 - acc: 0.7541 - val_loss: 0.4201 - val_acc: 0.6926\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1026 - acc: 0.7550 - val_loss: 0.4509 - val_acc: 0.6920\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1000 - acc: 0.7554 - val_loss: 0.4652 - val_acc: 0.6930\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0987 - acc: 0.7555 - val_loss: 0.4842 - val_acc: 0.6944\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4915 - val_acc: 0.6941\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5093 - val_acc: 0.6945\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5240 - val_acc: 0.6946\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5241 - val_acc: 0.6947\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5351 - val_acc: 0.6947\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5388 - val_acc: 0.6948\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5485 - val_acc: 0.6948\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5490 - val_acc: 0.6950\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5550 - val_acc: 0.6948\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5604 - val_acc: 0.6950\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5551 - val_acc: 0.6949\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5578 - val_acc: 0.6947\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5621 - val_acc: 0.6950\n",
      "17922/17922 [==============================] - 1s 50us/sample - loss: 1.4292 - acc: 0.5990\n",
      "35842/35842 [==============================] - 2s 52us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 277us/sample - loss: 0.2493 - acc: 0.7269 - val_loss: 0.3640 - val_acc: 0.6933\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1355 - acc: 0.7566 - val_loss: 0.3650 - val_acc: 0.7025\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1193 - acc: 0.7599 - val_loss: 0.3269 - val_acc: 0.7076\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1083 - acc: 0.7619 - val_loss: 0.3332 - val_acc: 0.7092\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1031 - acc: 0.7627 - val_loss: 0.3477 - val_acc: 0.7098\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0998 - acc: 0.7632 - val_loss: 0.3568 - val_acc: 0.7107\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0978 - acc: 0.7634 - val_loss: 0.3702 - val_acc: 0.7104\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.3847 - val_acc: 0.7110\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0961 - acc: 0.7635 - val_loss: 0.3774 - val_acc: 0.7110\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0955 - acc: 0.7635 - val_loss: 0.3883 - val_acc: 0.7115\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0952 - acc: 0.7635 - val_loss: 0.3977 - val_acc: 0.7115\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4048 - val_acc: 0.7118\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4105 - val_acc: 0.7115\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4102 - val_acc: 0.7117\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4128 - val_acc: 0.7115\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4158 - val_acc: 0.7117 0s - loss: 0.0948 - \n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4188 - val_acc: 0.7119\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4245 - val_acc: 0.7118\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4202 - val_acc: 0.7118\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0947 - acc: 0.7636 - val_loss: 0.4287 - val_acc: 0.7120\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.9054 - acc: 0.6572\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.0947 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 266us/sample - loss: 0.2626 - acc: 0.6977 - val_loss: 0.3641 - val_acc: 0.6821\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1500 - acc: 0.7290 - val_loss: 0.3594 - val_acc: 0.6870\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1310 - acc: 0.7325 - val_loss: 0.3750 - val_acc: 0.6830\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1215 - acc: 0.7340 - val_loss: 0.3803 - val_acc: 0.6864\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1155 - acc: 0.7350 - val_loss: 0.4046 - val_acc: 0.6838\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1120 - acc: 0.7355 - val_loss: 0.4070 - val_acc: 0.6882\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1099 - acc: 0.7358 - val_loss: 0.4247 - val_acc: 0.6877\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1086 - acc: 0.7359 - val_loss: 0.4376 - val_acc: 0.6882\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1078 - acc: 0.7360 - val_loss: 0.4546 - val_acc: 0.6877\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1081 - acc: 0.7360 - val_loss: 0.4748 - val_acc: 0.6847\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4802 - val_acc: 0.6867\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4637 - val_acc: 0.6864\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4816 - val_acc: 0.6880\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4881 - val_acc: 0.6859\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4937 - val_acc: 0.6871\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.4999 - val_acc: 0.6872\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5019 - val_acc: 0.6873\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5038 - val_acc: 0.6870\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5049 - val_acc: 0.6871\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5071 - val_acc: 0.6871\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.9298 - acc: 0.6572\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.1063 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 270us/sample - loss: 0.2670 - acc: 0.7094 - val_loss: 0.3923 - val_acc: 0.6907\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.1312 - acc: 0.7507 - val_loss: 0.3783 - val_acc: 0.6934\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1152 - acc: 0.7533 - val_loss: 0.3926 - val_acc: 0.6934\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1079 - acc: 0.7544 - val_loss: 0.4041 - val_acc: 0.6935\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1036 - acc: 0.7551 - val_loss: 0.4376 - val_acc: 0.6967\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1008 - acc: 0.7554 - val_loss: 0.4519 - val_acc: 0.6951\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0992 - acc: 0.7555 - val_loss: 0.4578 - val_acc: 0.6955\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0984 - acc: 0.7556 - val_loss: 0.4826 - val_acc: 0.6954\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4985 - val_acc: 0.6956\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.5049 - val_acc: 0.6959\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.5165 - val_acc: 0.6964\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5272 - val_acc: 0.6959\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5325 - val_acc: 0.6968\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5405 - val_acc: 0.6965\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5479 - val_acc: 0.6968\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5454 - val_acc: 0.6968\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5555 - val_acc: 0.6966\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5536 - val_acc: 0.6965\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5658 - val_acc: 0.6970\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5642 - val_acc: 0.6972\n",
      "17922/17922 [==============================] - 1s 51us/sample - loss: 1.4687 - acc: 0.6058\n",
      "35842/35842 [==============================] - 2s 52us/sample - loss: 0.0965 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 267us/sample - loss: 0.2735 - acc: 0.7170 - val_loss: 0.3767 - val_acc: 0.6912\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1384 - acc: 0.7564 - val_loss: 0.3396 - val_acc: 0.7025\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1193 - acc: 0.7604 - val_loss: 0.3327 - val_acc: 0.7081\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1094 - acc: 0.7621 - val_loss: 0.3413 - val_acc: 0.7083\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1041 - acc: 0.7628 - val_loss: 0.3465 - val_acc: 0.7098\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1003 - acc: 0.7633 - val_loss: 0.3640 - val_acc: 0.7103\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0983 - acc: 0.7634 - val_loss: 0.3700 - val_acc: 0.7107\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0969 - acc: 0.7635 - val_loss: 0.3892 - val_acc: 0.7107\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3935 - val_acc: 0.7111\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.4059 - val_acc: 0.7110\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0954 - acc: 0.7636 - val_loss: 0.4096 - val_acc: 0.7115\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4135 - val_acc: 0.7113\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4211 - val_acc: 0.7112\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4269 - val_acc: 0.7114\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4283 - val_acc: 0.7116\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4297 - val_acc: 0.7112\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4333 - val_acc: 0.7115\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4361 - val_acc: 0.7118\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4383 - val_acc: 0.7114\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4479 - val_acc: 0.7119\n",
      "17921/17921 [==============================] - 1s 53us/sample - loss: 0.9344 - acc: 0.6570\n",
      "35843/35843 [==============================] - 2s 52us/sample - loss: 0.0949 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 267us/sample - loss: 0.2864 - acc: 0.6890 - val_loss: 0.3673 - val_acc: 0.6835\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1512 - acc: 0.7292 - val_loss: 0.3661 - val_acc: 0.6815\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1322 - acc: 0.7326 - val_loss: 0.3626 - val_acc: 0.6879\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1224 - acc: 0.7342 - val_loss: 0.3663 - val_acc: 0.6899\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1163 - acc: 0.7352 - val_loss: 0.3871 - val_acc: 0.6889\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1126 - acc: 0.7357 - val_loss: 0.4146 - val_acc: 0.6871\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1107 - acc: 0.7359 - val_loss: 0.4205 - val_acc: 0.6899\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1091 - acc: 0.7360 - val_loss: 0.4218 - val_acc: 0.6895\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1101 - acc: 0.7358 - val_loss: 0.4401 - val_acc: 0.6881\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1083 - acc: 0.7360 - val_loss: 0.4581 - val_acc: 0.6875\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4603 - val_acc: 0.6888\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4633 - val_acc: 0.6888\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4752 - val_acc: 0.6906\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4985 - val_acc: 0.6871\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1080 - acc: 0.7360 - val_loss: 0.4642 - val_acc: 0.6877\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1146 - acc: 0.7347 - val_loss: 0.4725 - val_acc: 0.6837\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1115 - acc: 0.7353 - val_loss: 0.4549 - val_acc: 0.6889\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1083 - acc: 0.7359 - val_loss: 0.4764 - val_acc: 0.6864\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4930 - val_acc: 0.6871\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4964 - val_acc: 0.6885\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.8943 - acc: 0.6611\n",
      "35843/35843 [==============================] - 2s 52us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 271us/sample - loss: 0.2848 - acc: 0.7016 - val_loss: 0.4069 - val_acc: 0.6898\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1330 - acc: 0.7507 - val_loss: 0.3792 - val_acc: 0.6920\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1177 - acc: 0.7533 - val_loss: 0.3907 - val_acc: 0.6951\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1097 - acc: 0.7545 - val_loss: 0.4008 - val_acc: 0.6966\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1054 - acc: 0.7551 - val_loss: 0.4256 - val_acc: 0.6948\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1023 - acc: 0.7554 - val_loss: 0.4386 - val_acc: 0.6952\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1004 - acc: 0.7556 - val_loss: 0.4493 - val_acc: 0.6958\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0992 - acc: 0.7556 - val_loss: 0.4717 - val_acc: 0.6962\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0988 - acc: 0.7556 - val_loss: 0.4768 - val_acc: 0.6966\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0982 - acc: 0.7556 - val_loss: 0.4847 - val_acc: 0.6966\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4936 - val_acc: 0.6966\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4969 - val_acc: 0.6968\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.5108 - val_acc: 0.6967\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.5138 - val_acc: 0.6966\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0971 - acc: 0.7556 - val_loss: 0.5143 - val_acc: 0.6970\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5260 - val_acc: 0.6970\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5301 - val_acc: 0.6969\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5346 - val_acc: 0.6963\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5415 - val_acc: 0.6970\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5465 - val_acc: 0.6970\n",
      "17922/17922 [==============================] - 1s 51us/sample - loss: 1.3822 - acc: 0.6052\n",
      "35842/35842 [==============================] - 2s 52us/sample - loss: 0.0969 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 268us/sample - loss: 0.3047 - acc: 0.7079 - val_loss: 0.3799 - val_acc: 0.6913\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.1432 - acc: 0.7559 - val_loss: 0.3646 - val_acc: 0.6957\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1236 - acc: 0.7596 - val_loss: 0.3339 - val_acc: 0.7079\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1129 - acc: 0.7618 - val_loss: 0.3361 - val_acc: 0.7109\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1070 - acc: 0.7628 - val_loss: 0.3544 - val_acc: 0.7056\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1067 - acc: 0.7626 - val_loss: 0.3478 - val_acc: 0.7109\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1012 - acc: 0.7633 - val_loss: 0.3524 - val_acc: 0.7114\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0993 - acc: 0.7634 - val_loss: 0.3611 - val_acc: 0.7106\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0983 - acc: 0.7635 - val_loss: 0.3715 - val_acc: 0.7110\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0975 - acc: 0.7635 - val_loss: 0.3796 - val_acc: 0.7113\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0969 - acc: 0.7635 - val_loss: 0.3883 - val_acc: 0.7107\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0972 - acc: 0.7635 - val_loss: 0.3912 - val_acc: 0.7111\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0963 - acc: 0.7636 - val_loss: 0.3917 - val_acc: 0.7116\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0960 - acc: 0.7636 - val_loss: 0.4046 - val_acc: 0.7111\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.4034 - val_acc: 0.7112\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0957 - acc: 0.7636 - val_loss: 0.4081 - val_acc: 0.7111\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.4159 - val_acc: 0.7115\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4257 - val_acc: 0.7113\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4258 - val_acc: 0.7112\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4327 - val_acc: 0.7115\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.8731 - acc: 0.6569\n",
      "35843/35843 [==============================] - 2s 51us/sample - loss: 0.0952 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 272us/sample - loss: 0.3244 - acc: 0.6736 - val_loss: 0.3889 - val_acc: 0.6842\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1577 - acc: 0.7282 - val_loss: 0.3506 - val_acc: 0.6877\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1356 - acc: 0.7323 - val_loss: 0.3667 - val_acc: 0.6877\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1244 - acc: 0.7342 - val_loss: 0.3618 - val_acc: 0.6875\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1183 - acc: 0.7351 - val_loss: 0.3842 - val_acc: 0.6882\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1144 - acc: 0.7355 - val_loss: 0.3940 - val_acc: 0.6888\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1117 - acc: 0.7358 - val_loss: 0.4037 - val_acc: 0.6858\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1102 - acc: 0.7359 - val_loss: 0.4222 - val_acc: 0.6859\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1094 - acc: 0.7360 - val_loss: 0.4343 - val_acc: 0.6832\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1089 - acc: 0.7360 - val_loss: 0.4317 - val_acc: 0.6860\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4336 - val_acc: 0.6873\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1078 - acc: 0.7361 - val_loss: 0.4496 - val_acc: 0.6872\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4527 - val_acc: 0.6870\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1074 - acc: 0.7361 - val_loss: 0.4554 - val_acc: 0.6873\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4663 - val_acc: 0.6873\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4593 - val_acc: 0.6878\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4582 - val_acc: 0.6865\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1071 - acc: 0.7361 - val_loss: 0.4667 - val_acc: 0.6863\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4660 - val_acc: 0.6844\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1176 - acc: 0.7342 - val_loss: 0.4980 - val_acc: 0.6721\n",
      "17921/17921 [==============================] - 1s 54us/sample - loss: 0.8469 - acc: 0.6335\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.1197 - acc: 0.7340\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 275us/sample - loss: 0.2438 - acc: 0.7183 - val_loss: 0.3862 - val_acc: 0.6881\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1301 - acc: 0.7507 - val_loss: 0.3718 - val_acc: 0.6943\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1157 - acc: 0.7530 - val_loss: 0.3772 - val_acc: 0.6941\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1080 - acc: 0.7542 - val_loss: 0.4062 - val_acc: 0.6943\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1037 - acc: 0.7549 - val_loss: 0.4243 - val_acc: 0.6924\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.1010 - acc: 0.7552 - val_loss: 0.4383 - val_acc: 0.6938\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0995 - acc: 0.7554 - val_loss: 0.4618 - val_acc: 0.6929\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0986 - acc: 0.7555 - val_loss: 0.4679 - val_acc: 0.6947\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.4790 - val_acc: 0.6946\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4941 - val_acc: 0.6946\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5041 - val_acc: 0.6942\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5161 - val_acc: 0.6946\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5208 - val_acc: 0.6947\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5270 - val_acc: 0.6949\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5301 - val_acc: 0.6942\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5216 - val_acc: 0.6948\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0965 - acc: 0.7556 - val_loss: 0.5338 - val_acc: 0.6947\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5378 - val_acc: 0.6950\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5401 - val_acc: 0.6949\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0964 - acc: 0.7556 - val_loss: 0.5440 - val_acc: 0.6953\n",
      "17922/17922 [==============================] - 1s 52us/sample - loss: 1.4146 - acc: 0.5993\n",
      "35842/35842 [==============================] - 2s 52us/sample - loss: 0.0964 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 292us/sample - loss: 0.2497 - acc: 0.7266 - val_loss: 0.3739 - val_acc: 0.6927\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1354 - acc: 0.7567 - val_loss: 0.3428 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1182 - acc: 0.7602 - val_loss: 0.3295 - val_acc: 0.7073\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1086 - acc: 0.7619 - val_loss: 0.3444 - val_acc: 0.7092\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1029 - acc: 0.7629 - val_loss: 0.3578 - val_acc: 0.7105\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0999 - acc: 0.7632 - val_loss: 0.3632 - val_acc: 0.7110\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0977 - acc: 0.7634 - val_loss: 0.3753 - val_acc: 0.7105\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 91us/sample - loss: 0.0965 - acc: 0.7635 - val_loss: 0.3860 - val_acc: 0.7116\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 90us/sample - loss: 0.0959 - acc: 0.7635 - val_loss: 0.3924 - val_acc: 0.7116\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.0956 - acc: 0.7635 - val_loss: 0.4081 - val_acc: 0.7121\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4141 - val_acc: 0.7121\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4171 - val_acc: 0.7123\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4252 - val_acc: 0.7123\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4285 - val_acc: 0.7125\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4282 - val_acc: 0.7123\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4321 - val_acc: 0.7123\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4342 - val_acc: 0.7125\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0948 - acc: 0.7636 - val_loss: 0.4338 - val_acc: 0.7122\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1020 - acc: 0.7624 - val_loss: 0.4020 - val_acc: 0.7050\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1027 - acc: 0.7623 - val_loss: 0.4189 - val_acc: 0.7089\n",
      "17921/17921 [==============================] - 1s 52us/sample - loss: 0.9071 - acc: 0.6539\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.0992 - acc: 0.7630\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 288us/sample - loss: 0.2691 - acc: 0.6960 - val_loss: 0.3699 - val_acc: 0.6845\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1522 - acc: 0.7288 - val_loss: 0.3544 - val_acc: 0.6888\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1340 - acc: 0.7320 - val_loss: 0.3696 - val_acc: 0.6857\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1227 - acc: 0.7338 - val_loss: 0.3830 - val_acc: 0.6862\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1167 - acc: 0.7348 - val_loss: 0.4162 - val_acc: 0.6810\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1127 - acc: 0.7354 - val_loss: 0.4149 - val_acc: 0.6851\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1103 - acc: 0.7357 - val_loss: 0.4268 - val_acc: 0.6826\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1090 - acc: 0.7359 - val_loss: 0.4549 - val_acc: 0.6864\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4505 - val_acc: 0.6863\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1076 - acc: 0.7360 - val_loss: 0.4754 - val_acc: 0.6869\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4786 - val_acc: 0.6866\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1069 - acc: 0.7361 - val_loss: 0.4843 - val_acc: 0.6867\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4833 - val_acc: 0.6849\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4999 - val_acc: 0.6856\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.5046 - val_acc: 0.6858\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5054 - val_acc: 0.6863\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1064 - acc: 0.7361 - val_loss: 0.5140 - val_acc: 0.6858\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5146 - val_acc: 0.6861\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5163 - val_acc: 0.6859\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1063 - acc: 0.7361 - val_loss: 0.5241 - val_acc: 0.6858\n",
      "17921/17921 [==============================] - 1s 51us/sample - loss: 0.9529 - acc: 0.6557\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.1063 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 284us/sample - loss: 0.2598 - acc: 0.7121 - val_loss: 0.3892 - val_acc: 0.6866\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.1342 - acc: 0.7502 - val_loss: 0.3757 - val_acc: 0.6932\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1172 - acc: 0.7530 - val_loss: 0.3834 - val_acc: 0.6942\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1092 - acc: 0.7542 - val_loss: 0.4195 - val_acc: 0.6931\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.1052 - acc: 0.7548 - val_loss: 0.4115 - val_acc: 0.6937\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.1014 - acc: 0.7553 - val_loss: 0.4234 - val_acc: 0.6947\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0996 - acc: 0.7555 - val_loss: 0.4514 - val_acc: 0.6941\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0985 - acc: 0.7556 - val_loss: 0.4623 - val_acc: 0.6943\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.0979 - acc: 0.7556 - val_loss: 0.4750 - val_acc: 0.6939\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.4873 - val_acc: 0.6945\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0974 - acc: 0.7556 - val_loss: 0.4913 - val_acc: 0.6939\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5014 - val_acc: 0.6945\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5044 - val_acc: 0.6939\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5106 - val_acc: 0.6944\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5131 - val_acc: 0.6947\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0968 - acc: 0.7556 - val_loss: 0.5203 - val_acc: 0.6946\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 95us/sample - loss: 0.0967 - acc: 0.7556 - val_loss: 0.5235 - val_acc: 0.6945\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5305 - val_acc: 0.6950\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5232 - val_acc: 0.6946\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 96us/sample - loss: 0.0966 - acc: 0.7556 - val_loss: 0.5316 - val_acc: 0.6948\n",
      "17922/17922 [==============================] - 1s 53us/sample - loss: 1.3397 - acc: 0.5994\n",
      "35842/35842 [==============================] - 2s 52us/sample - loss: 0.0966 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 281us/sample - loss: 0.2675 - acc: 0.7200 - val_loss: 0.3766 - val_acc: 0.6915\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1387 - acc: 0.7564 - val_loss: 0.3400 - val_acc: 0.7013\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1206 - acc: 0.7600 - val_loss: 0.3319 - val_acc: 0.7047\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1109 - acc: 0.7617 - val_loss: 0.3348 - val_acc: 0.7058\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1052 - acc: 0.7626 - val_loss: 0.3614 - val_acc: 0.7095\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1013 - acc: 0.7631 - val_loss: 0.3521 - val_acc: 0.7105\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0997 - acc: 0.7633 - val_loss: 0.3667 - val_acc: 0.7105\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0978 - acc: 0.7634 - val_loss: 0.3691 - val_acc: 0.7105\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0969 - acc: 0.7635 - val_loss: 0.3774 - val_acc: 0.7108\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3872 - val_acc: 0.7109\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0960 - acc: 0.7635 - val_loss: 0.3944 - val_acc: 0.7110\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0956 - acc: 0.7635 - val_loss: 0.4002 - val_acc: 0.7114\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.0955 - acc: 0.7636 - val_loss: 0.4045 - val_acc: 0.7116\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0953 - acc: 0.7636 - val_loss: 0.4094 - val_acc: 0.7118\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.0952 - acc: 0.7636 - val_loss: 0.4094 - val_acc: 0.7114\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0951 - acc: 0.7636 - val_loss: 0.4180 - val_acc: 0.7116\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4178 - val_acc: 0.7118\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0950 - acc: 0.7636 - val_loss: 0.4213 - val_acc: 0.7115\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4199 - val_acc: 0.7116\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.0949 - acc: 0.7636 - val_loss: 0.4257 - val_acc: 0.7119\n",
      "17921/17921 [==============================] - 1s 53us/sample - loss: 0.8249 - acc: 0.6593\n",
      "35843/35843 [==============================] - 2s 54us/sample - loss: 0.0948 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 285us/sample - loss: 0.2813 - acc: 0.6911 - val_loss: 0.3596 - val_acc: 0.6835\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1545 - acc: 0.7287 - val_loss: 0.3676 - val_acc: 0.6844\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1352 - acc: 0.7321 - val_loss: 0.3556 - val_acc: 0.6850\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1254 - acc: 0.7337 - val_loss: 0.3611 - val_acc: 0.6908\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1182 - acc: 0.7349 - val_loss: 0.4004 - val_acc: 0.6852\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1143 - acc: 0.7354 - val_loss: 0.4005 - val_acc: 0.6878\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1116 - acc: 0.7357 - val_loss: 0.4326 - val_acc: 0.6825\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1103 - acc: 0.7358 - val_loss: 0.4298 - val_acc: 0.6868\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1090 - acc: 0.7360 - val_loss: 0.4360 - val_acc: 0.6872\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1097 - acc: 0.7358 - val_loss: 0.4415 - val_acc: 0.6861\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1080 - acc: 0.7360 - val_loss: 0.4554 - val_acc: 0.6874\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4650 - val_acc: 0.6872\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4598 - val_acc: 0.6882\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1070 - acc: 0.7361 - val_loss: 0.4716 - val_acc: 0.6880\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1068 - acc: 0.7361 - val_loss: 0.4749 - val_acc: 0.6888\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1067 - acc: 0.7361 - val_loss: 0.4734 - val_acc: 0.6893\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4800 - val_acc: 0.6892\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 92us/sample - loss: 0.1066 - acc: 0.7361 - val_loss: 0.4837 - val_acc: 0.6892\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4884 - val_acc: 0.6889\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1065 - acc: 0.7361 - val_loss: 0.4936 - val_acc: 0.6890\n",
      "17921/17921 [==============================] - 1s 55us/sample - loss: 0.9021 - acc: 0.6617\n",
      "35843/35843 [==============================] - 2s 54us/sample - loss: 0.1065 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 10s 288us/sample - loss: 0.2932 - acc: 0.7000 - val_loss: 0.3966 - val_acc: 0.6881\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 90us/sample - loss: 0.1367 - acc: 0.7500 - val_loss: 0.3870 - val_acc: 0.6932\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1206 - acc: 0.7530 - val_loss: 0.3913 - val_acc: 0.6919\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1124 - acc: 0.7543 - val_loss: 0.3956 - val_acc: 0.6959\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.1073 - acc: 0.7550 - val_loss: 0.4083 - val_acc: 0.6970\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1036 - acc: 0.7554 - val_loss: 0.4102 - val_acc: 0.6962\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.1015 - acc: 0.7555 - val_loss: 0.4441 - val_acc: 0.6950\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0999 - acc: 0.7556 - val_loss: 0.4435 - val_acc: 0.6960\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0990 - acc: 0.7556 - val_loss: 0.4584 - val_acc: 0.6956\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 94us/sample - loss: 0.0984 - acc: 0.7556 - val_loss: 0.4730 - val_acc: 0.6957\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0981 - acc: 0.7556 - val_loss: 0.4811 - val_acc: 0.6955\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.4786 - val_acc: 0.6949\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0975 - acc: 0.7556 - val_loss: 0.4881 - val_acc: 0.6953\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.4968 - val_acc: 0.6953\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5024 - val_acc: 0.6947\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.4982 - val_acc: 0.6958\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.4996 - val_acc: 0.6943\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 91us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5056 - val_acc: 0.6950\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 92us/sample - loss: 0.0969 - acc: 0.7556 - val_loss: 0.5068 - val_acc: 0.6954\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 93us/sample - loss: 0.0970 - acc: 0.7556 - val_loss: 0.5140 - val_acc: 0.6950\n",
      "17922/17922 [==============================] - 1s 51us/sample - loss: 1.2905 - acc: 0.6033\n",
      "35842/35842 [==============================] - 2s 51us/sample - loss: 0.0969 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 10s 292us/sample - loss: 0.3177 - acc: 0.6775 - val_loss: 0.3681 - val_acc: 0.6832\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1609 - acc: 0.7279 - val_loss: 0.3551 - val_acc: 0.6859\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1407 - acc: 0.7318 - val_loss: 0.3535 - val_acc: 0.6898\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1301 - acc: 0.7338 - val_loss: 0.3605 - val_acc: 0.6877\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1235 - acc: 0.7348 - val_loss: 0.3599 - val_acc: 0.6920\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1183 - acc: 0.7355 - val_loss: 0.3780 - val_acc: 0.6874\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1151 - acc: 0.7358 - val_loss: 0.3861 - val_acc: 0.6905\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1126 - acc: 0.7360 - val_loss: 0.4052 - val_acc: 0.6877\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1111 - acc: 0.7360 - val_loss: 0.4106 - val_acc: 0.6878\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1100 - acc: 0.7361 - val_loss: 0.4117 - val_acc: 0.6897\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1092 - acc: 0.7361 - val_loss: 0.4272 - val_acc: 0.6875\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1087 - acc: 0.7361 - val_loss: 0.4277 - val_acc: 0.6904\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1081 - acc: 0.7361 - val_loss: 0.4370 - val_acc: 0.6890\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 95us/sample - loss: 0.1078 - acc: 0.7361 - val_loss: 0.4379 - val_acc: 0.6894\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1075 - acc: 0.7361 - val_loss: 0.4475 - val_acc: 0.6898\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1073 - acc: 0.7361 - val_loss: 0.4479 - val_acc: 0.6903\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1072 - acc: 0.7361 - val_loss: 0.4554 - val_acc: 0.6893\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1095 - acc: 0.7358 - val_loss: 0.4580 - val_acc: 0.6853\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 94us/sample - loss: 0.1160 - acc: 0.7348 - val_loss: 0.4460 - val_acc: 0.6794\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1097 - acc: 0.7359 - val_loss: 0.4402 - val_acc: 0.6865\n",
      "17921/17921 [==============================] - 1s 54us/sample - loss: 0.7853 - acc: 0.6584\n",
      "35843/35843 [==============================] - 2s 53us/sample - loss: 0.1083 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 11s 305us/sample - loss: 0.2250 - acc: 0.7253 - val_loss: 0.3834 - val_acc: 0.6869\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1358 - acc: 0.7511 - val_loss: 0.3800 - val_acc: 0.6972\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1221 - acc: 0.7535 - val_loss: 0.3789 - val_acc: 0.6971\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1155 - acc: 0.7547 - val_loss: 0.3849 - val_acc: 0.6975\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1115 - acc: 0.7552 - val_loss: 0.3884 - val_acc: 0.6985\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1091 - acc: 0.7554 - val_loss: 0.3907 - val_acc: 0.6987\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1063 - acc: 0.7556 - val_loss: 0.3927 - val_acc: 0.6954\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1060 - acc: 0.7556 - val_loss: 0.3966 - val_acc: 0.6998\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1040 - acc: 0.7556 - val_loss: 0.3987 - val_acc: 0.6984\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1031 - acc: 0.7556 - val_loss: 0.4080 - val_acc: 0.6997\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1025 - acc: 0.7556 - val_loss: 0.4093 - val_acc: 0.7003\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1020 - acc: 0.7556 - val_loss: 0.4060 - val_acc: 0.6995\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1011 - acc: 0.7556 - val_loss: 0.4176 - val_acc: 0.7000\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1010 - acc: 0.7556 - val_loss: 0.4139 - val_acc: 0.6998\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1004 - acc: 0.7556 - val_loss: 0.4221 - val_acc: 0.6992\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1002 - acc: 0.7556 - val_loss: 0.4426 - val_acc: 0.7003\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1009 - acc: 0.7556 - val_loss: 0.4257 - val_acc: 0.6987\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1006 - acc: 0.7556 - val_loss: 0.4369 - val_acc: 0.7000\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1005 - acc: 0.7556 - val_loss: 0.4389 - val_acc: 0.6988\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1008 - acc: 0.7556 - val_loss: 0.4185 - val_acc: 0.6996\n",
      "17922/17922 [==============================] - 1s 59us/sample - loss: 0.9623 - acc: 0.6182\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.1000 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 11s 314us/sample - loss: 0.2274 - acc: 0.7338 - val_loss: 0.3556 - val_acc: 0.6954\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1374 - acc: 0.7581 - val_loss: 0.3340 - val_acc: 0.7087\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1225 - acc: 0.7612 - val_loss: 0.3260 - val_acc: 0.7074\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1181 - acc: 0.7622 - val_loss: 0.3323 - val_acc: 0.7129\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1119 - acc: 0.7630 - val_loss: 0.3329 - val_acc: 0.7135\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1079 - acc: 0.7633 - val_loss: 0.3383 - val_acc: 0.7141\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1060 - acc: 0.7634 - val_loss: 0.3367 - val_acc: 0.7117\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1052 - acc: 0.7635 - val_loss: 0.3512 - val_acc: 0.7141\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1032 - acc: 0.7635 - val_loss: 0.3472 - val_acc: 0.7136\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1024 - acc: 0.7635 - val_loss: 0.3512 - val_acc: 0.7140\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1014 - acc: 0.7636 - val_loss: 0.3583 - val_acc: 0.7135\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1014 - acc: 0.7636 - val_loss: 0.3632 - val_acc: 0.7149\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1005 - acc: 0.7636 - val_loss: 0.3604 - val_acc: 0.7138\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.0998 - acc: 0.7636 - val_loss: 0.3593 - val_acc: 0.7141\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.0992 - acc: 0.7636 - val_loss: 0.3598 - val_acc: 0.7145\n",
      "Epoch 16/20\n",
      "24576/35843 [===================>..........] - ETA: 1s - loss: 0.0990 - acc: 0.7634"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1086 - acc: 0.7555 - val_loss: 0.3872 - val_acc: 0.6978\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1074 - acc: 0.7555 - val_loss: 0.3942 - val_acc: 0.7008\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1054 - acc: 0.7556 - val_loss: 0.3966 - val_acc: 0.7000\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1047 - acc: 0.7556 - val_loss: 0.3999 - val_acc: 0.6997\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1032 - acc: 0.7556 - val_loss: 0.4024 - val_acc: 0.6968\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1037 - acc: 0.7556 - val_loss: 0.4104 - val_acc: 0.7003\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1033 - acc: 0.7556 - val_loss: 0.4066 - val_acc: 0.6995\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1016 - acc: 0.7556 - val_loss: 0.4138 - val_acc: 0.6985\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1011 - acc: 0.7556 - val_loss: 0.4327 - val_acc: 0.7024\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1015 - acc: 0.7556 - val_loss: 0.4205 - val_acc: 0.7004\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1005 - acc: 0.7556 - val_loss: 0.4262 - val_acc: 0.7000\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1004 - acc: 0.7556 - val_loss: 0.4378 - val_acc: 0.7012\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1007 - acc: 0.7556 - val_loss: 0.4253 - val_acc: 0.6979\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1001 - acc: 0.7556 - val_loss: 0.4324 - val_acc: 0.7000\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1003 - acc: 0.7556 - val_loss: 0.4215 - val_acc: 0.6987\n",
      "17922/17922 [==============================] - 1s 57us/sample - loss: 0.9714 - acc: 0.6143\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.1000 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 11s 313us/sample - loss: 0.2441 - acc: 0.7282 - val_loss: 0.3662 - val_acc: 0.6933\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1410 - acc: 0.7571 - val_loss: 0.3414 - val_acc: 0.7029\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1254 - acc: 0.7605 - val_loss: 0.3228 - val_acc: 0.7095\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1174 - acc: 0.7621 - val_loss: 0.3301 - val_acc: 0.7127\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1117 - acc: 0.7630 - val_loss: 0.3267 - val_acc: 0.7139\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1087 - acc: 0.7633 - val_loss: 0.3324 - val_acc: 0.7141\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1178 - acc: 0.7360 - val_loss: 0.3698 - val_acc: 0.6926\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1163 - acc: 0.7360 - val_loss: 0.3694 - val_acc: 0.6949\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1156 - acc: 0.7361 - val_loss: 0.3725 - val_acc: 0.6936\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1147 - acc: 0.7361 - val_loss: 0.3785 - val_acc: 0.6928\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1140 - acc: 0.7361 - val_loss: 0.3828 - val_acc: 0.6958\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1139 - acc: 0.7361 - val_loss: 0.3766 - val_acc: 0.6931\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1130 - acc: 0.7361 - val_loss: 0.3800 - val_acc: 0.6932\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1138 - acc: 0.7361 - val_loss: 0.3929 - val_acc: 0.6897\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1128 - acc: 0.7361 - val_loss: 0.3854 - val_acc: 0.6941\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1122 - acc: 0.7361 - val_loss: 0.3855 - val_acc: 0.6941\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1126 - acc: 0.7361 - val_loss: 0.3905 - val_acc: 0.6927\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.6503 - acc: 0.6735\n",
      "35843/35843 [==============================] - 2s 57us/sample - loss: 0.1167 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 12s 332us/sample - loss: 0.2450 - acc: 0.7177 - val_loss: 0.3956 - val_acc: 0.6910\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1366 - acc: 0.7509 - val_loss: 0.3724 - val_acc: 0.6921\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1255 - acc: 0.7531 - val_loss: 0.3803 - val_acc: 0.6946\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1194 - acc: 0.7542 - val_loss: 0.3882 - val_acc: 0.6978\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1139 - acc: 0.7549 - val_loss: 0.3815 - val_acc: 0.6964\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1120 - acc: 0.7552 - val_loss: 0.3956 - val_acc: 0.6985\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1092 - acc: 0.7554 - val_loss: 0.3892 - val_acc: 0.6992\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1088 - acc: 0.7555 - val_loss: 0.3902 - val_acc: 0.6982\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1064 - acc: 0.7556 - val_loss: 0.4105 - val_acc: 0.6927\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1068 - acc: 0.7555 - val_loss: 0.4030 - val_acc: 0.6983\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1045 - acc: 0.7556 - val_loss: 0.4110 - val_acc: 0.6992\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1038 - acc: 0.7556 - val_loss: 0.4246 - val_acc: 0.7010\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1033 - acc: 0.7556 - val_loss: 0.4123 - val_acc: 0.6991\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1030 - acc: 0.7556 - val_loss: 0.4197 - val_acc: 0.6995\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1022 - acc: 0.7556 - val_loss: 0.4260 - val_acc: 0.6990\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1023 - acc: 0.7556 - val_loss: 0.4256 - val_acc: 0.6990\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1016 - acc: 0.7556 - val_loss: 0.4224 - val_acc: 0.6994\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1011 - acc: 0.7556 - val_loss: 0.4279 - val_acc: 0.6985\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1007 - acc: 0.7556 - val_loss: 0.4234 - val_acc: 0.6984\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1007 - acc: 0.7556 - val_loss: 0.4326 - val_acc: 0.6988\n",
      "17922/17922 [==============================] - 1s 56us/sample - loss: 0.9941 - acc: 0.6167\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.1001 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 12s 331us/sample - loss: 0.2538 - acc: 0.7265 - val_loss: 0.3769 - val_acc: 0.6936\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1443 - acc: 0.7565 - val_loss: 0.3514 - val_acc: 0.7001\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1288 - acc: 0.7597 - val_loss: 0.3317 - val_acc: 0.7063\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1218 - acc: 0.7615 - val_loss: 0.3236 - val_acc: 0.7109\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1160 - acc: 0.7625 - val_loss: 0.3325 - val_acc: 0.7125\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1118 - acc: 0.7630 - val_loss: 0.3284 - val_acc: 0.7132\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1092 - acc: 0.7633 - val_loss: 0.3418 - val_acc: 0.7136\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1074 - acc: 0.7634 - val_loss: 0.3386 - val_acc: 0.7137\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1056 - acc: 0.7635 - val_loss: 0.3393 - val_acc: 0.7125\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1050 - acc: 0.7635 - val_loss: 0.3421 - val_acc: 0.7109\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1046 - acc: 0.7635 - val_loss: 0.3524 - val_acc: 0.7147\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1027 - acc: 0.7635 - val_loss: 0.3444 - val_acc: 0.7134\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1025 - acc: 0.7635 - val_loss: 0.3518 - val_acc: 0.7144\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1016 - acc: 0.7636 - val_loss: 0.3565 - val_acc: 0.7132\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1016 - acc: 0.7636 - val_loss: 0.3589 - val_acc: 0.7140\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1007 - acc: 0.7636 - val_loss: 0.3563 - val_acc: 0.7140\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1002 - acc: 0.7636 - val_loss: 0.3661 - val_acc: 0.7141\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.0999 - acc: 0.7636 - val_loss: 0.3602 - val_acc: 0.7139\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.0995 - acc: 0.7636 - val_loss: 0.3648 - val_acc: 0.7140\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0993 - acc: 0.7636 - val_loss: 0.3673 - val_acc: 0.7136\n",
      "17921/17921 [==============================] - 1s 57us/sample - loss: 0.7165 - acc: 0.6619\n",
      "35843/35843 [==============================] - 2s 57us/sample - loss: 0.0992 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 12s 333us/sample - loss: 0.2749 - acc: 0.6949 - val_loss: 0.3686 - val_acc: 0.6790\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1598 - acc: 0.7290 - val_loss: 0.3523 - val_acc: 0.6876\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1432 - acc: 0.7325 - val_loss: 0.3531 - val_acc: 0.6917\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1352 - acc: 0.7340 - val_loss: 0.3600 - val_acc: 0.6892\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1295 - acc: 0.7350 - val_loss: 0.3500 - val_acc: 0.6933\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1261 - acc: 0.7354 - val_loss: 0.3560 - val_acc: 0.6952\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1238 - acc: 0.7357 - val_loss: 0.3604 - val_acc: 0.6946\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1213 - acc: 0.7359 - val_loss: 0.3578 - val_acc: 0.6958\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1195 - acc: 0.7360 - val_loss: 0.3734 - val_acc: 0.6987\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1208 - acc: 0.7359 - val_loss: 0.3623 - val_acc: 0.6956\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1171 - acc: 0.7360 - val_loss: 0.3736 - val_acc: 0.6953\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1162 - acc: 0.7361 - val_loss: 0.3735 - val_acc: 0.6929\n",
      "Epoch 13/20\n",
      "21248/35843 [================>.............] - ETA: 1s - loss: 0.1153 - acc: 0.7362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1064 - acc: 0.7556 - val_loss: 0.3930 - val_acc: 0.6981\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1064 - acc: 0.7556 - val_loss: 0.4098 - val_acc: 0.6995\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1050 - acc: 0.7556 - val_loss: 0.4002 - val_acc: 0.6987\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1039 - acc: 0.7556 - val_loss: 0.4069 - val_acc: 0.6978\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1035 - acc: 0.7556 - val_loss: 0.4118 - val_acc: 0.6988\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1027 - acc: 0.7556 - val_loss: 0.4087 - val_acc: 0.6988\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1022 - acc: 0.7556 - val_loss: 0.4178 - val_acc: 0.6982\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1018 - acc: 0.7556 - val_loss: 0.4156 - val_acc: 0.6989\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1014 - acc: 0.7556 - val_loss: 0.4083 - val_acc: 0.6987\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1013 - acc: 0.7556 - val_loss: 0.4248 - val_acc: 0.6995\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1008 - acc: 0.7556 - val_loss: 0.4285 - val_acc: 0.6992\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1008 - acc: 0.7556 - val_loss: 0.4324 - val_acc: 0.6999\n",
      "17922/17922 [==============================] - 1s 57us/sample - loss: 0.9830 - acc: 0.6226\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.1027 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 12s 345us/sample - loss: 0.2727 - acc: 0.7191 - val_loss: 0.3753 - val_acc: 0.6914\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1460 - acc: 0.7562 - val_loss: 0.3487 - val_acc: 0.6989\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1315 - acc: 0.7591 - val_loss: 0.3435 - val_acc: 0.7040\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1226 - acc: 0.7611 - val_loss: 0.3337 - val_acc: 0.7103\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1165 - acc: 0.7623 - val_loss: 0.3350 - val_acc: 0.7114\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1129 - acc: 0.7628 - val_loss: 0.3321 - val_acc: 0.7125\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1105 - acc: 0.7631 - val_loss: 0.3326 - val_acc: 0.7116\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1086 - acc: 0.7633 - val_loss: 0.3402 - val_acc: 0.7138\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1068 - acc: 0.7634 - val_loss: 0.3473 - val_acc: 0.7072\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1090 - acc: 0.7632 - val_loss: 0.3477 - val_acc: 0.7139\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1048 - acc: 0.7635 - val_loss: 0.3492 - val_acc: 0.7135\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1038 - acc: 0.7635 - val_loss: 0.3585 - val_acc: 0.7140\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1031 - acc: 0.7635 - val_loss: 0.3537 - val_acc: 0.7131\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1027 - acc: 0.7635 - val_loss: 0.3504 - val_acc: 0.7137\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1017 - acc: 0.7636 - val_loss: 0.3660 - val_acc: 0.7131\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1022 - acc: 0.7635 - val_loss: 0.3558 - val_acc: 0.7137\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1011 - acc: 0.7636 - val_loss: 0.3657 - val_acc: 0.7141\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1006 - acc: 0.7636 - val_loss: 0.3641 - val_acc: 0.7128\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1010 - acc: 0.7635 - val_loss: 0.3661 - val_acc: 0.7135\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1001 - acc: 0.7636 - val_loss: 0.3593 - val_acc: 0.7134\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.6929 - acc: 0.6616\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1008 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 12s 337us/sample - loss: 0.2928 - acc: 0.6886 - val_loss: 0.3679 - val_acc: 0.6845\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1605 - acc: 0.7289 - val_loss: 0.3477 - val_acc: 0.6926\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1439 - acc: 0.7324 - val_loss: 0.3504 - val_acc: 0.6946\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1362 - acc: 0.7340 - val_loss: 0.3502 - val_acc: 0.6937\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1306 - acc: 0.7349 - val_loss: 0.3573 - val_acc: 0.6926\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1262 - acc: 0.7355 - val_loss: 0.3640 - val_acc: 0.6917\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1246 - acc: 0.7357 - val_loss: 0.3651 - val_acc: 0.6909\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1222 - acc: 0.7358 - val_loss: 0.3612 - val_acc: 0.6917\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1199 - acc: 0.7360 - val_loss: 0.3620 - val_acc: 0.6921\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1185 - acc: 0.7360 - val_loss: 0.3723 - val_acc: 0.6876\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1179 - acc: 0.7360 - val_loss: 0.3694 - val_acc: 0.6921\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1169 - acc: 0.7361 - val_loss: 0.3623 - val_acc: 0.6946\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1158 - acc: 0.7361 - val_loss: 0.3630 - val_acc: 0.6940\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 3s 93us/sample - loss: 0.1153 - acc: 0.7361 - val_loss: 0.3795 - val_acc: 0.6931\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1146 - acc: 0.7361 - val_loss: 0.3735 - val_acc: 0.6942\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1161 - acc: 0.7360 - val_loss: 0.3752 - val_acc: 0.6942\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1135 - acc: 0.7361 - val_loss: 0.3787 - val_acc: 0.6900\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1133 - acc: 0.7361 - val_loss: 0.3735 - val_acc: 0.6947\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1126 - acc: 0.7361 - val_loss: 0.3793 - val_acc: 0.6938\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1121 - acc: 0.7361 - val_loss: 0.3811 - val_acc: 0.6952\n",
      "17921/17921 [==============================] - 1s 57us/sample - loss: 0.6265 - acc: 0.6791\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1121 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 12s 344us/sample - loss: 0.2345 - acc: 0.7226 - val_loss: 0.4029 - val_acc: 0.6846\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1394 - acc: 0.7503 - val_loss: 0.3787 - val_acc: 0.6952\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1255 - acc: 0.7530 - val_loss: 0.3934 - val_acc: 0.6970\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1190 - acc: 0.7542 - val_loss: 0.3917 - val_acc: 0.6902\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1164 - acc: 0.7546 - val_loss: 0.3778 - val_acc: 0.6961\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1126 - acc: 0.7551 - val_loss: 0.3956 - val_acc: 0.6977\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1094 - acc: 0.7554 - val_loss: 0.3954 - val_acc: 0.6991\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1078 - acc: 0.7555 - val_loss: 0.4103 - val_acc: 0.6994\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1067 - acc: 0.7555 - val_loss: 0.3994 - val_acc: 0.6964\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1057 - acc: 0.7556 - val_loss: 0.3952 - val_acc: 0.6974\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1048 - acc: 0.7556 - val_loss: 0.4127 - val_acc: 0.6966\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1048 - acc: 0.7556 - val_loss: 0.3985 - val_acc: 0.6988\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1033 - acc: 0.7556 - val_loss: 0.4106 - val_acc: 0.6989\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1028 - acc: 0.7556 - val_loss: 0.4120 - val_acc: 0.6980\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1023 - acc: 0.7556 - val_loss: 0.4115 - val_acc: 0.7000\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1017 - acc: 0.7556 - val_loss: 0.4156 - val_acc: 0.6957\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1031 - acc: 0.7556 - val_loss: 0.4102 - val_acc: 0.6982\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1013 - acc: 0.7556 - val_loss: 0.4227 - val_acc: 0.6991\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1017 - acc: 0.7556 - val_loss: 0.4190 - val_acc: 0.6978\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1007 - acc: 0.7556 - val_loss: 0.4259 - val_acc: 0.6992\n",
      "17922/17922 [==============================] - 1s 60us/sample - loss: 0.9699 - acc: 0.6170\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.1005 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 12s 345us/sample - loss: 0.2464 - acc: 0.7290 - val_loss: 0.3789 - val_acc: 0.6876\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1452 - acc: 0.7564 - val_loss: 0.3433 - val_acc: 0.7040\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1288 - acc: 0.7599 - val_loss: 0.3323 - val_acc: 0.7088\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1209 - acc: 0.7616 - val_loss: 0.3352 - val_acc: 0.7113\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1159 - acc: 0.7624 - val_loss: 0.3311 - val_acc: 0.7122\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1118 - acc: 0.7630 - val_loss: 0.3332 - val_acc: 0.7124\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1088 - acc: 0.7633 - val_loss: 0.3410 - val_acc: 0.7131\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1082 - acc: 0.7633 - val_loss: 0.3416 - val_acc: 0.7136\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1058 - acc: 0.7634 - val_loss: 0.3554 - val_acc: 0.7143\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1046 - acc: 0.7635 - val_loss: 0.3535 - val_acc: 0.7139\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1038 - acc: 0.7635 - val_loss: 0.3529 - val_acc: 0.7133\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1026 - acc: 0.7635 - val_loss: 0.3596 - val_acc: 0.7137\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1020 - acc: 0.7635 - val_loss: 0.3544 - val_acc: 0.7131\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1023 - acc: 0.7635 - val_loss: 0.3590 - val_acc: 0.7131\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1011 - acc: 0.7635 - val_loss: 0.3664 - val_acc: 0.7138\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1004 - acc: 0.7635 - val_loss: 0.3708 - val_acc: 0.7138\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1001 - acc: 0.7636 - val_loss: 0.3716 - val_acc: 0.7141\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.0997 - acc: 0.7636 - val_loss: 0.3725 - val_acc: 0.7139\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0997 - acc: 0.7636 - val_loss: 0.3764 - val_acc: 0.7134\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0997 - acc: 0.7635 - val_loss: 0.3750 - val_acc: 0.7137\n",
      "17921/17921 [==============================] - 1s 60us/sample - loss: 0.7511 - acc: 0.6603\n",
      "35843/35843 [==============================] - 2s 60us/sample - loss: 0.0996 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 12s 347us/sample - loss: 0.2640 - acc: 0.6981 - val_loss: 0.3716 - val_acc: 0.6779\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1605 - acc: 0.7288 - val_loss: 0.3618 - val_acc: 0.6852\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1439 - acc: 0.7324 - val_loss: 0.3546 - val_acc: 0.6889\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1355 - acc: 0.7340 - val_loss: 0.3574 - val_acc: 0.6911\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1302 - acc: 0.7349 - val_loss: 0.3547 - val_acc: 0.6887\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1266 - acc: 0.7354 - val_loss: 0.3688 - val_acc: 0.6882\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1240 - acc: 0.7357 - val_loss: 0.3671 - val_acc: 0.6959\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1215 - acc: 0.7359 - val_loss: 0.3616 - val_acc: 0.6917\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1198 - acc: 0.7359 - val_loss: 0.3632 - val_acc: 0.6921\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1183 - acc: 0.7360 - val_loss: 0.3696 - val_acc: 0.6945\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1173 - acc: 0.7360 - val_loss: 0.3693 - val_acc: 0.6931\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1165 - acc: 0.7361 - val_loss: 0.3702 - val_acc: 0.6918\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1151 - acc: 0.7361 - val_loss: 0.3776 - val_acc: 0.6940\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1155 - acc: 0.7361 - val_loss: 0.3778 - val_acc: 0.6913\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1141 - acc: 0.7361 - val_loss: 0.3798 - val_acc: 0.6961\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1143 - acc: 0.7361 - val_loss: 0.3812 - val_acc: 0.6927\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1134 - acc: 0.7361 - val_loss: 0.3852 - val_acc: 0.6881\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1136 - acc: 0.7361 - val_loss: 0.3754 - val_acc: 0.6937\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1126 - acc: 0.7361 - val_loss: 0.3910 - val_acc: 0.6927\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1126 - acc: 0.7361 - val_loss: 0.3864 - val_acc: 0.6910\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.6463 - acc: 0.6696\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1134 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 12s 344us/sample - loss: 0.2456 - acc: 0.7189 - val_loss: 0.3950 - val_acc: 0.6884\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1395 - acc: 0.7504 - val_loss: 0.3858 - val_acc: 0.6969\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1263 - acc: 0.7529 - val_loss: 0.3937 - val_acc: 0.6965\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1199 - acc: 0.7541 - val_loss: 0.3874 - val_acc: 0.6970\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1148 - acc: 0.7548 - val_loss: 0.4009 - val_acc: 0.7003\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1141 - acc: 0.7549 - val_loss: 0.3957 - val_acc: 0.6940\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1115 - acc: 0.7553 - val_loss: 0.3918 - val_acc: 0.6945\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1094 - acc: 0.7554 - val_loss: 0.3940 - val_acc: 0.6984\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1069 - acc: 0.7555 - val_loss: 0.3995 - val_acc: 0.6980\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1058 - acc: 0.7556 - val_loss: 0.3993 - val_acc: 0.6988\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1051 - acc: 0.7556 - val_loss: 0.4084 - val_acc: 0.6989\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1041 - acc: 0.7556 - val_loss: 0.3996 - val_acc: 0.6977\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1054 - acc: 0.7556 - val_loss: 0.4131 - val_acc: 0.6970\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1039 - acc: 0.7556 - val_loss: 0.4005 - val_acc: 0.6966\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1042 - acc: 0.7556 - val_loss: 0.4097 - val_acc: 0.6986\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1026 - acc: 0.7556 - val_loss: 0.4196 - val_acc: 0.6985\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1031 - acc: 0.7556 - val_loss: 0.4172 - val_acc: 0.6993\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1019 - acc: 0.7556 - val_loss: 0.4197 - val_acc: 0.6976\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1016 - acc: 0.7556 - val_loss: 0.4222 - val_acc: 0.6992\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1009 - acc: 0.7556 - val_loss: 0.4250 - val_acc: 0.6982\n",
      "17922/17922 [==============================] - 1s 58us/sample - loss: 0.9683 - acc: 0.6168\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.1008 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 13s 356us/sample - loss: 0.2563 - acc: 0.7253 - val_loss: 0.3743 - val_acc: 0.6929\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1458 - acc: 0.7563 - val_loss: 0.3531 - val_acc: 0.7021\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1298 - acc: 0.7596 - val_loss: 0.3433 - val_acc: 0.7096\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1215 - acc: 0.7615 - val_loss: 0.3276 - val_acc: 0.7121\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1157 - acc: 0.7625 - val_loss: 0.3282 - val_acc: 0.7124\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1122 - acc: 0.7629 - val_loss: 0.3340 - val_acc: 0.7133\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1096 - acc: 0.7632 - val_loss: 0.3420 - val_acc: 0.7125\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1081 - acc: 0.7633 - val_loss: 0.3489 - val_acc: 0.7148\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1061 - acc: 0.7634 - val_loss: 0.3432 - val_acc: 0.7132\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1051 - acc: 0.7635 - val_loss: 0.3447 - val_acc: 0.7143\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1036 - acc: 0.7635 - val_loss: 0.3421 - val_acc: 0.7133\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1029 - acc: 0.7635 - val_loss: 0.3478 - val_acc: 0.7146\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1023 - acc: 0.7635 - val_loss: 0.3618 - val_acc: 0.7147\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1018 - acc: 0.7635 - val_loss: 0.3512 - val_acc: 0.7143\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1011 - acc: 0.7636 - val_loss: 0.3595 - val_acc: 0.7135\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1008 - acc: 0.7636 - val_loss: 0.3591 - val_acc: 0.7131\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1006 - acc: 0.7636 - val_loss: 0.3793 - val_acc: 0.7151\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1009 - acc: 0.7635 - val_loss: 0.3664 - val_acc: 0.7146\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.0998 - acc: 0.7636 - val_loss: 0.3649 - val_acc: 0.7144\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.0993 - acc: 0.7636 - val_loss: 0.3754 - val_acc: 0.7147\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.7281 - acc: 0.6638\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.0996 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 13s 353us/sample - loss: 0.2732 - acc: 0.6950 - val_loss: 0.3662 - val_acc: 0.6860\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1606 - acc: 0.7289 - val_loss: 0.3591 - val_acc: 0.6881\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1445 - acc: 0.7324 - val_loss: 0.3482 - val_acc: 0.6945\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1358 - acc: 0.7339 - val_loss: 0.3597 - val_acc: 0.6916\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1314 - acc: 0.7348 - val_loss: 0.3471 - val_acc: 0.6950\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1268 - acc: 0.7354 - val_loss: 0.3628 - val_acc: 0.6918\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1239 - acc: 0.7357 - val_loss: 0.3559 - val_acc: 0.6927\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1215 - acc: 0.7358 - val_loss: 0.3582 - val_acc: 0.6943\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1198 - acc: 0.7360 - val_loss: 0.3663 - val_acc: 0.6932\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1184 - acc: 0.7360 - val_loss: 0.3718 - val_acc: 0.6899\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1174 - acc: 0.7360 - val_loss: 0.3664 - val_acc: 0.6925\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1167 - acc: 0.7361 - val_loss: 0.3663 - val_acc: 0.6945\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1159 - acc: 0.7361 - val_loss: 0.3724 - val_acc: 0.6938\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1146 - acc: 0.7361 - val_loss: 0.3789 - val_acc: 0.6932\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1143 - acc: 0.7361 - val_loss: 0.3820 - val_acc: 0.6941\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1142 - acc: 0.7361 - val_loss: 0.3820 - val_acc: 0.6923\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1133 - acc: 0.7361 - val_loss: 0.3770 - val_acc: 0.6919\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1135 - acc: 0.7361 - val_loss: 0.3926 - val_acc: 0.6879\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1136 - acc: 0.7361 - val_loss: 0.3952 - val_acc: 0.6929\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1121 - acc: 0.7361 - val_loss: 0.3873 - val_acc: 0.6946\n",
      "17921/17921 [==============================] - 1s 59us/sample - loss: 0.6388 - acc: 0.6768\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1129 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 12s 348us/sample - loss: 0.2704 - acc: 0.7104 - val_loss: 0.3904 - val_acc: 0.6854\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1407 - acc: 0.7502 - val_loss: 0.3784 - val_acc: 0.6943\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1266 - acc: 0.7528 - val_loss: 0.3812 - val_acc: 0.6951\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1198 - acc: 0.7541 - val_loss: 0.3783 - val_acc: 0.6962\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1159 - acc: 0.7547 - val_loss: 0.3875 - val_acc: 0.6988\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1131 - acc: 0.7551 - val_loss: 0.3869 - val_acc: 0.7000\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1113 - acc: 0.7553 - val_loss: 0.3889 - val_acc: 0.6996\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1089 - acc: 0.7555 - val_loss: 0.3945 - val_acc: 0.6955\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1082 - acc: 0.7555 - val_loss: 0.3947 - val_acc: 0.6995\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1062 - acc: 0.7556 - val_loss: 0.3983 - val_acc: 0.6961\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1061 - acc: 0.7556 - val_loss: 0.3991 - val_acc: 0.6987\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1049 - acc: 0.7556 - val_loss: 0.3992 - val_acc: 0.6991\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1039 - acc: 0.7556 - val_loss: 0.4050 - val_acc: 0.6984\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1033 - acc: 0.7556 - val_loss: 0.4064 - val_acc: 0.7004\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1028 - acc: 0.7556 - val_loss: 0.4044 - val_acc: 0.6984\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1026 - acc: 0.7556 - val_loss: 0.4220 - val_acc: 0.6979\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1030 - acc: 0.7556 - val_loss: 0.4138 - val_acc: 0.7007\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1021 - acc: 0.7556 - val_loss: 0.4148 - val_acc: 0.6992\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1016 - acc: 0.7556 - val_loss: 0.4208 - val_acc: 0.6998\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1013 - acc: 0.7556 - val_loss: 0.4208 - val_acc: 0.6986\n",
      "17922/17922 [==============================] - 1s 58us/sample - loss: 0.9542 - acc: 0.6145\n",
      "35842/35842 [==============================] - 2s 57us/sample - loss: 0.1018 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 13s 355us/sample - loss: 0.2771 - acc: 0.7171 - val_loss: 0.3947 - val_acc: 0.6916\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1476 - acc: 0.7558 - val_loss: 0.3579 - val_acc: 0.6990\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1316 - acc: 0.7589 - val_loss: 0.3429 - val_acc: 0.7056\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1225 - acc: 0.7610 - val_loss: 0.3307 - val_acc: 0.7110\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1167 - acc: 0.7623 - val_loss: 0.3340 - val_acc: 0.7131\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1129 - acc: 0.7628 - val_loss: 0.3288 - val_acc: 0.7123\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1103 - acc: 0.7631 - val_loss: 0.3308 - val_acc: 0.7130\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1079 - acc: 0.7633 - val_loss: 0.3386 - val_acc: 0.7144\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1063 - acc: 0.7634 - val_loss: 0.3413 - val_acc: 0.7135\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1051 - acc: 0.7635 - val_loss: 0.3380 - val_acc: 0.7132\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1051 - acc: 0.7635 - val_loss: 0.3409 - val_acc: 0.7137\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1036 - acc: 0.7635 - val_loss: 0.3405 - val_acc: 0.7141\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1031 - acc: 0.7635 - val_loss: 0.3554 - val_acc: 0.7145\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1025 - acc: 0.7635 - val_loss: 0.3477 - val_acc: 0.7139\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1016 - acc: 0.7636 - val_loss: 0.3522 - val_acc: 0.7142\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1010 - acc: 0.7636 - val_loss: 0.3579 - val_acc: 0.7141\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1006 - acc: 0.7636 - val_loss: 0.3557 - val_acc: 0.7139\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1004 - acc: 0.7636 - val_loss: 0.3600 - val_acc: 0.7142\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.0999 - acc: 0.7636 - val_loss: 0.3649 - val_acc: 0.7139\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.0997 - acc: 0.7636 - val_loss: 0.3638 - val_acc: 0.7136\n",
      "17921/17921 [==============================] - 1s 59us/sample - loss: 0.7221 - acc: 0.6593\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1010 - acc: 0.7636\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 13s 359us/sample - loss: 0.2986 - acc: 0.6848 - val_loss: 0.3724 - val_acc: 0.6799\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1621 - acc: 0.7285 - val_loss: 0.3495 - val_acc: 0.6931\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1468 - acc: 0.7319 - val_loss: 0.3583 - val_acc: 0.6898\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1387 - acc: 0.7335 - val_loss: 0.3498 - val_acc: 0.6938\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1325 - acc: 0.7346 - val_loss: 0.3567 - val_acc: 0.6933\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1285 - acc: 0.7352 - val_loss: 0.3530 - val_acc: 0.6970\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1286 - acc: 0.7352 - val_loss: 0.3604 - val_acc: 0.6912\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1235 - acc: 0.7357 - val_loss: 0.3622 - val_acc: 0.6923\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1218 - acc: 0.7358 - val_loss: 0.3620 - val_acc: 0.6938\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1199 - acc: 0.7360 - val_loss: 0.3628 - val_acc: 0.6944\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1187 - acc: 0.7360 - val_loss: 0.3634 - val_acc: 0.6938\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1177 - acc: 0.7360 - val_loss: 0.3722 - val_acc: 0.6915\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1166 - acc: 0.7361 - val_loss: 0.3701 - val_acc: 0.6924\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1160 - acc: 0.7361 - val_loss: 0.3703 - val_acc: 0.6950\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1166 - acc: 0.7360 - val_loss: 0.3790 - val_acc: 0.6929\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1153 - acc: 0.7361 - val_loss: 0.3813 - val_acc: 0.6927\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1144 - acc: 0.7361 - val_loss: 0.3825 - val_acc: 0.6927\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1137 - acc: 0.7361 - val_loss: 0.3806 - val_acc: 0.6933\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1135 - acc: 0.7361 - val_loss: 0.3824 - val_acc: 0.6921\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1133 - acc: 0.7361 - val_loss: 0.3905 - val_acc: 0.6924\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.6530 - acc: 0.6720\n",
      "35843/35843 [==============================] - 2s 59us/sample - loss: 0.1127 - acc: 0.7361\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 13s 370us/sample - loss: 0.2426 - acc: 0.7192 - val_loss: 0.4107 - val_acc: 0.6840\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1431 - acc: 0.7478 - val_loss: 0.4033 - val_acc: 0.6897\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1290 - acc: 0.7504 - val_loss: 0.3859 - val_acc: 0.6893\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1202 - acc: 0.7519 - val_loss: 0.3806 - val_acc: 0.6849\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1191 - acc: 0.7519 - val_loss: 0.3980 - val_acc: 0.6905\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1112 - acc: 0.7534 - val_loss: 0.4134 - val_acc: 0.6924\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1082 - acc: 0.7541 - val_loss: 0.4140 - val_acc: 0.6903\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1057 - acc: 0.7545 - val_loss: 0.4125 - val_acc: 0.6912\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1057 - acc: 0.7544 - val_loss: 0.4482 - val_acc: 0.6910\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1033 - acc: 0.7548 - val_loss: 0.4551 - val_acc: 0.6914\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1022 - acc: 0.7550 - val_loss: 0.4627 - val_acc: 0.6924\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1005 - acc: 0.7553 - val_loss: 0.4689 - val_acc: 0.6927\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.0994 - acc: 0.7554 - val_loss: 0.4747 - val_acc: 0.6919\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.0988 - acc: 0.7555 - val_loss: 0.4749 - val_acc: 0.6924\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.0997 - acc: 0.7553 - val_loss: 0.4772 - val_acc: 0.6911\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.0989 - acc: 0.7555 - val_loss: 0.4954 - val_acc: 0.6912\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0981 - acc: 0.7555 - val_loss: 0.5041 - val_acc: 0.6928\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.5042 - val_acc: 0.6933\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.5175 - val_acc: 0.6930\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.0972 - acc: 0.7556 - val_loss: 0.5256 - val_acc: 0.6932\n",
      "17922/17922 [==============================] - 1s 62us/sample - loss: 1.3564 - acc: 0.5972\n",
      "35842/35842 [==============================] - 2s 64us/sample - loss: 0.0972 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 13s 373us/sample - loss: 0.2551 - acc: 0.7256 - val_loss: 0.3771 - val_acc: 0.6882\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1515 - acc: 0.7532 - val_loss: 0.3466 - val_acc: 0.6972\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1350 - acc: 0.7566 - val_loss: 0.3413 - val_acc: 0.7039\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1230 - acc: 0.7591 - val_loss: 0.3380 - val_acc: 0.7061\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1172 - acc: 0.7603 - val_loss: 0.3296 - val_acc: 0.7048\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1138 - acc: 0.7609 - val_loss: 0.3477 - val_acc: 0.7057\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1135 - acc: 0.7609 - val_loss: 0.3428 - val_acc: 0.7083\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1065 - acc: 0.7622 - val_loss: 0.3822 - val_acc: 0.7062\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1081 - acc: 0.7619 - val_loss: 0.3435 - val_acc: 0.7082\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1025 - acc: 0.7628 - val_loss: 0.3570 - val_acc: 0.7087\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1027 - acc: 0.7628 - val_loss: 0.3617 - val_acc: 0.7097\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0996 - acc: 0.7632 - val_loss: 0.3564 - val_acc: 0.7101\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0984 - acc: 0.7633 - val_loss: 0.3697 - val_acc: 0.7098\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0978 - acc: 0.7633 - val_loss: 0.3752 - val_acc: 0.7099\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0973 - acc: 0.7634 - val_loss: 0.3804 - val_acc: 0.7100\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0968 - acc: 0.7634 - val_loss: 0.3761 - val_acc: 0.7097\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1026 - acc: 0.7625 - val_loss: 0.3894 - val_acc: 0.7083\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0999 - acc: 0.7630 - val_loss: 0.3906 - val_acc: 0.7092\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0979 - acc: 0.7633 - val_loss: 0.3946 - val_acc: 0.7101\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0966 - acc: 0.7634 - val_loss: 0.3938 - val_acc: 0.7102\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.8137 - acc: 0.6569\n",
      "35843/35843 [==============================] - 2s 65us/sample - loss: 0.0962 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 13s 375us/sample - loss: 0.2750 - acc: 0.6937 - val_loss: 0.3843 - val_acc: 0.6825\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1693 - acc: 0.7244 - val_loss: 0.3674 - val_acc: 0.6828\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1499 - acc: 0.7286 - val_loss: 0.3768 - val_acc: 0.6848\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1386 - acc: 0.7307 - val_loss: 0.3799 - val_acc: 0.6849\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1350 - acc: 0.7314 - val_loss: 0.3828 - val_acc: 0.6865\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1305 - acc: 0.7323 - val_loss: 0.3889 - val_acc: 0.6856\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1239 - acc: 0.7336 - val_loss: 0.3934 - val_acc: 0.6840\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1197 - acc: 0.7344 - val_loss: 0.3873 - val_acc: 0.6865\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1183 - acc: 0.7346 - val_loss: 0.4044 - val_acc: 0.6875\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1159 - acc: 0.7350 - val_loss: 0.4034 - val_acc: 0.6847\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1136 - acc: 0.7354 - val_loss: 0.4362 - val_acc: 0.6877\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1146 - acc: 0.7352 - val_loss: 0.4375 - val_acc: 0.6856\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1120 - acc: 0.7356 - val_loss: 0.4360 - val_acc: 0.6856\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1111 - acc: 0.7357 - val_loss: 0.4182 - val_acc: 0.6862\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1108 - acc: 0.7357 - val_loss: 0.4511 - val_acc: 0.6862\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1164 - acc: 0.7347 - val_loss: 0.4624 - val_acc: 0.6830\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1120 - acc: 0.7355 - val_loss: 0.4434 - val_acc: 0.6837\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1095 - acc: 0.7359 - val_loss: 0.4623 - val_acc: 0.6839\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1086 - acc: 0.7359 - val_loss: 0.4627 - val_acc: 0.6852\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1082 - acc: 0.7360 - val_loss: 0.4620 - val_acc: 0.6852\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.8157 - acc: 0.6553\n",
      "35843/35843 [==============================] - 2s 67us/sample - loss: 0.1079 - acc: 0.7360\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 14s 378us/sample - loss: 0.2659 - acc: 0.7112 - val_loss: 0.4026 - val_acc: 0.6838\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1479 - acc: 0.7469 - val_loss: 0.4001 - val_acc: 0.6878\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1359 - acc: 0.7490 - val_loss: 0.3980 - val_acc: 0.6874\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1252 - acc: 0.7511 - val_loss: 0.3827 - val_acc: 0.6908\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1190 - acc: 0.7522 - val_loss: 0.3970 - val_acc: 0.6896\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1146 - acc: 0.7530 - val_loss: 0.4085 - val_acc: 0.6908\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1112 - acc: 0.7536 - val_loss: 0.4258 - val_acc: 0.6926\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1081 - acc: 0.7542 - val_loss: 0.4240 - val_acc: 0.6915\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1060 - acc: 0.7545 - val_loss: 0.4305 - val_acc: 0.6919\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1054 - acc: 0.7546 - val_loss: 0.4335 - val_acc: 0.6923\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1029 - acc: 0.7550 - val_loss: 0.4426 - val_acc: 0.6907\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1016 - acc: 0.7552 - val_loss: 0.4736 - val_acc: 0.6923\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1017 - acc: 0.7552 - val_loss: 0.4701 - val_acc: 0.6904\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1011 - acc: 0.7552 - val_loss: 0.4773 - val_acc: 0.6915\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1004 - acc: 0.7553 - val_loss: 0.4771 - val_acc: 0.6928\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0991 - acc: 0.7554 - val_loss: 0.4823 - val_acc: 0.6925\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0984 - acc: 0.7555 - val_loss: 0.4877 - val_acc: 0.6931\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.0980 - acc: 0.7555 - val_loss: 0.4944 - val_acc: 0.6931\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.0978 - acc: 0.7555 - val_loss: 0.4870 - val_acc: 0.6934\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0976 - acc: 0.7555 - val_loss: 0.4935 - val_acc: 0.6929\n",
      "17922/17922 [==============================] - 1s 62us/sample - loss: 1.2196 - acc: 0.5968\n",
      "35842/35842 [==============================] - 2s 63us/sample - loss: 0.0976 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 394us/sample - loss: 0.2733 - acc: 0.7196 - val_loss: 0.3903 - val_acc: 0.6891\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1506 - acc: 0.7540 - val_loss: 0.3486 - val_acc: 0.6967\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1340 - acc: 0.7573 - val_loss: 0.3469 - val_acc: 0.7042\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1243 - acc: 0.7592 - val_loss: 0.3575 - val_acc: 0.7039\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1179 - acc: 0.7603 - val_loss: 0.3286 - val_acc: 0.7055\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1131 - acc: 0.7612 - val_loss: 0.3458 - val_acc: 0.7076\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1105 - acc: 0.7616 - val_loss: 0.3499 - val_acc: 0.7063\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1113 - acc: 0.7614 - val_loss: 0.3366 - val_acc: 0.7087\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1050 - acc: 0.7625 - val_loss: 0.3392 - val_acc: 0.7094\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1029 - acc: 0.7628 - val_loss: 0.3421 - val_acc: 0.7087\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1017 - acc: 0.7630 - val_loss: 0.3518 - val_acc: 0.7099\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1001 - acc: 0.7632 - val_loss: 0.3583 - val_acc: 0.7100\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0992 - acc: 0.7633 - val_loss: 0.3735 - val_acc: 0.7104\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0984 - acc: 0.7633 - val_loss: 0.3706 - val_acc: 0.7104\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0978 - acc: 0.7634 - val_loss: 0.3737 - val_acc: 0.7109\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0990 - acc: 0.7633 - val_loss: 0.3680 - val_acc: 0.7103\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.0973 - acc: 0.7634 - val_loss: 0.3811 - val_acc: 0.7108\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0968 - acc: 0.7634 - val_loss: 0.3865 - val_acc: 0.7107\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.3807 - val_acc: 0.7102\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0962 - acc: 0.7635 - val_loss: 0.3906 - val_acc: 0.7112\n",
      "17921/17921 [==============================] - 1s 63us/sample - loss: 0.7710 - acc: 0.6551\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.0959 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 378us/sample - loss: 0.2941 - acc: 0.6876 - val_loss: 0.3955 - val_acc: 0.6823\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1729 - acc: 0.7242 - val_loss: 0.3723 - val_acc: 0.6850\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1558 - acc: 0.7276 - val_loss: 0.3548 - val_acc: 0.6815\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1430 - acc: 0.7302 - val_loss: 0.3621 - val_acc: 0.6841\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1353 - acc: 0.7317 - val_loss: 0.3580 - val_acc: 0.6875\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1301 - acc: 0.7327 - val_loss: 0.3734 - val_acc: 0.6849\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1265 - acc: 0.7333 - val_loss: 0.3721 - val_acc: 0.6851\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1227 - acc: 0.7340 - val_loss: 0.3891 - val_acc: 0.6851\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1207 - acc: 0.7343 - val_loss: 0.3892 - val_acc: 0.6863\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1171 - acc: 0.7349 - val_loss: 0.3841 - val_acc: 0.6869\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1155 - acc: 0.7351 - val_loss: 0.3791 - val_acc: 0.6850\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1179 - acc: 0.7347 - val_loss: 0.3993 - val_acc: 0.6839\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1135 - acc: 0.7354 - val_loss: 0.4192 - val_acc: 0.6848\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1137 - acc: 0.7354 - val_loss: 0.4120 - val_acc: 0.6844\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1113 - acc: 0.7357 - val_loss: 0.4129 - val_acc: 0.6873\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1116 - acc: 0.7356 - val_loss: 0.4135 - val_acc: 0.6872\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1102 - acc: 0.7358 - val_loss: 0.4197 - val_acc: 0.6860\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1093 - acc: 0.7359 - val_loss: 0.4177 - val_acc: 0.6875\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1088 - acc: 0.7359 - val_loss: 0.4323 - val_acc: 0.6862\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1085 - acc: 0.7359 - val_loss: 0.4403 - val_acc: 0.6854\n",
      "17921/17921 [==============================] - 1s 62us/sample - loss: 0.7605 - acc: 0.6553\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.1082 - acc: 0.7360\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 15s 406us/sample - loss: 0.2927 - acc: 0.7001 - val_loss: 0.4170 - val_acc: 0.6828\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1512 - acc: 0.7467 - val_loss: 0.4009 - val_acc: 0.6859\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1387 - acc: 0.7489 - val_loss: 0.3958 - val_acc: 0.6917\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1266 - acc: 0.7512 - val_loss: 0.3861 - val_acc: 0.6920\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1212 - acc: 0.7521 - val_loss: 0.3885 - val_acc: 0.6907\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1157 - acc: 0.7531 - val_loss: 0.3883 - val_acc: 0.6925\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1128 - acc: 0.7536 - val_loss: 0.4055 - val_acc: 0.6911\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1098 - acc: 0.7542 - val_loss: 0.4023 - val_acc: 0.6912\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1077 - acc: 0.7544 - val_loss: 0.4082 - val_acc: 0.6920\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1055 - acc: 0.7548 - val_loss: 0.4252 - val_acc: 0.6901\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1047 - acc: 0.7548 - val_loss: 0.4337 - val_acc: 0.6924\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1031 - acc: 0.7551 - val_loss: 0.4259 - val_acc: 0.6919\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1024 - acc: 0.7552 - val_loss: 0.4401 - val_acc: 0.6924\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1013 - acc: 0.7553 - val_loss: 0.4564 - val_acc: 0.6930\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1026 - acc: 0.7551 - val_loss: 0.4632 - val_acc: 0.6896\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1011 - acc: 0.7552 - val_loss: 0.4576 - val_acc: 0.6921\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1004 - acc: 0.7554 - val_loss: 0.4824 - val_acc: 0.6922\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0997 - acc: 0.7554 - val_loss: 0.4837 - val_acc: 0.6917\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1003 - acc: 0.7552 - val_loss: 0.4822 - val_acc: 0.6891\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0996 - acc: 0.7554 - val_loss: 0.4984 - val_acc: 0.6909\n",
      "17922/17922 [==============================] - 1s 62us/sample - loss: 1.2471 - acc: 0.5924\n",
      "35842/35842 [==============================] - 2s 63us/sample - loss: 0.0993 - acc: 0.7554\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 382us/sample - loss: 0.3121 - acc: 0.7062 - val_loss: 0.4053 - val_acc: 0.6843\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1554 - acc: 0.7532 - val_loss: 0.3520 - val_acc: 0.6988\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1366 - acc: 0.7571 - val_loss: 0.3447 - val_acc: 0.7053\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1267 - acc: 0.7591 - val_loss: 0.3370 - val_acc: 0.7070\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1206 - acc: 0.7601 - val_loss: 0.3457 - val_acc: 0.7072\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1155 - acc: 0.7610 - val_loss: 0.3315 - val_acc: 0.7067\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1120 - acc: 0.7616 - val_loss: 0.3467 - val_acc: 0.7080\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1088 - acc: 0.7621 - val_loss: 0.3389 - val_acc: 0.7097\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1063 - acc: 0.7625 - val_loss: 0.3333 - val_acc: 0.7086\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1043 - acc: 0.7628 - val_loss: 0.3360 - val_acc: 0.7093\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1031 - acc: 0.7629 - val_loss: 0.3382 - val_acc: 0.7095\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1016 - acc: 0.7630 - val_loss: 0.3478 - val_acc: 0.7091\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1039 - acc: 0.7627 - val_loss: 0.3703 - val_acc: 0.7086\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1005 - acc: 0.7631 - val_loss: 0.3429 - val_acc: 0.7075\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0993 - acc: 0.7632 - val_loss: 0.3564 - val_acc: 0.7095\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0985 - acc: 0.7633 - val_loss: 0.3568 - val_acc: 0.7096\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.0981 - acc: 0.7633 - val_loss: 0.3766 - val_acc: 0.7089\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0991 - acc: 0.7632 - val_loss: 0.3667 - val_acc: 0.7101\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0979 - acc: 0.7633 - val_loss: 0.3699 - val_acc: 0.7094\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.0972 - acc: 0.7634 - val_loss: 0.3851 - val_acc: 0.7097\n",
      "17921/17921 [==============================] - 1s 65us/sample - loss: 0.7594 - acc: 0.6541\n",
      "35843/35843 [==============================] - 2s 66us/sample - loss: 0.0970 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 392us/sample - loss: 0.3250 - acc: 0.6759 - val_loss: 0.3828 - val_acc: 0.6809\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1763 - acc: 0.7238 - val_loss: 0.3763 - val_acc: 0.6819\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1564 - acc: 0.7280 - val_loss: 0.3606 - val_acc: 0.6882\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1455 - acc: 0.7301 - val_loss: 0.3685 - val_acc: 0.6899\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1375 - acc: 0.7316 - val_loss: 0.3661 - val_acc: 0.6887\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1316 - acc: 0.7327 - val_loss: 0.3765 - val_acc: 0.6875\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1279 - acc: 0.7332 - val_loss: 0.3802 - val_acc: 0.6875\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1240 - acc: 0.7339 - val_loss: 0.3889 - val_acc: 0.6897\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1212 - acc: 0.7343 - val_loss: 0.3793 - val_acc: 0.6863\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1187 - acc: 0.7347 - val_loss: 0.4066 - val_acc: 0.6890\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1164 - acc: 0.7351 - val_loss: 0.4053 - val_acc: 0.6865\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1151 - acc: 0.7353 - val_loss: 0.4227 - val_acc: 0.6873\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1135 - acc: 0.7355 - val_loss: 0.4133 - val_acc: 0.6897\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1125 - acc: 0.7355 - val_loss: 0.4074 - val_acc: 0.6857\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1117 - acc: 0.7357 - val_loss: 0.4228 - val_acc: 0.6863\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1113 - acc: 0.7357 - val_loss: 0.4365 - val_acc: 0.6871\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1104 - acc: 0.7358 - val_loss: 0.4428 - val_acc: 0.6853\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1098 - acc: 0.7358 - val_loss: 0.4430 - val_acc: 0.6862\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1098 - acc: 0.7358 - val_loss: 0.4485 - val_acc: 0.6856\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1091 - acc: 0.7359 - val_loss: 0.4526 - val_acc: 0.6847\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.7780 - acc: 0.6570\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.1090 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 14s 391us/sample - loss: 0.2530 - acc: 0.7151 - val_loss: 0.4124 - val_acc: 0.6825\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1458 - acc: 0.7470 - val_loss: 0.4123 - val_acc: 0.6855\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1332 - acc: 0.7495 - val_loss: 0.3992 - val_acc: 0.6906\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1226 - acc: 0.7516 - val_loss: 0.4036 - val_acc: 0.6919\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1225 - acc: 0.7515 - val_loss: 0.4191 - val_acc: 0.6888\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1141 - acc: 0.7530 - val_loss: 0.4211 - val_acc: 0.6914\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1113 - acc: 0.7535 - val_loss: 0.4127 - val_acc: 0.6901\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1082 - acc: 0.7541 - val_loss: 0.4298 - val_acc: 0.6926\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1056 - acc: 0.7546 - val_loss: 0.4377 - val_acc: 0.6915\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1082 - acc: 0.7542 - val_loss: 0.4488 - val_acc: 0.6919\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1036 - acc: 0.7549 - val_loss: 0.4575 - val_acc: 0.6919\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1021 - acc: 0.7551 - val_loss: 0.4683 - val_acc: 0.6907\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1011 - acc: 0.7552 - val_loss: 0.4784 - val_acc: 0.6917\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1001 - acc: 0.7554 - val_loss: 0.4904 - val_acc: 0.6908\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.0994 - acc: 0.7554 - val_loss: 0.5052 - val_acc: 0.6925\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0989 - acc: 0.7555 - val_loss: 0.4959 - val_acc: 0.6921\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0994 - acc: 0.7554 - val_loss: 0.5041 - val_acc: 0.6916\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.0992 - acc: 0.7554 - val_loss: 0.5174 - val_acc: 0.6896\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.0985 - acc: 0.7555 - val_loss: 0.5058 - val_acc: 0.6914\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.0978 - acc: 0.7556 - val_loss: 0.5318 - val_acc: 0.6915\n",
      "17922/17922 [==============================] - 1s 65us/sample - loss: 1.3568 - acc: 0.5945\n",
      "35842/35842 [==============================] - 2s 63us/sample - loss: 0.0975 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 386us/sample - loss: 0.2566 - acc: 0.7250 - val_loss: 0.3928 - val_acc: 0.6877\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1502 - acc: 0.7537 - val_loss: 0.3528 - val_acc: 0.7003\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1339 - acc: 0.7571 - val_loss: 0.3550 - val_acc: 0.7004\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1263 - acc: 0.7585 - val_loss: 0.3395 - val_acc: 0.7042\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1169 - acc: 0.7604 - val_loss: 0.3467 - val_acc: 0.7065\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1122 - acc: 0.7613 - val_loss: 0.3380 - val_acc: 0.7074\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1090 - acc: 0.7618 - val_loss: 0.3474 - val_acc: 0.7077\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1066 - acc: 0.7623 - val_loss: 0.3544 - val_acc: 0.7088\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1038 - acc: 0.7627 - val_loss: 0.3619 - val_acc: 0.7090\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1022 - acc: 0.7629 - val_loss: 0.3611 - val_acc: 0.7094\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1008 - acc: 0.7631 - val_loss: 0.3604 - val_acc: 0.7091\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.0997 - acc: 0.7632 - val_loss: 0.3812 - val_acc: 0.7098\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.0988 - acc: 0.7633 - val_loss: 0.3934 - val_acc: 0.7091\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.0979 - acc: 0.7634 - val_loss: 0.3870 - val_acc: 0.7094\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0978 - acc: 0.7634 - val_loss: 0.3858 - val_acc: 0.7087\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1044 - acc: 0.7623 - val_loss: 0.4010 - val_acc: 0.7047\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1018 - acc: 0.7627 - val_loss: 0.4137 - val_acc: 0.7082\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0989 - acc: 0.7632 - val_loss: 0.4134 - val_acc: 0.7078\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0969 - acc: 0.7634 - val_loss: 0.4218 - val_acc: 0.7093\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0962 - acc: 0.7635 - val_loss: 0.4174 - val_acc: 0.7094\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.8165 - acc: 0.6566\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.0962 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 391us/sample - loss: 0.2716 - acc: 0.6950 - val_loss: 0.3860 - val_acc: 0.6787\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1692 - acc: 0.7246 - val_loss: 0.3751 - val_acc: 0.6752\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1522 - acc: 0.7282 - val_loss: 0.3813 - val_acc: 0.6815\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1410 - acc: 0.7304 - val_loss: 0.4030 - val_acc: 0.6866\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1351 - acc: 0.7315 - val_loss: 0.3724 - val_acc: 0.6866\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1281 - acc: 0.7329 - val_loss: 0.4063 - val_acc: 0.6831\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1347 - acc: 0.7314 - val_loss: 0.3813 - val_acc: 0.6845\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1226 - acc: 0.7339 - val_loss: 0.3846 - val_acc: 0.6882\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1199 - acc: 0.7344 - val_loss: 0.3928 - val_acc: 0.6843\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1172 - acc: 0.7348 - val_loss: 0.4301 - val_acc: 0.6839\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1165 - acc: 0.7349 - val_loss: 0.4378 - val_acc: 0.6812\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1139 - acc: 0.7353 - val_loss: 0.4320 - val_acc: 0.6847\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1125 - acc: 0.7355 - val_loss: 0.4310 - val_acc: 0.6876\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1114 - acc: 0.7357 - val_loss: 0.4540 - val_acc: 0.6849\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1131 - acc: 0.7354 - val_loss: 0.4608 - val_acc: 0.6848\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1114 - acc: 0.7356 - val_loss: 0.4811 - val_acc: 0.6823\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1113 - acc: 0.7356 - val_loss: 0.4520 - val_acc: 0.6838\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1098 - acc: 0.7358 - val_loss: 0.4621 - val_acc: 0.6844\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1088 - acc: 0.7359 - val_loss: 0.4702 - val_acc: 0.6842\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1088 - acc: 0.7359 - val_loss: 0.4838 - val_acc: 0.6844\n",
      "17921/17921 [==============================] - 1s 63us/sample - loss: 0.8442 - acc: 0.6550\n",
      "35843/35843 [==============================] - 2s 63us/sample - loss: 0.1081 - acc: 0.7360\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 15s 407us/sample - loss: 0.2716 - acc: 0.7101 - val_loss: 0.4177 - val_acc: 0.6842\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1474 - acc: 0.7471 - val_loss: 0.3956 - val_acc: 0.6881\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1320 - acc: 0.7500 - val_loss: 0.4096 - val_acc: 0.6897\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1240 - acc: 0.7514 - val_loss: 0.4162 - val_acc: 0.6903\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1207 - acc: 0.7520 - val_loss: 0.3956 - val_acc: 0.6902\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1142 - acc: 0.7532 - val_loss: 0.4220 - val_acc: 0.6864\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1179 - acc: 0.7523 - val_loss: 0.4070 - val_acc: 0.6916\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1103 - acc: 0.7538 - val_loss: 0.4173 - val_acc: 0.6908\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1095 - acc: 0.7538 - val_loss: 0.4413 - val_acc: 0.6906\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1055 - acc: 0.7546 - val_loss: 0.4353 - val_acc: 0.6929\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1041 - acc: 0.7548 - val_loss: 0.4245 - val_acc: 0.6921\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1034 - acc: 0.7549 - val_loss: 0.4372 - val_acc: 0.6917\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1022 - acc: 0.7551 - val_loss: 0.4508 - val_acc: 0.6926\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1010 - acc: 0.7552 - val_loss: 0.4633 - val_acc: 0.6918\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1002 - acc: 0.7553 - val_loss: 0.4644 - val_acc: 0.6918\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.0997 - acc: 0.7554 - val_loss: 0.4689 - val_acc: 0.6919\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.0991 - acc: 0.7554 - val_loss: 0.4812 - val_acc: 0.6918\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.0989 - acc: 0.7555 - val_loss: 0.4896 - val_acc: 0.6923\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.0984 - acc: 0.7555 - val_loss: 0.4905 - val_acc: 0.6924\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1032 - acc: 0.7548 - val_loss: 0.4900 - val_acc: 0.6902\n",
      "17922/17922 [==============================] - 1s 63us/sample - loss: 1.2159 - acc: 0.5973\n",
      "35842/35842 [==============================] - 2s 64us/sample - loss: 0.1010 - acc: 0.7551\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 391us/sample - loss: 0.2793 - acc: 0.7173 - val_loss: 0.3740 - val_acc: 0.6854\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1545 - acc: 0.7531 - val_loss: 0.3600 - val_acc: 0.6958\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1384 - acc: 0.7565 - val_loss: 0.3395 - val_acc: 0.7051\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1291 - acc: 0.7583 - val_loss: 0.3255 - val_acc: 0.7070\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1210 - acc: 0.7597 - val_loss: 0.3392 - val_acc: 0.7060\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1200 - acc: 0.7597 - val_loss: 0.3362 - val_acc: 0.7069\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1135 - acc: 0.7611 - val_loss: 0.3531 - val_acc: 0.7070\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1102 - acc: 0.7618 - val_loss: 0.3355 - val_acc: 0.7081\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1071 - acc: 0.7622 - val_loss: 0.3371 - val_acc: 0.7083\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1048 - acc: 0.7626 - val_loss: 0.3357 - val_acc: 0.7095\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1032 - acc: 0.7628 - val_loss: 0.3525 - val_acc: 0.7071\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1027 - acc: 0.7629 - val_loss: 0.3609 - val_acc: 0.7095\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1009 - acc: 0.7631 - val_loss: 0.3588 - val_acc: 0.7094\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1019 - acc: 0.7629 - val_loss: 0.3615 - val_acc: 0.7092\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1001 - acc: 0.7631 - val_loss: 0.3713 - val_acc: 0.7092\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0991 - acc: 0.7632 - val_loss: 0.3670 - val_acc: 0.7089\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1027 - acc: 0.7626 - val_loss: 0.3709 - val_acc: 0.7082\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.0986 - acc: 0.7633 - val_loss: 0.3981 - val_acc: 0.7099\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.0996 - acc: 0.7631 - val_loss: 0.3750 - val_acc: 0.7090\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0990 - acc: 0.7632 - val_loss: 0.3925 - val_acc: 0.7060\n",
      "17921/17921 [==============================] - 1s 61us/sample - loss: 0.7682 - acc: 0.6539\n",
      "35843/35843 [==============================] - 2s 61us/sample - loss: 0.1003 - acc: 0.7630\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 414us/sample - loss: 0.2961 - acc: 0.6871 - val_loss: 0.3891 - val_acc: 0.6755\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1761 - acc: 0.7235 - val_loss: 0.3774 - val_acc: 0.6828\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1563 - acc: 0.7279 - val_loss: 0.3975 - val_acc: 0.6843\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1481 - acc: 0.7296 - val_loss: 0.3682 - val_acc: 0.6826\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1399 - acc: 0.7309 - val_loss: 0.3675 - val_acc: 0.6884\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1330 - acc: 0.7323 - val_loss: 0.3694 - val_acc: 0.6864\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1283 - acc: 0.7332 - val_loss: 0.3840 - val_acc: 0.6828\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1246 - acc: 0.7338 - val_loss: 0.3842 - val_acc: 0.6842\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1220 - acc: 0.7343 - val_loss: 0.3871 - val_acc: 0.6846\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1196 - acc: 0.7346 - val_loss: 0.3906 - val_acc: 0.6861\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1170 - acc: 0.7350 - val_loss: 0.4017 - val_acc: 0.6840\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1160 - acc: 0.7352 - val_loss: 0.3994 - val_acc: 0.6845\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1143 - acc: 0.7354 - val_loss: 0.4307 - val_acc: 0.6827\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1133 - acc: 0.7355 - val_loss: 0.4294 - val_acc: 0.6854\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1129 - acc: 0.7356 - val_loss: 0.4424 - val_acc: 0.6815\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1127 - acc: 0.7356 - val_loss: 0.4461 - val_acc: 0.6841\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1113 - acc: 0.7357 - val_loss: 0.4321 - val_acc: 0.6860\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1104 - acc: 0.7358 - val_loss: 0.4500 - val_acc: 0.6834\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1103 - acc: 0.7358 - val_loss: 0.4404 - val_acc: 0.6854\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1094 - acc: 0.7359 - val_loss: 0.4665 - val_acc: 0.6839\n",
      "17921/17921 [==============================] - 1s 62us/sample - loss: 0.8021 - acc: 0.6540\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.1093 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 14s 392us/sample - loss: 0.2974 - acc: 0.7006 - val_loss: 0.4279 - val_acc: 0.6838\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1578 - acc: 0.7455 - val_loss: 0.4140 - val_acc: 0.6885\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1404 - acc: 0.7489 - val_loss: 0.3994 - val_acc: 0.6861\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1315 - acc: 0.7505 - val_loss: 0.3850 - val_acc: 0.6905\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1252 - acc: 0.7516 - val_loss: 0.3930 - val_acc: 0.6886\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1210 - acc: 0.7524 - val_loss: 0.4001 - val_acc: 0.6933\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1188 - acc: 0.7528 - val_loss: 0.3982 - val_acc: 0.6909\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1138 - acc: 0.7535 - val_loss: 0.4101 - val_acc: 0.6927\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1110 - acc: 0.7541 - val_loss: 0.4179 - val_acc: 0.6907\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1093 - acc: 0.7543 - val_loss: 0.4357 - val_acc: 0.6922\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1072 - acc: 0.7546 - val_loss: 0.4346 - val_acc: 0.6929\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1068 - acc: 0.7546 - val_loss: 0.4385 - val_acc: 0.6923\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1045 - acc: 0.7550 - val_loss: 0.4604 - val_acc: 0.6920\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1032 - acc: 0.7552 - val_loss: 0.4629 - val_acc: 0.6918\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1023 - acc: 0.7553 - val_loss: 0.4715 - val_acc: 0.6924\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1016 - acc: 0.7553 - val_loss: 0.4785 - val_acc: 0.6919\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1010 - acc: 0.7554 - val_loss: 0.4811 - val_acc: 0.6920\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1005 - acc: 0.7554 - val_loss: 0.4977 - val_acc: 0.6924\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0997 - acc: 0.7555 - val_loss: 0.5099 - val_acc: 0.6932\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0996 - acc: 0.7555 - val_loss: 0.5021 - val_acc: 0.6923\n",
      "17922/17922 [==============================] - 1s 64us/sample - loss: 1.2140 - acc: 0.5976\n",
      "35842/35842 [==============================] - 2s 62us/sample - loss: 0.0990 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 399us/sample - loss: 0.3073 - acc: 0.7095 - val_loss: 0.4064 - val_acc: 0.6835\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1617 - acc: 0.7521 - val_loss: 0.3709 - val_acc: 0.6969\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1428 - acc: 0.7558 - val_loss: 0.3472 - val_acc: 0.7031\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1316 - acc: 0.7582 - val_loss: 0.3440 - val_acc: 0.7045\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1246 - acc: 0.7595 - val_loss: 0.3460 - val_acc: 0.7060\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1197 - acc: 0.7604 - val_loss: 0.3382 - val_acc: 0.7075\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1159 - acc: 0.7611 - val_loss: 0.3481 - val_acc: 0.7082\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1137 - acc: 0.7615 - val_loss: 0.3471 - val_acc: 0.7092\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1100 - acc: 0.7621 - val_loss: 0.3487 - val_acc: 0.7081\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1090 - acc: 0.7622 - val_loss: 0.3525 - val_acc: 0.7082\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1060 - acc: 0.7627 - val_loss: 0.3614 - val_acc: 0.7092\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1042 - acc: 0.7628 - val_loss: 0.3532 - val_acc: 0.7086\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1040 - acc: 0.7629 - val_loss: 0.3814 - val_acc: 0.7085\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1030 - acc: 0.7629 - val_loss: 0.3764 - val_acc: 0.7097\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1037 - acc: 0.7628 - val_loss: 0.3772 - val_acc: 0.7073\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1008 - acc: 0.7631 - val_loss: 0.3785 - val_acc: 0.7091\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0999 - acc: 0.7632 - val_loss: 0.3812 - val_acc: 0.7091\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0992 - acc: 0.7633 - val_loss: 0.3869 - val_acc: 0.7093\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0986 - acc: 0.7633 - val_loss: 0.3954 - val_acc: 0.7093\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0989 - acc: 0.7633 - val_loss: 0.3937 - val_acc: 0.7087\n",
      "17921/17921 [==============================] - 1s 63us/sample - loss: 0.7451 - acc: 0.6546\n",
      "35843/35843 [==============================] - 2s 62us/sample - loss: 0.0982 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 394us/sample - loss: 0.3246 - acc: 0.6762 - val_loss: 0.3930 - val_acc: 0.6778\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1793 - acc: 0.7233 - val_loss: 0.3810 - val_acc: 0.6830\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1607 - acc: 0.7272 - val_loss: 0.3766 - val_acc: 0.6818\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1494 - acc: 0.7294 - val_loss: 0.3682 - val_acc: 0.6789\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1425 - acc: 0.7308 - val_loss: 0.3634 - val_acc: 0.6860\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1356 - acc: 0.7321 - val_loss: 0.3740 - val_acc: 0.6883\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1314 - acc: 0.7328 - val_loss: 0.3550 - val_acc: 0.6874\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1283 - acc: 0.7333 - val_loss: 0.3646 - val_acc: 0.6884\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1243 - acc: 0.7340 - val_loss: 0.3780 - val_acc: 0.6831\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1226 - acc: 0.7342 - val_loss: 0.4099 - val_acc: 0.6817\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1237 - acc: 0.7338 - val_loss: 0.3787 - val_acc: 0.6858\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1231 - acc: 0.7340 - val_loss: 0.3933 - val_acc: 0.6861\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1171 - acc: 0.7351 - val_loss: 0.4021 - val_acc: 0.6888\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1156 - acc: 0.7352 - val_loss: 0.3970 - val_acc: 0.6870\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1141 - acc: 0.7354 - val_loss: 0.3928 - val_acc: 0.6877\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1143 - acc: 0.7354 - val_loss: 0.4093 - val_acc: 0.6866\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1136 - acc: 0.7354 - val_loss: 0.4143 - val_acc: 0.6850\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1129 - acc: 0.7355 - val_loss: 0.4194 - val_acc: 0.6839\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1119 - acc: 0.7356 - val_loss: 0.4263 - val_acc: 0.6863\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1115 - acc: 0.7357 - val_loss: 0.4237 - val_acc: 0.6853\n",
      "17921/17921 [==============================] - 1s 63us/sample - loss: 0.7243 - acc: 0.6570\n",
      "35843/35843 [==============================] - 2s 65us/sample - loss: 0.1106 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 14s 401us/sample - loss: 0.2456 - acc: 0.7177 - val_loss: 0.3996 - val_acc: 0.6822\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1428 - acc: 0.7478 - val_loss: 0.4003 - val_acc: 0.6897\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1284 - acc: 0.7506 - val_loss: 0.3934 - val_acc: 0.6859\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1255 - acc: 0.7508 - val_loss: 0.4018 - val_acc: 0.6933\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1164 - acc: 0.7527 - val_loss: 0.4017 - val_acc: 0.6906\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1120 - acc: 0.7534 - val_loss: 0.4249 - val_acc: 0.6874\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1145 - acc: 0.7529 - val_loss: 0.4038 - val_acc: 0.6908\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1077 - acc: 0.7541 - val_loss: 0.4331 - val_acc: 0.6928\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1047 - acc: 0.7547 - val_loss: 0.4372 - val_acc: 0.6930\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1034 - acc: 0.7549 - val_loss: 0.4393 - val_acc: 0.6942\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1020 - acc: 0.7551 - val_loss: 0.4460 - val_acc: 0.6935\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1009 - acc: 0.7552 - val_loss: 0.4752 - val_acc: 0.6924\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1000 - acc: 0.7553 - val_loss: 0.4758 - val_acc: 0.6918\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.0998 - acc: 0.7553 - val_loss: 0.4816 - val_acc: 0.6929\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0989 - acc: 0.7555 - val_loss: 0.4949 - val_acc: 0.6922\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0985 - acc: 0.7555 - val_loss: 0.5055 - val_acc: 0.6914\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.0980 - acc: 0.7556 - val_loss: 0.5075 - val_acc: 0.6921\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.0977 - acc: 0.7556 - val_loss: 0.5218 - val_acc: 0.6925\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.0976 - acc: 0.7556 - val_loss: 0.5189 - val_acc: 0.6928\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.0973 - acc: 0.7556 - val_loss: 0.5264 - val_acc: 0.6922\n",
      "17922/17922 [==============================] - 1s 66us/sample - loss: 1.3353 - acc: 0.5950\n",
      "35842/35842 [==============================] - 2s 64us/sample - loss: 0.0972 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 14s 399us/sample - loss: 0.2588 - acc: 0.7237 - val_loss: 0.3810 - val_acc: 0.6872\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1490 - acc: 0.7539 - val_loss: 0.3461 - val_acc: 0.6974\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1319 - acc: 0.7575 - val_loss: 0.3487 - val_acc: 0.7055\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1223 - acc: 0.7593 - val_loss: 0.3371 - val_acc: 0.7066\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1187 - acc: 0.7599 - val_loss: 0.3338 - val_acc: 0.7077\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1117 - acc: 0.7612 - val_loss: 0.3557 - val_acc: 0.7079\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1083 - acc: 0.7619 - val_loss: 0.3481 - val_acc: 0.7087\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1065 - acc: 0.7622 - val_loss: 0.3530 - val_acc: 0.7084\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1038 - acc: 0.7626 - val_loss: 0.3559 - val_acc: 0.7077\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1013 - acc: 0.7629 - val_loss: 0.3615 - val_acc: 0.7086\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1008 - acc: 0.7630 - val_loss: 0.3626 - val_acc: 0.7084\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1001 - acc: 0.7631 - val_loss: 0.3685 - val_acc: 0.7088\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0983 - acc: 0.7633 - val_loss: 0.3742 - val_acc: 0.7104\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0975 - acc: 0.7634 - val_loss: 0.3823 - val_acc: 0.7102\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0969 - acc: 0.7634 - val_loss: 0.3911 - val_acc: 0.7098\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0965 - acc: 0.7635 - val_loss: 0.3947 - val_acc: 0.7096\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0963 - acc: 0.7635 - val_loss: 0.4031 - val_acc: 0.7092\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0961 - acc: 0.7635 - val_loss: 0.3997 - val_acc: 0.7096\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0958 - acc: 0.7635 - val_loss: 0.4126 - val_acc: 0.7097\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0957 - acc: 0.7635 - val_loss: 0.4031 - val_acc: 0.7100\n",
      "17921/17921 [==============================] - 1s 63us/sample - loss: 0.7966 - acc: 0.6557\n",
      "35843/35843 [==============================] - 2s 63us/sample - loss: 0.0958 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 413us/sample - loss: 0.2800 - acc: 0.6929 - val_loss: 0.4023 - val_acc: 0.6733\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1713 - acc: 0.7241 - val_loss: 0.3901 - val_acc: 0.6771\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1520 - acc: 0.7282 - val_loss: 0.3672 - val_acc: 0.6817\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1406 - acc: 0.7303 - val_loss: 0.3737 - val_acc: 0.6870\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1336 - acc: 0.7318 - val_loss: 0.3709 - val_acc: 0.6865\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1281 - acc: 0.7328 - val_loss: 0.3575 - val_acc: 0.6883\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1236 - acc: 0.7337 - val_loss: 0.3762 - val_acc: 0.6872\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1202 - acc: 0.7343 - val_loss: 0.4063 - val_acc: 0.6868\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1172 - acc: 0.7348 - val_loss: 0.4010 - val_acc: 0.6869\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1153 - acc: 0.7351 - val_loss: 0.4188 - val_acc: 0.6844\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1149 - acc: 0.7351 - val_loss: 0.4317 - val_acc: 0.6838\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1141 - acc: 0.7352 - val_loss: 0.4118 - val_acc: 0.6864\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1129 - acc: 0.7354 - val_loss: 0.4271 - val_acc: 0.6865\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1108 - acc: 0.7357 - val_loss: 0.4289 - val_acc: 0.6874\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1095 - acc: 0.7359 - val_loss: 0.4491 - val_acc: 0.6859\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1091 - acc: 0.7359 - val_loss: 0.4538 - val_acc: 0.6856\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1090 - acc: 0.7359 - val_loss: 0.4618 - val_acc: 0.6859\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1089 - acc: 0.7359 - val_loss: 0.4753 - val_acc: 0.6854\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1166 - acc: 0.7346 - val_loss: 0.4643 - val_acc: 0.6834\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.1169 - acc: 0.7345 - val_loss: 0.4600 - val_acc: 0.6834\n",
      "17921/17921 [==============================] - 1s 57us/sample - loss: 0.7969 - acc: 0.6542\n",
      "35843/35843 [==============================] - 2s 58us/sample - loss: 0.1104 - acc: 0.7357\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 15s 414us/sample - loss: 0.2782 - acc: 0.7065 - val_loss: 0.4060 - val_acc: 0.6791\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1504 - acc: 0.7464 - val_loss: 0.3944 - val_acc: 0.6863\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1344 - acc: 0.7496 - val_loss: 0.3883 - val_acc: 0.6908\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1253 - acc: 0.7513 - val_loss: 0.4009 - val_acc: 0.6910\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1199 - acc: 0.7522 - val_loss: 0.3981 - val_acc: 0.6878\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1157 - acc: 0.7529 - val_loss: 0.4161 - val_acc: 0.6913\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1120 - acc: 0.7536 - val_loss: 0.4051 - val_acc: 0.6907\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1088 - acc: 0.7541 - val_loss: 0.4083 - val_acc: 0.6936\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1084 - acc: 0.7542 - val_loss: 0.4307 - val_acc: 0.6928\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1051 - acc: 0.7547 - val_loss: 0.4225 - val_acc: 0.6933\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1033 - acc: 0.7550 - val_loss: 0.4227 - val_acc: 0.6921\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1023 - acc: 0.7551 - val_loss: 0.4468 - val_acc: 0.6923\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1012 - acc: 0.7552 - val_loss: 0.4534 - val_acc: 0.6925\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1002 - acc: 0.7554 - val_loss: 0.4650 - val_acc: 0.6926\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.0995 - acc: 0.7554 - val_loss: 0.4583 - val_acc: 0.6931\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.0990 - acc: 0.7555 - val_loss: 0.4872 - val_acc: 0.6925\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1011 - acc: 0.7552 - val_loss: 0.4835 - val_acc: 0.6893\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1019 - acc: 0.7550 - val_loss: 0.4623 - val_acc: 0.6905\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1007 - acc: 0.7552 - val_loss: 0.4866 - val_acc: 0.6925\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.0996 - acc: 0.7554 - val_loss: 0.4737 - val_acc: 0.6926\n",
      "17922/17922 [==============================] - 1s 63us/sample - loss: 1.1712 - acc: 0.5982\n",
      "35842/35842 [==============================] - 2s 64us/sample - loss: 0.0994 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 409us/sample - loss: 0.2833 - acc: 0.7156 - val_loss: 0.3906 - val_acc: 0.6900\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1559 - acc: 0.7525 - val_loss: 0.3653 - val_acc: 0.6976\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1367 - acc: 0.7565 - val_loss: 0.3441 - val_acc: 0.7025\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1259 - acc: 0.7587 - val_loss: 0.3284 - val_acc: 0.7054\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1198 - acc: 0.7599 - val_loss: 0.3358 - val_acc: 0.7053\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1161 - acc: 0.7605 - val_loss: 0.3391 - val_acc: 0.7081\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1110 - acc: 0.7616 - val_loss: 0.3375 - val_acc: 0.7082\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1077 - acc: 0.7621 - val_loss: 0.3605 - val_acc: 0.7069\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1058 - acc: 0.7624 - val_loss: 0.3483 - val_acc: 0.7091\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1050 - acc: 0.7626 - val_loss: 0.3529 - val_acc: 0.7084\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1024 - acc: 0.7629 - val_loss: 0.3519 - val_acc: 0.7095\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1008 - acc: 0.7631 - val_loss: 0.3535 - val_acc: 0.7093\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.0996 - acc: 0.7632 - val_loss: 0.3626 - val_acc: 0.7095\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0988 - acc: 0.7633 - val_loss: 0.3644 - val_acc: 0.7102\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0984 - acc: 0.7633 - val_loss: 0.3665 - val_acc: 0.7099\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0977 - acc: 0.7633 - val_loss: 0.3782 - val_acc: 0.7101\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0973 - acc: 0.7634 - val_loss: 0.3795 - val_acc: 0.7104\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0970 - acc: 0.7634 - val_loss: 0.3773 - val_acc: 0.7096\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0966 - acc: 0.7634 - val_loss: 0.3876 - val_acc: 0.7101\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0963 - acc: 0.7634 - val_loss: 0.3957 - val_acc: 0.7089\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.7556 - acc: 0.6531\n",
      "35843/35843 [==============================] - 2s 63us/sample - loss: 0.0965 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 405us/sample - loss: 0.3014 - acc: 0.6852 - val_loss: 0.3853 - val_acc: 0.6779\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1753 - acc: 0.7236 - val_loss: 0.3599 - val_acc: 0.6822\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1588 - acc: 0.7270 - val_loss: 0.3600 - val_acc: 0.6860\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1469 - acc: 0.7293 - val_loss: 0.3669 - val_acc: 0.6849\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1387 - acc: 0.7310 - val_loss: 0.3571 - val_acc: 0.6906\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1330 - acc: 0.7322 - val_loss: 0.3736 - val_acc: 0.6841\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1288 - acc: 0.7329 - val_loss: 0.3791 - val_acc: 0.6836\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1298 - acc: 0.7326 - val_loss: 0.3811 - val_acc: 0.6857\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1243 - acc: 0.7336 - val_loss: 0.3828 - val_acc: 0.6846\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1202 - acc: 0.7344 - val_loss: 0.4061 - val_acc: 0.6874\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1173 - acc: 0.7349 - val_loss: 0.3990 - val_acc: 0.6868\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1192 - acc: 0.7345 - val_loss: 0.4086 - val_acc: 0.6850\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1149 - acc: 0.7352 - val_loss: 0.4161 - val_acc: 0.6841\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1134 - acc: 0.7354 - val_loss: 0.4097 - val_acc: 0.6847\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 6s 169us/sample - loss: 0.1174 - acc: 0.7346 - val_loss: 0.4215 - val_acc: 0.6868\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1122 - acc: 0.7355 - val_loss: 0.4235 - val_acc: 0.6860\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1120 - acc: 0.7355 - val_loss: 0.4281 - val_acc: 0.6860\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1111 - acc: 0.7357 - val_loss: 0.4265 - val_acc: 0.6868\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1101 - acc: 0.7358 - val_loss: 0.4362 - val_acc: 0.6872\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1103 - acc: 0.7358 - val_loss: 0.4319 - val_acc: 0.6865\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.7610 - acc: 0.6584\n",
      "35843/35843 [==============================] - 2s 57us/sample - loss: 0.1093 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 15s 409us/sample - loss: 0.3024 - acc: 0.6977 - val_loss: 0.4269 - val_acc: 0.6823\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1532 - acc: 0.7464 - val_loss: 0.3960 - val_acc: 0.6837\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1375 - acc: 0.7493 - val_loss: 0.3876 - val_acc: 0.6894\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1280 - acc: 0.7511 - val_loss: 0.3904 - val_acc: 0.6905\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1216 - acc: 0.7522 - val_loss: 0.3945 - val_acc: 0.6886\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1185 - acc: 0.7527 - val_loss: 0.4038 - val_acc: 0.6915\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1154 - acc: 0.7533 - val_loss: 0.4051 - val_acc: 0.6901\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1126 - acc: 0.7537 - val_loss: 0.3968 - val_acc: 0.6948\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1087 - acc: 0.7544 - val_loss: 0.4263 - val_acc: 0.6922\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1069 - acc: 0.7546 - val_loss: 0.4083 - val_acc: 0.6931\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1050 - acc: 0.7549 - val_loss: 0.4367 - val_acc: 0.6932\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1049 - acc: 0.7549 - val_loss: 0.4317 - val_acc: 0.6921\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 6s 166us/sample - loss: 0.1027 - acc: 0.7552 - val_loss: 0.4367 - val_acc: 0.6917\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1016 - acc: 0.7553 - val_loss: 0.4412 - val_acc: 0.6916\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.1008 - acc: 0.7554 - val_loss: 0.4548 - val_acc: 0.6921\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 108us/sample - loss: 0.1001 - acc: 0.7554 - val_loss: 0.4622 - val_acc: 0.6924\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.0997 - acc: 0.7554 - val_loss: 0.4618 - val_acc: 0.6920\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1002 - acc: 0.7554 - val_loss: 0.4644 - val_acc: 0.6911\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1005 - acc: 0.7554 - val_loss: 0.4682 - val_acc: 0.6947\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 108us/sample - loss: 0.0993 - acc: 0.7555 - val_loss: 0.4680 - val_acc: 0.6918\n",
      "17922/17922 [==============================] - 1s 61us/sample - loss: 1.1287 - acc: 0.5963\n",
      "35842/35842 [==============================] - 2s 59us/sample - loss: 0.0987 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 409us/sample - loss: 0.3074 - acc: 0.7081 - val_loss: 0.4019 - val_acc: 0.6848\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1593 - acc: 0.7525 - val_loss: 0.3710 - val_acc: 0.6963\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1395 - acc: 0.7566 - val_loss: 0.3422 - val_acc: 0.7023\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1298 - acc: 0.7585 - val_loss: 0.3298 - val_acc: 0.7062\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1229 - acc: 0.7599 - val_loss: 0.3378 - val_acc: 0.7077\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1179 - acc: 0.7607 - val_loss: 0.3361 - val_acc: 0.7075\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1140 - acc: 0.7613 - val_loss: 0.3391 - val_acc: 0.7084\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1107 - acc: 0.7619 - val_loss: 0.3450 - val_acc: 0.7073\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1101 - acc: 0.7620 - val_loss: 0.3274 - val_acc: 0.7079\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1072 - acc: 0.7624 - val_loss: 0.3468 - val_acc: 0.7090\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1044 - acc: 0.7628 - val_loss: 0.3433 - val_acc: 0.7083\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1067 - acc: 0.7623 - val_loss: 0.3415 - val_acc: 0.7101\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1026 - acc: 0.7630 - val_loss: 0.3457 - val_acc: 0.7099\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1011 - acc: 0.7631 - val_loss: 0.3583 - val_acc: 0.7100\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1001 - acc: 0.7632 - val_loss: 0.3561 - val_acc: 0.7105\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0993 - acc: 0.7633 - val_loss: 0.3636 - val_acc: 0.7099\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0987 - acc: 0.7633 - val_loss: 0.3555 - val_acc: 0.7102\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0982 - acc: 0.7633 - val_loss: 0.3592 - val_acc: 0.7097\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.0979 - acc: 0.7634 - val_loss: 0.3754 - val_acc: 0.7106\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0976 - acc: 0.7634 - val_loss: 0.3822 - val_acc: 0.7099\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.7353 - acc: 0.6565\n",
      "35843/35843 [==============================] - 2s 70us/sample - loss: 0.0972 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 405us/sample - loss: 0.3385 - acc: 0.6716 - val_loss: 0.3860 - val_acc: 0.6758\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1818 - acc: 0.7225 - val_loss: 0.3857 - val_acc: 0.6815\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 141us/sample - loss: 0.1616 - acc: 0.7269 - val_loss: 0.3667 - val_acc: 0.6844\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 143us/sample - loss: 0.1541 - acc: 0.7286 - val_loss: 0.3700 - val_acc: 0.6852\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1456 - acc: 0.7301 - val_loss: 0.3674 - val_acc: 0.6835\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1388 - acc: 0.7315 - val_loss: 0.3548 - val_acc: 0.6881\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 106us/sample - loss: 0.1330 - acc: 0.7326 - val_loss: 0.3692 - val_acc: 0.6896\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1308 - acc: 0.7329 - val_loss: 0.3818 - val_acc: 0.6875\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1340 - acc: 0.7318 - val_loss: 0.3657 - val_acc: 0.6890\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1244 - acc: 0.7339 - val_loss: 0.3733 - val_acc: 0.6908\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1223 - acc: 0.7343 - val_loss: 0.3824 - val_acc: 0.6877\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1194 - acc: 0.7348 - val_loss: 0.3858 - val_acc: 0.6871\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1177 - acc: 0.7350 - val_loss: 0.4005 - val_acc: 0.6885\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1161 - acc: 0.7352 - val_loss: 0.3820 - val_acc: 0.6899\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1149 - acc: 0.7354 - val_loss: 0.4044 - val_acc: 0.6857\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1164 - acc: 0.7350 - val_loss: 0.4140 - val_acc: 0.6855\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1144 - acc: 0.7353 - val_loss: 0.3977 - val_acc: 0.6886\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1124 - acc: 0.7356 - val_loss: 0.4039 - val_acc: 0.6879\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1117 - acc: 0.7357 - val_loss: 0.4178 - val_acc: 0.6872\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1110 - acc: 0.7357 - val_loss: 0.4245 - val_acc: 0.6862\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.7203 - acc: 0.6568\n",
      "35843/35843 [==============================] - 2s 57us/sample - loss: 0.1104 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 14s 404us/sample - loss: 0.2581 - acc: 0.7151 - val_loss: 0.4178 - val_acc: 0.6822\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1478 - acc: 0.7466 - val_loss: 0.3994 - val_acc: 0.6872\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 104us/sample - loss: 0.1315 - acc: 0.7499 - val_loss: 0.3903 - val_acc: 0.6904\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1233 - acc: 0.7514 - val_loss: 0.4007 - val_acc: 0.6868\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1180 - acc: 0.7524 - val_loss: 0.3917 - val_acc: 0.6920\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 6s 156us/sample - loss: 0.1136 - acc: 0.7532 - val_loss: 0.3953 - val_acc: 0.6919\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1102 - acc: 0.7538 - val_loss: 0.4056 - val_acc: 0.6944\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1154 - acc: 0.7527 - val_loss: 0.4079 - val_acc: 0.6925\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 106us/sample - loss: 0.1069 - acc: 0.7543 - val_loss: 0.4307 - val_acc: 0.6908\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1044 - acc: 0.7547 - val_loss: 0.4329 - val_acc: 0.6929\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1036 - acc: 0.7549 - val_loss: 0.4551 - val_acc: 0.6904\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1021 - acc: 0.7551 - val_loss: 0.4536 - val_acc: 0.6925\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1008 - acc: 0.7553 - val_loss: 0.4675 - val_acc: 0.6931\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.0999 - acc: 0.7554 - val_loss: 0.4747 - val_acc: 0.6926\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0993 - acc: 0.7554 - val_loss: 0.4822 - val_acc: 0.6926\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.0987 - acc: 0.7555 - val_loss: 0.5057 - val_acc: 0.6908\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1019 - acc: 0.7550 - val_loss: 0.5187 - val_acc: 0.6874 - lo\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 105us/sample - loss: 0.1022 - acc: 0.7549 - val_loss: 0.5013 - val_acc: 0.6923\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 101us/sample - loss: 0.1002 - acc: 0.7553 - val_loss: 0.5022 - val_acc: 0.6915\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 100us/sample - loss: 0.0990 - acc: 0.7554 - val_loss: 0.5085 - val_acc: 0.6926\n",
      "17922/17922 [==============================] - 1s 58us/sample - loss: 1.2047 - acc: 0.6006\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.0992 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 420us/sample - loss: 0.2620 - acc: 0.7243 - val_loss: 0.3841 - val_acc: 0.6857\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1522 - acc: 0.7534 - val_loss: 0.3711 - val_acc: 0.6976\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1360 - acc: 0.7567 - val_loss: 0.3352 - val_acc: 0.7037\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1250 - acc: 0.7588 - val_loss: 0.3401 - val_acc: 0.7055\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1182 - acc: 0.7601 - val_loss: 0.3588 - val_acc: 0.7055\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1144 - acc: 0.7607 - val_loss: 0.3456 - val_acc: 0.7074\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1119 - acc: 0.7613 - val_loss: 0.3429 - val_acc: 0.7055\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1072 - acc: 0.7621 - val_loss: 0.3582 - val_acc: 0.7057\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1052 - acc: 0.7624 - val_loss: 0.3628 - val_acc: 0.7080\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1033 - acc: 0.7628 - val_loss: 0.3643 - val_acc: 0.7079\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1056 - acc: 0.7623 - val_loss: 0.3706 - val_acc: 0.7078\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1015 - acc: 0.7630 - val_loss: 0.3710 - val_acc: 0.7082\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1001 - acc: 0.7631 - val_loss: 0.3741 - val_acc: 0.7086\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.0989 - acc: 0.7633 - val_loss: 0.3807 - val_acc: 0.7094\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0979 - acc: 0.7634 - val_loss: 0.3882 - val_acc: 0.7096\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.0974 - acc: 0.7634 - val_loss: 0.3872 - val_acc: 0.7095\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0971 - acc: 0.7634 - val_loss: 0.3934 - val_acc: 0.7101\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0967 - acc: 0.7635 - val_loss: 0.4006 - val_acc: 0.7099\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.0964 - acc: 0.7635 - val_loss: 0.4038 - val_acc: 0.7097\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.0962 - acc: 0.7635 - val_loss: 0.4166 - val_acc: 0.7093\n",
      "17921/17921 [==============================] - 1s 66us/sample - loss: 0.8441 - acc: 0.6544\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.0960 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 435us/sample - loss: 0.2813 - acc: 0.6925 - val_loss: 0.3917 - val_acc: 0.6778\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1740 - acc: 0.7234 - val_loss: 0.3744 - val_acc: 0.6852\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1546 - acc: 0.7276 - val_loss: 0.3655 - val_acc: 0.6864\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1446 - acc: 0.7298 - val_loss: 0.3884 - val_acc: 0.6837\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1357 - acc: 0.7314 - val_loss: 0.3955 - val_acc: 0.6833\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1335 - acc: 0.7318 - val_loss: 0.3949 - val_acc: 0.6880\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1287 - acc: 0.7328 - val_loss: 0.3946 - val_acc: 0.6848\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1229 - acc: 0.7339 - val_loss: 0.3915 - val_acc: 0.6896\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1201 - acc: 0.7344 - val_loss: 0.3977 - val_acc: 0.6865\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1177 - acc: 0.7348 - val_loss: 0.4306 - val_acc: 0.6836\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1163 - acc: 0.7350 - val_loss: 0.4408 - val_acc: 0.6865\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1166 - acc: 0.7350 - val_loss: 0.4375 - val_acc: 0.6852\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1144 - acc: 0.7353 - val_loss: 0.4515 - val_acc: 0.6839\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1128 - acc: 0.7355 - val_loss: 0.4401 - val_acc: 0.6867\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1114 - acc: 0.7357 - val_loss: 0.4608 - val_acc: 0.6844\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1142 - acc: 0.7352 - val_loss: 0.4516 - val_acc: 0.6866\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1118 - acc: 0.7355 - val_loss: 0.4577 - val_acc: 0.6870\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1100 - acc: 0.7358 - val_loss: 0.4555 - val_acc: 0.6866\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 105us/sample - loss: 0.1091 - acc: 0.7359 - val_loss: 0.4716 - val_acc: 0.6870\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 104us/sample - loss: 0.1086 - acc: 0.7359 - val_loss: 0.4821 - val_acc: 0.6867\n",
      "17921/17921 [==============================] - 1s 58us/sample - loss: 0.8386 - acc: 0.6619\n",
      "35843/35843 [==============================] - 2s 61us/sample - loss: 0.1104 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 15s 406us/sample - loss: 0.2723 - acc: 0.7092 - val_loss: 0.4216 - val_acc: 0.6805\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.1487 - acc: 0.7468 - val_loss: 0.3983 - val_acc: 0.6837\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1365 - acc: 0.7490 - val_loss: 0.3891 - val_acc: 0.6900\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1260 - acc: 0.7511 - val_loss: 0.3984 - val_acc: 0.6910\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1202 - acc: 0.7521 - val_loss: 0.3913 - val_acc: 0.6902\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1172 - acc: 0.7526 - val_loss: 0.3922 - val_acc: 0.6892\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1126 - acc: 0.7535 - val_loss: 0.4129 - val_acc: 0.6916\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1102 - acc: 0.7539 - val_loss: 0.4229 - val_acc: 0.6923\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1079 - acc: 0.7543 - val_loss: 0.4115 - val_acc: 0.6922\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1056 - acc: 0.7547 - val_loss: 0.4396 - val_acc: 0.6922\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 102us/sample - loss: 0.1046 - acc: 0.7548 - val_loss: 0.4434 - val_acc: 0.6896\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1029 - acc: 0.7551 - val_loss: 0.4417 - val_acc: 0.6924\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.1017 - acc: 0.7552 - val_loss: 0.4572 - val_acc: 0.6917\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.1010 - acc: 0.7553 - val_loss: 0.4471 - val_acc: 0.6922\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 103us/sample - loss: 0.1003 - acc: 0.7554 - val_loss: 0.4648 - val_acc: 0.6925\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 98us/sample - loss: 0.0996 - acc: 0.7554 - val_loss: 0.4849 - val_acc: 0.6919\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 99us/sample - loss: 0.0992 - acc: 0.7555 - val_loss: 0.4770 - val_acc: 0.6921\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0988 - acc: 0.7555 - val_loss: 0.4919 - val_acc: 0.6917\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0985 - acc: 0.7555 - val_loss: 0.4934 - val_acc: 0.6918\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 3s 97us/sample - loss: 0.0983 - acc: 0.7555 - val_loss: 0.4986 - val_acc: 0.6921\n",
      "17922/17922 [==============================] - 1s 58us/sample - loss: 1.2210 - acc: 0.5977\n",
      "35842/35842 [==============================] - 2s 58us/sample - loss: 0.0980 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 413us/sample - loss: 0.2857 - acc: 0.7163 - val_loss: 0.3972 - val_acc: 0.6817\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1556 - acc: 0.7528 - val_loss: 0.3575 - val_acc: 0.6972\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1412 - acc: 0.7557 - val_loss: 0.3362 - val_acc: 0.7035\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1292 - acc: 0.7582 - val_loss: 0.3379 - val_acc: 0.7054\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1228 - acc: 0.7594 - val_loss: 0.3304 - val_acc: 0.7062\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1174 - acc: 0.7604 - val_loss: 0.3256 - val_acc: 0.7075\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1141 - acc: 0.7610 - val_loss: 0.3275 - val_acc: 0.7081\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1111 - acc: 0.7615 - val_loss: 0.3369 - val_acc: 0.7088\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1083 - acc: 0.7620 - val_loss: 0.3387 - val_acc: 0.7079\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1063 - acc: 0.7623 - val_loss: 0.3467 - val_acc: 0.7087\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1047 - acc: 0.7626 - val_loss: 0.3492 - val_acc: 0.7085\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1031 - acc: 0.7629 - val_loss: 0.3532 - val_acc: 0.7091\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1017 - acc: 0.7630 - val_loss: 0.3503 - val_acc: 0.7086\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1007 - acc: 0.7631 - val_loss: 0.3538 - val_acc: 0.7083\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1000 - acc: 0.7632 - val_loss: 0.3571 - val_acc: 0.7080\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0990 - acc: 0.7633 - val_loss: 0.3656 - val_acc: 0.7094\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0984 - acc: 0.7633 - val_loss: 0.3679 - val_acc: 0.7082\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.0984 - acc: 0.7633 - val_loss: 0.3777 - val_acc: 0.7091\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.0981 - acc: 0.7634 - val_loss: 0.3733 - val_acc: 0.7087\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1001 - acc: 0.7630 - val_loss: 0.4008 - val_acc: 0.7067\n",
      "17921/17921 [==============================] - 1s 63us/sample - loss: 0.7150 - acc: 0.6528\n",
      "35843/35843 [==============================] - 2s 63us/sample - loss: 0.1009 - acc: 0.7631\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 411us/sample - loss: 0.3047 - acc: 0.6841 - val_loss: 0.4101 - val_acc: 0.6764\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1788 - acc: 0.7228 - val_loss: 0.3841 - val_acc: 0.6828\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1589 - acc: 0.7270 - val_loss: 0.3732 - val_acc: 0.6830\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1480 - acc: 0.7292 - val_loss: 0.3611 - val_acc: 0.6833\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1405 - acc: 0.7305 - val_loss: 0.3790 - val_acc: 0.6824\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1341 - acc: 0.7318 - val_loss: 0.3705 - val_acc: 0.6870\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1286 - acc: 0.7329 - val_loss: 0.3597 - val_acc: 0.6900\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1257 - acc: 0.7334 - val_loss: 0.3767 - val_acc: 0.6854\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1225 - acc: 0.7339 - val_loss: 0.3778 - val_acc: 0.6836\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1194 - acc: 0.7345 - val_loss: 0.3785 - val_acc: 0.6876\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1182 - acc: 0.7347 - val_loss: 0.3938 - val_acc: 0.6866\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1164 - acc: 0.7349 - val_loss: 0.4046 - val_acc: 0.6843\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 3s 98us/sample - loss: 0.1152 - acc: 0.7351 - val_loss: 0.3988 - val_acc: 0.6872\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1142 - acc: 0.7352 - val_loss: 0.4096 - val_acc: 0.6855\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1130 - acc: 0.7354 - val_loss: 0.4142 - val_acc: 0.6862\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1117 - acc: 0.7355 - val_loss: 0.4235 - val_acc: 0.6865\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1110 - acc: 0.7356 - val_loss: 0.4466 - val_acc: 0.6841\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1103 - acc: 0.7357 - val_loss: 0.4243 - val_acc: 0.6868\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1097 - acc: 0.7358 - val_loss: 0.4398 - val_acc: 0.6849\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1093 - acc: 0.7358 - val_loss: 0.4364 - val_acc: 0.6864\n",
      "17921/17921 [==============================] - 1s 59us/sample - loss: 0.7313 - acc: 0.6600\n",
      "35843/35843 [==============================] - 2s 60us/sample - loss: 0.1096 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 15s 414us/sample - loss: 0.3024 - acc: 0.6964 - val_loss: 0.4212 - val_acc: 0.6732\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 107us/sample - loss: 0.1568 - acc: 0.7456 - val_loss: 0.4062 - val_acc: 0.6861\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1409 - acc: 0.7488 - val_loss: 0.4049 - val_acc: 0.6877\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1320 - acc: 0.7503 - val_loss: 0.3956 - val_acc: 0.6898\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1290 - acc: 0.7509 - val_loss: 0.3912 - val_acc: 0.6920\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1215 - acc: 0.7521 - val_loss: 0.4170 - val_acc: 0.6899\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1172 - acc: 0.7529 - val_loss: 0.4022 - val_acc: 0.6924\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1139 - acc: 0.7534 - val_loss: 0.4090 - val_acc: 0.6916\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1110 - acc: 0.7539 - val_loss: 0.4143 - val_acc: 0.6933\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1105 - acc: 0.7540 - val_loss: 0.4294 - val_acc: 0.6913\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 109us/sample - loss: 0.1070 - acc: 0.7545 - val_loss: 0.4216 - val_acc: 0.6927\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1056 - acc: 0.7548 - val_loss: 0.4322 - val_acc: 0.6916\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1047 - acc: 0.7549 - val_loss: 0.4383 - val_acc: 0.6929\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1034 - acc: 0.7550 - val_loss: 0.4577 - val_acc: 0.6915\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1029 - acc: 0.7551 - val_loss: 0.4631 - val_acc: 0.6924\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1018 - acc: 0.7552 - val_loss: 0.4662 - val_acc: 0.6920\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1011 - acc: 0.7553 - val_loss: 0.4813 - val_acc: 0.6913\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 108us/sample - loss: 0.1011 - acc: 0.7552 - val_loss: 0.4608 - val_acc: 0.6904\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1035 - acc: 0.7549 - val_loss: 0.4947 - val_acc: 0.6905\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1009 - acc: 0.7553 - val_loss: 0.4851 - val_acc: 0.6905\n",
      "17922/17922 [==============================] - 1s 66us/sample - loss: 1.2299 - acc: 0.5929\n",
      "35842/35842 [==============================] - 2s 65us/sample - loss: 0.1002 - acc: 0.7553\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 411us/sample - loss: 0.3100 - acc: 0.7095 - val_loss: 0.3935 - val_acc: 0.6859\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1653 - acc: 0.7513 - val_loss: 0.3637 - val_acc: 0.6962\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1453 - acc: 0.7552 - val_loss: 0.3432 - val_acc: 0.7028\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1343 - acc: 0.7576 - val_loss: 0.3388 - val_acc: 0.7046\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1272 - acc: 0.7589 - val_loss: 0.3399 - val_acc: 0.7045\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1216 - acc: 0.7600 - val_loss: 0.3359 - val_acc: 0.7071\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1173 - acc: 0.7608 - val_loss: 0.3422 - val_acc: 0.7084\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1146 - acc: 0.7612 - val_loss: 0.3395 - val_acc: 0.7085\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1112 - acc: 0.7618 - val_loss: 0.3475 - val_acc: 0.7095\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1093 - acc: 0.7621 - val_loss: 0.3589 - val_acc: 0.7083\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 102us/sample - loss: 0.1083 - acc: 0.7623 - val_loss: 0.3470 - val_acc: 0.7097\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1057 - acc: 0.7626 - val_loss: 0.3491 - val_acc: 0.7094\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 103us/sample - loss: 0.1040 - acc: 0.7628 - val_loss: 0.3549 - val_acc: 0.7083\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1027 - acc: 0.7630 - val_loss: 0.3601 - val_acc: 0.7094\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1020 - acc: 0.7631 - val_loss: 0.3544 - val_acc: 0.7098\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1010 - acc: 0.7631 - val_loss: 0.3753 - val_acc: 0.7090\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1003 - acc: 0.7632 - val_loss: 0.3782 - val_acc: 0.7101\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0996 - acc: 0.7633 - val_loss: 0.3825 - val_acc: 0.7103\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.0991 - acc: 0.7633 - val_loss: 0.3772 - val_acc: 0.7099\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 107us/sample - loss: 0.0985 - acc: 0.7633 - val_loss: 0.3764 - val_acc: 0.7098\n",
      "17921/17921 [==============================] - 1s 64us/sample - loss: 0.7051 - acc: 0.6549\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.0983 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 15s 411us/sample - loss: 0.3315 - acc: 0.6752 - val_loss: 0.3893 - val_acc: 0.6790\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 3s 96us/sample - loss: 0.1875 - acc: 0.7213 - val_loss: 0.3833 - val_acc: 0.6814\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1665 - acc: 0.7257 - val_loss: 0.3812 - val_acc: 0.6788\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1584 - acc: 0.7274 - val_loss: 0.3705 - val_acc: 0.6811\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1482 - acc: 0.7296 - val_loss: 0.3685 - val_acc: 0.6833\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 3s 97us/sample - loss: 0.1419 - acc: 0.7308 - val_loss: 0.3742 - val_acc: 0.6871\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1369 - acc: 0.7318 - val_loss: 0.3716 - val_acc: 0.6849\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1325 - acc: 0.7326 - val_loss: 0.3722 - val_acc: 0.6889\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 101us/sample - loss: 0.1313 - acc: 0.7328 - val_loss: 0.3652 - val_acc: 0.6879\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 100us/sample - loss: 0.1265 - acc: 0.7336 - val_loss: 0.3840 - val_acc: 0.6868\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1237 - acc: 0.7341 - val_loss: 0.3799 - val_acc: 0.6853\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 99us/sample - loss: 0.1215 - acc: 0.7345 - val_loss: 0.3828 - val_acc: 0.6876\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 98us/sample - loss: 0.1196 - acc: 0.7348 - val_loss: 0.4059 - val_acc: 0.6826\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1188 - acc: 0.7349 - val_loss: 0.3975 - val_acc: 0.6865\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 109us/sample - loss: 0.1168 - acc: 0.7352 - val_loss: 0.3906 - val_acc: 0.6873\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 108us/sample - loss: 0.1153 - acc: 0.7354 - val_loss: 0.4055 - val_acc: 0.6840\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1169 - acc: 0.7350 - val_loss: 0.4095 - val_acc: 0.6857\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1137 - acc: 0.7355 - val_loss: 0.4082 - val_acc: 0.6878\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1132 - acc: 0.7356 - val_loss: 0.4207 - val_acc: 0.6857\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 110us/sample - loss: 0.1117 - acc: 0.7357 - val_loss: 0.4303 - val_acc: 0.6862\n",
      "17921/17921 [==============================] - 1s 65us/sample - loss: 0.7392 - acc: 0.6583\n",
      "35843/35843 [==============================] - 2s 64us/sample - loss: 0.1112 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 16s 456us/sample - loss: 0.2338 - acc: 0.7227 - val_loss: 0.4111 - val_acc: 0.6873\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1488 - acc: 0.7480 - val_loss: 0.4158 - val_acc: 0.6887\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1377 - acc: 0.7504 - val_loss: 0.3948 - val_acc: 0.6935\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1292 - acc: 0.7519 - val_loss: 0.3908 - val_acc: 0.6939\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1235 - acc: 0.7529 - val_loss: 0.3978 - val_acc: 0.6980\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1213 - acc: 0.7534 - val_loss: 0.3978 - val_acc: 0.6945\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1168 - acc: 0.7541 - val_loss: 0.4048 - val_acc: 0.6929\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1140 - acc: 0.7545 - val_loss: 0.3912 - val_acc: 0.6970\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1122 - acc: 0.7548 - val_loss: 0.4138 - val_acc: 0.6972\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1114 - acc: 0.7549 - val_loss: 0.3986 - val_acc: 0.6958\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1103 - acc: 0.7551 - val_loss: 0.3942 - val_acc: 0.6945\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1082 - acc: 0.7553 - val_loss: 0.4027 - val_acc: 0.6949\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1074 - acc: 0.7554 - val_loss: 0.4095 - val_acc: 0.6961\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1064 - acc: 0.7554 - val_loss: 0.4109 - val_acc: 0.6954\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1064 - acc: 0.7554 - val_loss: 0.4173 - val_acc: 0.6967\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1052 - acc: 0.7555 - val_loss: 0.4223 - val_acc: 0.6925\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1099 - acc: 0.7552 - val_loss: 0.4247 - val_acc: 0.6962\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1056 - acc: 0.7555 - val_loss: 0.4370 - val_acc: 0.6963\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1041 - acc: 0.7556 - val_loss: 0.4199 - val_acc: 0.6972\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1032 - acc: 0.7556 - val_loss: 0.4363 - val_acc: 0.6976\n",
      "17922/17922 [==============================] - 1s 70us/sample - loss: 1.0236 - acc: 0.6154\n",
      "35842/35842 [==============================] - 2s 68us/sample - loss: 0.1035 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 446us/sample - loss: 0.2416 - acc: 0.7310 - val_loss: 0.3729 - val_acc: 0.6897\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1527 - acc: 0.7546 - val_loss: 0.3519 - val_acc: 0.7024\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1390 - acc: 0.7576 - val_loss: 0.3447 - val_acc: 0.7043\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1299 - acc: 0.7596 - val_loss: 0.3596 - val_acc: 0.7082\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1248 - acc: 0.7605 - val_loss: 0.3453 - val_acc: 0.7090\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1210 - acc: 0.7612 - val_loss: 0.3388 - val_acc: 0.7098\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1173 - acc: 0.7618 - val_loss: 0.3490 - val_acc: 0.7106\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1140 - acc: 0.7623 - val_loss: 0.3529 - val_acc: 0.7102\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1126 - acc: 0.7626 - val_loss: 0.3423 - val_acc: 0.7114\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1102 - acc: 0.7629 - val_loss: 0.3455 - val_acc: 0.7119\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1085 - acc: 0.7631 - val_loss: 0.3571 - val_acc: 0.7106\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1076 - acc: 0.7632 - val_loss: 0.3604 - val_acc: 0.7106\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1070 - acc: 0.7632 - val_loss: 0.3594 - val_acc: 0.7079\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1066 - acc: 0.7633 - val_loss: 0.3800 - val_acc: 0.7093\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1079 - acc: 0.7631 - val_loss: 0.3627 - val_acc: 0.7111\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1049 - acc: 0.7634 - val_loss: 0.3589 - val_acc: 0.7120\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1036 - acc: 0.7634 - val_loss: 0.3509 - val_acc: 0.7119\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1033 - acc: 0.7634 - val_loss: 0.3663 - val_acc: 0.7115\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1026 - acc: 0.7635 - val_loss: 0.3616 - val_acc: 0.7120\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1025 - acc: 0.7635 - val_loss: 0.3913 - val_acc: 0.7116\n",
      "17921/17921 [==============================] - 1s 70us/sample - loss: 0.7960 - acc: 0.6574\n",
      "35843/35843 [==============================] - 3s 70us/sample - loss: 0.1022 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 486us/sample - loss: 0.2623 - acc: 0.6987 - val_loss: 0.4274 - val_acc: 0.6671\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1805 - acc: 0.7235 - val_loss: 0.3972 - val_acc: 0.6758\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1608 - acc: 0.7281 - val_loss: 0.3718 - val_acc: 0.6861\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1498 - acc: 0.7305 - val_loss: 0.3666 - val_acc: 0.6892\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1442 - acc: 0.7318 - val_loss: 0.3598 - val_acc: 0.6893\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1381 - acc: 0.7330 - val_loss: 0.3826 - val_acc: 0.6930\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1376 - acc: 0.7331 - val_loss: 0.3685 - val_acc: 0.6897\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1316 - acc: 0.7341 - val_loss: 0.3778 - val_acc: 0.6946\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1340 - acc: 0.7338 - val_loss: 0.3628 - val_acc: 0.6892\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1271 - acc: 0.7349 - val_loss: 0.3688 - val_acc: 0.6905\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1250 - acc: 0.7352 - val_loss: 0.3557 - val_acc: 0.6910\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1240 - acc: 0.7353 - val_loss: 0.3786 - val_acc: 0.6919\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1221 - acc: 0.7355 - val_loss: 0.3752 - val_acc: 0.6892\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1208 - acc: 0.7356 - val_loss: 0.3834 - val_acc: 0.6873\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1198 - acc: 0.7358 - val_loss: 0.3618 - val_acc: 0.6888\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 112us/sample - loss: 0.1192 - acc: 0.7358 - val_loss: 0.3819 - val_acc: 0.6856\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1186 - acc: 0.7358 - val_loss: 0.3849 - val_acc: 0.6894\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 111us/sample - loss: 0.1180 - acc: 0.7359 - val_loss: 0.3905 - val_acc: 0.6915\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1176 - acc: 0.7359 - val_loss: 0.3978 - val_acc: 0.6887\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1165 - acc: 0.7359 - val_loss: 0.3984 - val_acc: 0.6852\n",
      "17921/17921 [==============================] - 1s 65us/sample - loss: 0.6736 - acc: 0.66110s - loss: 0.1\n",
      "35843/35843 [==============================] - 2s 66us/sample - loss: 0.1187 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 31s 851us/sample - loss: 0.2524 - acc: 0.7159 - val_loss: 0.4115 - val_acc: 0.6816\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 9s 239us/sample - loss: 0.1531 - acc: 0.7470 - val_loss: 0.3895 - val_acc: 0.6919\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 9s 259us/sample - loss: 0.1383 - acc: 0.7502 - val_loss: 0.3839 - val_acc: 0.6908\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 9s 242us/sample - loss: 0.1307 - acc: 0.7517 - val_loss: 0.4437 - val_acc: 0.6752\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 8s 234us/sample - loss: 0.1323 - acc: 0.7514 - val_loss: 0.3813 - val_acc: 0.6916\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 110us/sample - loss: 0.1218 - acc: 0.7533 - val_loss: 0.3886 - val_acc: 0.6945\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1216 - acc: 0.7534 - val_loss: 0.3890 - val_acc: 0.6959\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1166 - acc: 0.7542 - val_loss: 0.3861 - val_acc: 0.6970\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1147 - acc: 0.7545 - val_loss: 0.3989 - val_acc: 0.6935\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1133 - acc: 0.7547 - val_loss: 0.3916 - val_acc: 0.6954\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1114 - acc: 0.7550 - val_loss: 0.3998 - val_acc: 0.6947\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1105 - acc: 0.7551 - val_loss: 0.4268 - val_acc: 0.6972\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1126 - acc: 0.7548 - val_loss: 0.3933 - val_acc: 0.6960\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1092 - acc: 0.7552 - val_loss: 0.3909 - val_acc: 0.6966\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1079 - acc: 0.7554 - val_loss: 0.4150 - val_acc: 0.6960\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1068 - acc: 0.7554 - val_loss: 0.4221 - val_acc: 0.6930\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1078 - acc: 0.7554 - val_loss: 0.4229 - val_acc: 0.6945\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1094 - acc: 0.7552 - val_loss: 0.4033 - val_acc: 0.6979\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1061 - acc: 0.7555 - val_loss: 0.4186 - val_acc: 0.6971\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1057 - acc: 0.7555 - val_loss: 0.4294 - val_acc: 0.6965\n",
      "17922/17922 [==============================] - 1s 69us/sample - loss: 1.0127 - acc: 0.6088\n",
      "35842/35842 [==============================] - 3s 71us/sample - loss: 0.1046 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 446us/sample - loss: 0.2574 - acc: 0.7253 - val_loss: 0.3735 - val_acc: 0.6881\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1565 - acc: 0.7537 - val_loss: 0.3534 - val_acc: 0.6989\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1412 - acc: 0.7571 - val_loss: 0.3473 - val_acc: 0.7036\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1321 - acc: 0.7591 - val_loss: 0.3492 - val_acc: 0.7074\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1266 - acc: 0.7602 - val_loss: 0.3624 - val_acc: 0.7075\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1251 - acc: 0.7606 - val_loss: 0.3387 - val_acc: 0.7095\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1190 - acc: 0.7616 - val_loss: 0.3489 - val_acc: 0.7102\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1162 - acc: 0.7621 - val_loss: 0.3307 - val_acc: 0.7103\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1138 - acc: 0.7624 - val_loss: 0.3452 - val_acc: 0.7112\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1119 - acc: 0.7627 - val_loss: 0.3491 - val_acc: 0.7116\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1105 - acc: 0.7629 - val_loss: 0.3435 - val_acc: 0.7112\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1095 - acc: 0.7630 - val_loss: 0.3628 - val_acc: 0.7088\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1104 - acc: 0.7629 - val_loss: 0.3503 - val_acc: 0.7115\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1082 - acc: 0.7631 - val_loss: 0.3476 - val_acc: 0.7121\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1063 - acc: 0.7633 - val_loss: 0.3622 - val_acc: 0.7111\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1057 - acc: 0.7633 - val_loss: 0.3517 - val_acc: 0.7115\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1047 - acc: 0.7634 - val_loss: 0.3455 - val_acc: 0.7118\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1050 - acc: 0.7634 - val_loss: 0.3681 - val_acc: 0.7109\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1040 - acc: 0.7634 - val_loss: 0.3721 - val_acc: 0.7121\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1032 - acc: 0.7635 - val_loss: 0.3554 - val_acc: 0.7114\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6740 - acc: 0.6597\n",
      "35843/35843 [==============================] - 2s 68us/sample - loss: 0.1024 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 476us/sample - loss: 0.2829 - acc: 0.6923 - val_loss: 0.3979 - val_acc: 0.6747\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1792 - acc: 0.7240 - val_loss: 0.3760 - val_acc: 0.6809\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1614 - acc: 0.7282 - val_loss: 0.3617 - val_acc: 0.6864\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1517 - acc: 0.7304 - val_loss: 0.3682 - val_acc: 0.6885\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1446 - acc: 0.7318 - val_loss: 0.3699 - val_acc: 0.6842\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1412 - acc: 0.7325 - val_loss: 0.3687 - val_acc: 0.6863\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1369 - acc: 0.7333 - val_loss: 0.3801 - val_acc: 0.6845\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1341 - acc: 0.7338 - val_loss: 0.3637 - val_acc: 0.6903\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1307 - acc: 0.7344 - val_loss: 0.3718 - val_acc: 0.6890\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1296 - acc: 0.7346 - val_loss: 0.3748 - val_acc: 0.6928\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1266 - acc: 0.7351 - val_loss: 0.3595 - val_acc: 0.6917\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1255 - acc: 0.7352 - val_loss: 0.3766 - val_acc: 0.6875\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1235 - acc: 0.7354 - val_loss: 0.3741 - val_acc: 0.6902\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1227 - acc: 0.7355 - val_loss: 0.3848 - val_acc: 0.6889\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1216 - acc: 0.7356 - val_loss: 0.3742 - val_acc: 0.6900\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1202 - acc: 0.7358 - val_loss: 0.3798 - val_acc: 0.6916\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1204 - acc: 0.7357 - val_loss: 0.3859 - val_acc: 0.6900\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1188 - acc: 0.7358 - val_loss: 0.3776 - val_acc: 0.6905\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1177 - acc: 0.7359 - val_loss: 0.4171 - val_acc: 0.6922\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1217 - acc: 0.7356 - val_loss: 0.3742 - val_acc: 0.6899\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6137 - acc: 0.6712\n",
      "35843/35843 [==============================] - 2s 69us/sample - loss: 0.1178 - acc: 0.7360\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 16s 459us/sample - loss: 0.2751 - acc: 0.7085 - val_loss: 0.4148 - val_acc: 0.6860\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1540 - acc: 0.7471 - val_loss: 0.3877 - val_acc: 0.6905\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1400 - acc: 0.7501 - val_loss: 0.3883 - val_acc: 0.6920\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1325 - acc: 0.7515 - val_loss: 0.3928 - val_acc: 0.6933\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1278 - acc: 0.7524 - val_loss: 0.4057 - val_acc: 0.6955\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1236 - acc: 0.7532 - val_loss: 0.3936 - val_acc: 0.6961\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1205 - acc: 0.7537 - val_loss: 0.4059 - val_acc: 0.6963\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1216 - acc: 0.7536 - val_loss: 0.4070 - val_acc: 0.6883\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1203 - acc: 0.7538 - val_loss: 0.4029 - val_acc: 0.6967\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1159 - acc: 0.7545 - val_loss: 0.4098 - val_acc: 0.6938\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1138 - acc: 0.7548 - val_loss: 0.4074 - val_acc: 0.6903\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1155 - acc: 0.7545 - val_loss: 0.4063 - val_acc: 0.6948\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1116 - acc: 0.7550 - val_loss: 0.4027 - val_acc: 0.6961\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1116 - acc: 0.7550 - val_loss: 0.4138 - val_acc: 0.6965\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1091 - acc: 0.7553 - val_loss: 0.4135 - val_acc: 0.6962\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1085 - acc: 0.7553 - val_loss: 0.4088 - val_acc: 0.6967\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1075 - acc: 0.7554 - val_loss: 0.4102 - val_acc: 0.6953\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1082 - acc: 0.7553 - val_loss: 0.4235 - val_acc: 0.6980\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1081 - acc: 0.7554 - val_loss: 0.4212 - val_acc: 0.6959\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1067 - acc: 0.7555 - val_loss: 0.4182 - val_acc: 0.6968\n",
      "17922/17922 [==============================] - 1s 70us/sample - loss: 0.9781 - acc: 0.6103\n",
      "35842/35842 [==============================] - 2s 68us/sample - loss: 0.1051 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 449us/sample - loss: 0.2818 - acc: 0.7175 - val_loss: 0.4001 - val_acc: 0.6850\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1602 - acc: 0.7531 - val_loss: 0.3603 - val_acc: 0.6955\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1448 - acc: 0.7563 - val_loss: 0.3494 - val_acc: 0.7026\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1359 - acc: 0.7584 - val_loss: 0.3389 - val_acc: 0.7076\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1288 - acc: 0.7598 - val_loss: 0.3426 - val_acc: 0.7075\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1241 - acc: 0.7607 - val_loss: 0.3352 - val_acc: 0.7083\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1206 - acc: 0.7615 - val_loss: 0.3439 - val_acc: 0.7095\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1184 - acc: 0.7618 - val_loss: 0.3509 - val_acc: 0.7083\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1182 - acc: 0.7619 - val_loss: 0.3566 - val_acc: 0.7103\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1191 - acc: 0.7618 - val_loss: 0.3362 - val_acc: 0.7106\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1130 - acc: 0.7627 - val_loss: 0.3315 - val_acc: 0.7100\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1125 - acc: 0.7628 - val_loss: 0.3376 - val_acc: 0.7115\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1100 - acc: 0.7630 - val_loss: 0.3480 - val_acc: 0.7118\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1089 - acc: 0.7631 - val_loss: 0.3544 - val_acc: 0.7109\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1080 - acc: 0.7632 - val_loss: 0.3686 - val_acc: 0.7114\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1078 - acc: 0.7632 - val_loss: 0.3417 - val_acc: 0.7102\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1065 - acc: 0.7634 - val_loss: 0.3540 - val_acc: 0.7119\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1056 - acc: 0.7634 - val_loss: 0.3844 - val_acc: 0.7081\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1091 - acc: 0.7631 - val_loss: 0.3547 - val_acc: 0.7117\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1050 - acc: 0.7634 - val_loss: 0.3514 - val_acc: 0.7115\n",
      "17921/17921 [==============================] - 1s 68us/sample - loss: 0.6740 - acc: 0.6577\n",
      "35843/35843 [==============================] - 3s 71us/sample - loss: 0.1059 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 451us/sample - loss: 0.3038 - acc: 0.6842 - val_loss: 0.3874 - val_acc: 0.6841\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1791 - acc: 0.7241 - val_loss: 0.3676 - val_acc: 0.6831\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1644 - acc: 0.7277 - val_loss: 0.3704 - val_acc: 0.6912\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1609 - acc: 0.7287 - val_loss: 0.3735 - val_acc: 0.6804\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1529 - acc: 0.7304 - val_loss: 0.3584 - val_acc: 0.6884\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1442 - acc: 0.7320 - val_loss: 0.3598 - val_acc: 0.6849\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1463 - acc: 0.7317 - val_loss: 0.3629 - val_acc: 0.6908\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1401 - acc: 0.7328 - val_loss: 0.3588 - val_acc: 0.6927\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1400 - acc: 0.7328 - val_loss: 0.3734 - val_acc: 0.6880\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1334 - acc: 0.7340 - val_loss: 0.3669 - val_acc: 0.6884\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1315 - acc: 0.7343 - val_loss: 0.3655 - val_acc: 0.6906\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1295 - acc: 0.7346 - val_loss: 0.3786 - val_acc: 0.6889\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1276 - acc: 0.7349 - val_loss: 0.3682 - val_acc: 0.6899\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1263 - acc: 0.7351 - val_loss: 0.3760 - val_acc: 0.6893\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1251 - acc: 0.7353 - val_loss: 0.3756 - val_acc: 0.6881\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1239 - acc: 0.7354 - val_loss: 0.3770 - val_acc: 0.6867\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1235 - acc: 0.7355 - val_loss: 0.3747 - val_acc: 0.6884\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1220 - acc: 0.7356 - val_loss: 0.3890 - val_acc: 0.6881\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1216 - acc: 0.7357 - val_loss: 0.4019 - val_acc: 0.6847\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1236 - acc: 0.7355 - val_loss: 0.3888 - val_acc: 0.6929\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6391 - acc: 0.6774\n",
      "35843/35843 [==============================] - 2s 70us/sample - loss: 0.1210 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 16s 456us/sample - loss: 0.2416 - acc: 0.7205 - val_loss: 0.4256 - val_acc: 0.6771\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1546 - acc: 0.7467 - val_loss: 0.3911 - val_acc: 0.6909\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1386 - acc: 0.7500 - val_loss: 0.3887 - val_acc: 0.6942\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1309 - acc: 0.7515 - val_loss: 0.4042 - val_acc: 0.6924\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1256 - acc: 0.7525 - val_loss: 0.3988 - val_acc: 0.6915\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1219 - acc: 0.7532 - val_loss: 0.4014 - val_acc: 0.6938\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1194 - acc: 0.7537 - val_loss: 0.4037 - val_acc: 0.6956\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1176 - acc: 0.7541 - val_loss: 0.3997 - val_acc: 0.6964\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1176 - acc: 0.7540 - val_loss: 0.4080 - val_acc: 0.6937\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1145 - acc: 0.7546 - val_loss: 0.4135 - val_acc: 0.6941\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1137 - acc: 0.7547 - val_loss: 0.4224 - val_acc: 0.6956\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1109 - acc: 0.7550 - val_loss: 0.4219 - val_acc: 0.6923\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1102 - acc: 0.7551 - val_loss: 0.4181 - val_acc: 0.6950\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1086 - acc: 0.7552 - val_loss: 0.4292 - val_acc: 0.6961\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1076 - acc: 0.7553 - val_loss: 0.4308 - val_acc: 0.6940\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1069 - acc: 0.7554 - val_loss: 0.4217 - val_acc: 0.6962\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1066 - acc: 0.7554 - val_loss: 0.4326 - val_acc: 0.6965\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1059 - acc: 0.7555 - val_loss: 0.4211 - val_acc: 0.6959\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1057 - acc: 0.7555 - val_loss: 0.4159 - val_acc: 0.6979\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1058 - acc: 0.7554 - val_loss: 0.4116 - val_acc: 0.6947\n",
      "17922/17922 [==============================] - 1s 70us/sample - loss: 0.9483 - acc: 0.6091\n",
      "35842/35842 [==============================] - 2s 68us/sample - loss: 0.1048 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 457us/sample - loss: 0.2483 - acc: 0.7290 - val_loss: 0.4029 - val_acc: 0.6899\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1568 - acc: 0.7536 - val_loss: 0.3546 - val_acc: 0.6996\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1411 - acc: 0.7571 - val_loss: 0.3493 - val_acc: 0.7040\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1329 - acc: 0.7588 - val_loss: 0.3734 - val_acc: 0.7053\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1287 - acc: 0.7598 - val_loss: 0.3455 - val_acc: 0.7069\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1225 - acc: 0.7609 - val_loss: 0.3573 - val_acc: 0.7085\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1227 - acc: 0.7610 - val_loss: 0.3364 - val_acc: 0.7097\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1176 - acc: 0.7618 - val_loss: 0.3590 - val_acc: 0.7088\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1151 - acc: 0.7622 - val_loss: 0.3578 - val_acc: 0.7087\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1139 - acc: 0.7624 - val_loss: 0.3458 - val_acc: 0.7107\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1114 - acc: 0.7627 - val_loss: 0.3514 - val_acc: 0.7097\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1107 - acc: 0.7628 - val_loss: 0.3421 - val_acc: 0.7097\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1091 - acc: 0.7630 - val_loss: 0.3452 - val_acc: 0.7085\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1108 - acc: 0.7627 - val_loss: 0.3623 - val_acc: 0.7087\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1090 - acc: 0.7630 - val_loss: 0.3542 - val_acc: 0.7114\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1068 - acc: 0.7632 - val_loss: 0.3715 - val_acc: 0.7101\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1060 - acc: 0.7633 - val_loss: 0.3686 - val_acc: 0.7103\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1073 - acc: 0.7631 - val_loss: 0.3717 - val_acc: 0.7089\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1058 - acc: 0.7633 - val_loss: 0.3608 - val_acc: 0.7103\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1043 - acc: 0.7634 - val_loss: 0.3867 - val_acc: 0.7102\n",
      "17921/17921 [==============================] - 1s 68us/sample - loss: 0.7781 - acc: 0.6583\n",
      "35843/35843 [==============================] - 3s 70us/sample - loss: 0.1044 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 458us/sample - loss: 0.2683 - acc: 0.6970 - val_loss: 0.3760 - val_acc: 0.6778\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1767 - acc: 0.7246 - val_loss: 0.3682 - val_acc: 0.6794\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1599 - acc: 0.7285 - val_loss: 0.3687 - val_acc: 0.6857\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1502 - acc: 0.7306 - val_loss: 0.3616 - val_acc: 0.6911\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1444 - acc: 0.7319 - val_loss: 0.3679 - val_acc: 0.6873\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1400 - acc: 0.7328 - val_loss: 0.3663 - val_acc: 0.6893\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1363 - acc: 0.7334 - val_loss: 0.3786 - val_acc: 0.6857\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1334 - acc: 0.7339 - val_loss: 0.3758 - val_acc: 0.6900\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1314 - acc: 0.7343 - val_loss: 0.3571 - val_acc: 0.6910\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1283 - acc: 0.7348 - val_loss: 0.3810 - val_acc: 0.6928\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1268 - acc: 0.7350 - val_loss: 0.3755 - val_acc: 0.6910\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1247 - acc: 0.7352 - val_loss: 0.4017 - val_acc: 0.6833\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1265 - acc: 0.7350 - val_loss: 0.3746 - val_acc: 0.6912\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1233 - acc: 0.7354 - val_loss: 0.3993 - val_acc: 0.6883\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1282 - acc: 0.7347 - val_loss: 0.3994 - val_acc: 0.6870\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1222 - acc: 0.7355 - val_loss: 0.3954 - val_acc: 0.6878\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1208 - acc: 0.7356 - val_loss: 0.3836 - val_acc: 0.6895\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1195 - acc: 0.7357 - val_loss: 0.3914 - val_acc: 0.6876\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1187 - acc: 0.7358 - val_loss: 0.3900 - val_acc: 0.6896\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1195 - acc: 0.7357 - val_loss: 0.3819 - val_acc: 0.6879\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6304 - acc: 0.6685\n",
      "35843/35843 [==============================] - 3s 70us/sample - loss: 0.1228 - acc: 0.7357\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 470us/sample - loss: 0.2559 - acc: 0.7153 - val_loss: 0.4332 - val_acc: 0.6822\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1529 - acc: 0.7472 - val_loss: 0.3826 - val_acc: 0.6883\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1397 - acc: 0.7501 - val_loss: 0.4054 - val_acc: 0.6917\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1324 - acc: 0.7514 - val_loss: 0.3906 - val_acc: 0.6924\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1283 - acc: 0.7522 - val_loss: 0.4039 - val_acc: 0.6942\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1232 - acc: 0.7530 - val_loss: 0.4000 - val_acc: 0.6935\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1207 - acc: 0.7536 - val_loss: 0.3909 - val_acc: 0.6958\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1180 - acc: 0.7540 - val_loss: 0.4046 - val_acc: 0.6938\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1197 - acc: 0.7538 - val_loss: 0.4004 - val_acc: 0.6944\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1150 - acc: 0.7545 - val_loss: 0.4009 - val_acc: 0.6962\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1133 - acc: 0.7547 - val_loss: 0.4134 - val_acc: 0.6956\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1116 - acc: 0.7550 - val_loss: 0.4050 - val_acc: 0.6952\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1107 - acc: 0.7551 - val_loss: 0.4101 - val_acc: 0.6975\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1108 - acc: 0.7551 - val_loss: 0.4084 - val_acc: 0.6955\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1095 - acc: 0.7552 - val_loss: 0.4136 - val_acc: 0.6960\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1080 - acc: 0.7553 - val_loss: 0.4158 - val_acc: 0.6958\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1076 - acc: 0.7554 - val_loss: 0.4290 - val_acc: 0.6962\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1086 - acc: 0.7553 - val_loss: 0.4140 - val_acc: 0.6949\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1068 - acc: 0.7554 - val_loss: 0.4240 - val_acc: 0.6955\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1069 - acc: 0.7554 - val_loss: 0.4235 - val_acc: 0.6907\n",
      "17922/17922 [==============================] - 1s 69us/sample - loss: 0.9765 - acc: 0.5926\n",
      "35842/35842 [==============================] - 2s 68us/sample - loss: 0.1136 - acc: 0.7551\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 468us/sample - loss: 0.2674 - acc: 0.7228 - val_loss: 0.3936 - val_acc: 0.6860\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1597 - acc: 0.7530 - val_loss: 0.3671 - val_acc: 0.6939\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1446 - acc: 0.7565 - val_loss: 0.3554 - val_acc: 0.7029\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1346 - acc: 0.7586 - val_loss: 0.3548 - val_acc: 0.7064\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1293 - acc: 0.7597 - val_loss: 0.3423 - val_acc: 0.7075\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1259 - acc: 0.7604 - val_loss: 0.3375 - val_acc: 0.7078\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1215 - acc: 0.7612 - val_loss: 0.3457 - val_acc: 0.7089\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1190 - acc: 0.7616 - val_loss: 0.3483 - val_acc: 0.7089\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1168 - acc: 0.7620 - val_loss: 0.3470 - val_acc: 0.7106\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1145 - acc: 0.7624 - val_loss: 0.3380 - val_acc: 0.7110\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1135 - acc: 0.7625 - val_loss: 0.3953 - val_acc: 0.7108\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1134 - acc: 0.7626 - val_loss: 0.3567 - val_acc: 0.7109\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1103 - acc: 0.7629 - val_loss: 0.3868 - val_acc: 0.7092\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1189 - acc: 0.7617 - val_loss: 0.3712 - val_acc: 0.7104\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1105 - acc: 0.7628 - val_loss: 0.3921 - val_acc: 0.7099\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1120 - acc: 0.7627 - val_loss: 0.3563 - val_acc: 0.7108\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1084 - acc: 0.7631 - val_loss: 0.3550 - val_acc: 0.7102\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1075 - acc: 0.7632 - val_loss: 0.3711 - val_acc: 0.7110\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1065 - acc: 0.7633 - val_loss: 0.3901 - val_acc: 0.7106\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1083 - acc: 0.7630 - val_loss: 0.3809 - val_acc: 0.7094\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.7484 - acc: 0.6567\n",
      "35843/35843 [==============================] - 2s 69us/sample - loss: 0.1085 - acc: 0.7630\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 458us/sample - loss: 0.2821 - acc: 0.6922 - val_loss: 0.3776 - val_acc: 0.6815\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1833 - acc: 0.7231 - val_loss: 0.3745 - val_acc: 0.6819\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1650 - acc: 0.7274 - val_loss: 0.3791 - val_acc: 0.6810\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1566 - acc: 0.7295 - val_loss: 0.3716 - val_acc: 0.6828\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1503 - acc: 0.7307 - val_loss: 0.3520 - val_acc: 0.6872\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1448 - acc: 0.7319 - val_loss: 0.3655 - val_acc: 0.6837\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1399 - acc: 0.7328 - val_loss: 0.3625 - val_acc: 0.6894\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1369 - acc: 0.7333 - val_loss: 0.3705 - val_acc: 0.6841\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1341 - acc: 0.7339 - val_loss: 0.3624 - val_acc: 0.6908\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1309 - acc: 0.7344 - val_loss: 0.3657 - val_acc: 0.6891\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1293 - acc: 0.7346 - val_loss: 0.3962 - val_acc: 0.6864\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1310 - acc: 0.7345 - val_loss: 0.3797 - val_acc: 0.6876\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1291 - acc: 0.7348 - val_loss: 0.3857 - val_acc: 0.6868\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1255 - acc: 0.7352 - val_loss: 0.3773 - val_acc: 0.6851\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1272 - acc: 0.7349 - val_loss: 0.3731 - val_acc: 0.6902\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1251 - acc: 0.7353 - val_loss: 0.3754 - val_acc: 0.6914\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1227 - acc: 0.7355 - val_loss: 0.3905 - val_acc: 0.6888\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1222 - acc: 0.7356 - val_loss: 0.4075 - val_acc: 0.6881\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1237 - acc: 0.7354 - val_loss: 0.3776 - val_acc: 0.6899\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1203 - acc: 0.7357 - val_loss: 0.3886 - val_acc: 0.6903\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6384 - acc: 0.6723\n",
      "35843/35843 [==============================] - 2s 68us/sample - loss: 0.1191 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 469us/sample - loss: 0.2821 - acc: 0.7061 - val_loss: 0.4162 - val_acc: 0.6844\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1550 - acc: 0.7469 - val_loss: 0.4014 - val_acc: 0.6849\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1456 - acc: 0.7489 - val_loss: 0.3928 - val_acc: 0.6902\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1354 - acc: 0.7509 - val_loss: 0.3942 - val_acc: 0.6928\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1301 - acc: 0.7519 - val_loss: 0.3868 - val_acc: 0.6946\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1263 - acc: 0.7526 - val_loss: 0.3978 - val_acc: 0.6934\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1245 - acc: 0.7530 - val_loss: 0.3870 - val_acc: 0.6954\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1206 - acc: 0.7536 - val_loss: 0.3937 - val_acc: 0.6945\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1187 - acc: 0.7540 - val_loss: 0.3973 - val_acc: 0.6935\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1180 - acc: 0.7541 - val_loss: 0.4003 - val_acc: 0.6960\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1157 - acc: 0.7545 - val_loss: 0.4197 - val_acc: 0.6910\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1171 - acc: 0.7542 - val_loss: 0.3979 - val_acc: 0.6929\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1150 - acc: 0.7546 - val_loss: 0.4018 - val_acc: 0.6955\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1121 - acc: 0.7550 - val_loss: 0.4032 - val_acc: 0.6963\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1110 - acc: 0.7551 - val_loss: 0.4169 - val_acc: 0.6972\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1106 - acc: 0.7552 - val_loss: 0.4168 - val_acc: 0.6965\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1100 - acc: 0.7552 - val_loss: 0.4148 - val_acc: 0.6977\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1090 - acc: 0.7553 - val_loss: 0.4271 - val_acc: 0.6953\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1082 - acc: 0.7554 - val_loss: 0.4320 - val_acc: 0.6956\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1076 - acc: 0.7554 - val_loss: 0.4229 - val_acc: 0.6951\n",
      "17922/17922 [==============================] - 1s 69us/sample - loss: 0.9897 - acc: 0.6084\n",
      "35842/35842 [==============================] - 2s 67us/sample - loss: 0.1077 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 16s 459us/sample - loss: 0.2886 - acc: 0.7156 - val_loss: 0.4006 - val_acc: 0.6805\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1664 - acc: 0.7518 - val_loss: 0.3700 - val_acc: 0.6933\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1486 - acc: 0.7555 - val_loss: 0.3561 - val_acc: 0.6986\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1414 - acc: 0.7573 - val_loss: 0.3444 - val_acc: 0.7042\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1326 - acc: 0.7591 - val_loss: 0.3483 - val_acc: 0.7057\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1305 - acc: 0.7596 - val_loss: 0.3466 - val_acc: 0.7067\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1266 - acc: 0.7603 - val_loss: 0.3445 - val_acc: 0.7085\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1224 - acc: 0.7612 - val_loss: 0.3472 - val_acc: 0.7098\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1204 - acc: 0.7615 - val_loss: 0.3498 - val_acc: 0.7102\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1184 - acc: 0.7618 - val_loss: 0.3309 - val_acc: 0.7104\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1159 - acc: 0.7622 - val_loss: 0.3542 - val_acc: 0.7102\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1147 - acc: 0.7624 - val_loss: 0.3530 - val_acc: 0.7095\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1150 - acc: 0.7624 - val_loss: 0.3453 - val_acc: 0.7107\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1122 - acc: 0.7627 - val_loss: 0.3464 - val_acc: 0.7105\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1115 - acc: 0.7628 - val_loss: 0.3464 - val_acc: 0.7102\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1129 - acc: 0.7627 - val_loss: 0.3524 - val_acc: 0.7114\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1098 - acc: 0.7631 - val_loss: 0.3812 - val_acc: 0.7106\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1108 - acc: 0.7630 - val_loss: 0.3720 - val_acc: 0.7103\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1096 - acc: 0.7630 - val_loss: 0.3636 - val_acc: 0.7116\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1075 - acc: 0.7632 - val_loss: 0.3551 - val_acc: 0.7121\n",
      "17921/17921 [==============================] - 1s 70us/sample - loss: 0.6667 - acc: 0.6629\n",
      "35843/35843 [==============================] - 3s 71us/sample - loss: 0.1067 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 472us/sample - loss: 0.3082 - acc: 0.6824 - val_loss: 0.3798 - val_acc: 0.6801\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1819 - acc: 0.7235 - val_loss: 0.3752 - val_acc: 0.6807\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1659 - acc: 0.7273 - val_loss: 0.3558 - val_acc: 0.6859\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1575 - acc: 0.7291 - val_loss: 0.3646 - val_acc: 0.6898\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1512 - acc: 0.7306 - val_loss: 0.3644 - val_acc: 0.6869\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1449 - acc: 0.7319 - val_loss: 0.3585 - val_acc: 0.6911\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1413 - acc: 0.7326 - val_loss: 0.3685 - val_acc: 0.6874\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1382 - acc: 0.7332 - val_loss: 0.3660 - val_acc: 0.6892\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1358 - acc: 0.7336 - val_loss: 0.3636 - val_acc: 0.6898\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1338 - acc: 0.7340 - val_loss: 0.3680 - val_acc: 0.6904\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1312 - acc: 0.7345 - val_loss: 0.3749 - val_acc: 0.6915\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1295 - acc: 0.7347 - val_loss: 0.3784 - val_acc: 0.6890\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1283 - acc: 0.7349 - val_loss: 0.3789 - val_acc: 0.6888\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1273 - acc: 0.7351 - val_loss: 0.3782 - val_acc: 0.6896\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1255 - acc: 0.7352 - val_loss: 0.3842 - val_acc: 0.6904\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1263 - acc: 0.7352 - val_loss: 0.3915 - val_acc: 0.6894\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1241 - acc: 0.7354 - val_loss: 0.3784 - val_acc: 0.6900\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1243 - acc: 0.7354 - val_loss: 0.3889 - val_acc: 0.6908\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1226 - acc: 0.7356 - val_loss: 0.3950 - val_acc: 0.6879\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1220 - acc: 0.7356 - val_loss: 0.4440 - val_acc: 0.6804\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.7388 - acc: 0.6536\n",
      "35843/35843 [==============================] - 2s 68us/sample - loss: 0.1270 - acc: 0.7351\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 482us/sample - loss: 0.2412 - acc: 0.7204 - val_loss: 0.3935 - val_acc: 0.6876\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1508 - acc: 0.7474 - val_loss: 0.4010 - val_acc: 0.6894\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1371 - acc: 0.7503 - val_loss: 0.3919 - val_acc: 0.6880\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1299 - acc: 0.7518 - val_loss: 0.3810 - val_acc: 0.6934\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1238 - acc: 0.7528 - val_loss: 0.4029 - val_acc: 0.6937\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1206 - acc: 0.7534 - val_loss: 0.3972 - val_acc: 0.6942\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1169 - acc: 0.7541 - val_loss: 0.3960 - val_acc: 0.6940\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1148 - acc: 0.7545 - val_loss: 0.4002 - val_acc: 0.6967\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1139 - acc: 0.7546 - val_loss: 0.4033 - val_acc: 0.6963\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1155 - acc: 0.7545 - val_loss: 0.3937 - val_acc: 0.6952\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1104 - acc: 0.7551 - val_loss: 0.4124 - val_acc: 0.6970\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1090 - acc: 0.7553 - val_loss: 0.4179 - val_acc: 0.6955\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1082 - acc: 0.7553 - val_loss: 0.4163 - val_acc: 0.6965\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1072 - acc: 0.7554 - val_loss: 0.4198 - val_acc: 0.6954\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1062 - acc: 0.7555 - val_loss: 0.4178 - val_acc: 0.6956\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1061 - acc: 0.7555 - val_loss: 0.4412 - val_acc: 0.6965\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1071 - acc: 0.7554 - val_loss: 0.4367 - val_acc: 0.6944\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1054 - acc: 0.7555 - val_loss: 0.4387 - val_acc: 0.6972\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1063 - acc: 0.7554 - val_loss: 0.4216 - val_acc: 0.6981\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1057 - acc: 0.7555 - val_loss: 0.4185 - val_acc: 0.6957\n",
      "17922/17922 [==============================] - 1s 71us/sample - loss: 0.9992 - acc: 0.6052\n",
      "35842/35842 [==============================] - 2s 67us/sample - loss: 0.1041 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 474us/sample - loss: 0.2511 - acc: 0.7282 - val_loss: 0.3843 - val_acc: 0.6889\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1553 - acc: 0.7540 - val_loss: 0.3567 - val_acc: 0.6972\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1407 - acc: 0.7572 - val_loss: 0.3460 - val_acc: 0.7057\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1318 - acc: 0.7591 - val_loss: 0.3528 - val_acc: 0.7075\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1258 - acc: 0.7603 - val_loss: 0.3356 - val_acc: 0.7095\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1214 - acc: 0.7611 - val_loss: 0.3469 - val_acc: 0.7103\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1189 - acc: 0.7616 - val_loss: 0.3437 - val_acc: 0.7093\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1158 - acc: 0.7621 - val_loss: 0.3544 - val_acc: 0.7107\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1133 - acc: 0.7625 - val_loss: 0.3438 - val_acc: 0.7108\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1116 - acc: 0.7628 - val_loss: 0.3445 - val_acc: 0.7112\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1099 - acc: 0.7629 - val_loss: 0.3525 - val_acc: 0.7104\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1100 - acc: 0.7629 - val_loss: 0.3506 - val_acc: 0.7115\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1075 - acc: 0.7632 - val_loss: 0.3511 - val_acc: 0.7121\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1065 - acc: 0.7633 - val_loss: 0.3574 - val_acc: 0.7122\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1058 - acc: 0.7633 - val_loss: 0.3680 - val_acc: 0.7122\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1053 - acc: 0.7634 - val_loss: 0.3604 - val_acc: 0.7119\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1047 - acc: 0.7634 - val_loss: 0.3679 - val_acc: 0.7114\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1046 - acc: 0.7634 - val_loss: 0.3758 - val_acc: 0.7114\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1036 - acc: 0.7634 - val_loss: 0.3548 - val_acc: 0.7094\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1054 - acc: 0.7633 - val_loss: 0.3745 - val_acc: 0.7113\n",
      "17921/17921 [==============================] - 1s 72us/sample - loss: 0.7368 - acc: 0.6587\n",
      "35843/35843 [==============================] - 3s 71us/sample - loss: 0.1037 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 475us/sample - loss: 0.2686 - acc: 0.6965 - val_loss: 0.4092 - val_acc: 0.6683\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1773 - acc: 0.7243 - val_loss: 0.3659 - val_acc: 0.6831\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1592 - acc: 0.7285 - val_loss: 0.3730 - val_acc: 0.6859\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1502 - acc: 0.7306 - val_loss: 0.3819 - val_acc: 0.6865\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1452 - acc: 0.7316 - val_loss: 0.3576 - val_acc: 0.6886\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1393 - acc: 0.7329 - val_loss: 0.3763 - val_acc: 0.6804\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1367 - acc: 0.7332 - val_loss: 0.3614 - val_acc: 0.6860\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1325 - acc: 0.7340 - val_loss: 0.3739 - val_acc: 0.6913\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1292 - acc: 0.7345 - val_loss: 0.3605 - val_acc: 0.6910\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1267 - acc: 0.7349 - val_loss: 0.3753 - val_acc: 0.6870\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1263 - acc: 0.7351 - val_loss: 0.3842 - val_acc: 0.6885\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1251 - acc: 0.7352 - val_loss: 0.3660 - val_acc: 0.6899\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1246 - acc: 0.7353 - val_loss: 0.4064 - val_acc: 0.6827\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1249 - acc: 0.7352 - val_loss: 0.3870 - val_acc: 0.6872\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1209 - acc: 0.7356 - val_loss: 0.3668 - val_acc: 0.6899\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1195 - acc: 0.7358 - val_loss: 0.3834 - val_acc: 0.6905\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1193 - acc: 0.7358 - val_loss: 0.3805 - val_acc: 0.6869\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1200 - acc: 0.7357 - val_loss: 0.3779 - val_acc: 0.6885\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1179 - acc: 0.7359 - val_loss: 0.3761 - val_acc: 0.6918\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1168 - acc: 0.7359 - val_loss: 0.3959 - val_acc: 0.6883\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6594 - acc: 0.6675\n",
      "35843/35843 [==============================] - 2s 68us/sample - loss: 0.1170 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 469us/sample - loss: 0.2578 - acc: 0.7152 - val_loss: 0.4102 - val_acc: 0.6836\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1535 - acc: 0.7471 - val_loss: 0.3979 - val_acc: 0.6889\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1397 - acc: 0.7499 - val_loss: 0.3924 - val_acc: 0.6928\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1317 - acc: 0.7515 - val_loss: 0.4007 - val_acc: 0.6943\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1268 - acc: 0.7524 - val_loss: 0.3900 - val_acc: 0.6953\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1221 - acc: 0.7532 - val_loss: 0.3966 - val_acc: 0.6936\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1194 - acc: 0.7537 - val_loss: 0.3864 - val_acc: 0.6958\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1168 - acc: 0.7542 - val_loss: 0.3981 - val_acc: 0.6956\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1205 - acc: 0.7536 - val_loss: 0.4079 - val_acc: 0.6979\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1158 - acc: 0.7544 - val_loss: 0.3929 - val_acc: 0.6945\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1122 - acc: 0.7549 - val_loss: 0.4033 - val_acc: 0.6955\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1118 - acc: 0.7549 - val_loss: 0.3966 - val_acc: 0.6964\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1096 - acc: 0.7552 - val_loss: 0.3988 - val_acc: 0.6954\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1087 - acc: 0.7553 - val_loss: 0.3955 - val_acc: 0.6965\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1077 - acc: 0.7553 - val_loss: 0.4036 - val_acc: 0.6954\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1074 - acc: 0.7554 - val_loss: 0.4147 - val_acc: 0.6968\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1072 - acc: 0.7554 - val_loss: 0.4200 - val_acc: 0.6955\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1060 - acc: 0.7555 - val_loss: 0.4066 - val_acc: 0.6954\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1054 - acc: 0.7555 - val_loss: 0.4088 - val_acc: 0.6962\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1049 - acc: 0.7555 - val_loss: 0.4097 - val_acc: 0.6957\n",
      "17922/17922 [==============================] - 1s 68us/sample - loss: 0.9498 - acc: 0.6076\n",
      "35842/35842 [==============================] - 3s 70us/sample - loss: 0.1052 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 477us/sample - loss: 0.2619 - acc: 0.7246 - val_loss: 0.4020 - val_acc: 0.6804\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1606 - acc: 0.7526 - val_loss: 0.3609 - val_acc: 0.6963\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1468 - acc: 0.7560 - val_loss: 0.3591 - val_acc: 0.7030\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1379 - acc: 0.7579 - val_loss: 0.3396 - val_acc: 0.7056\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1300 - acc: 0.7595 - val_loss: 0.3434 - val_acc: 0.7080\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1260 - acc: 0.7603 - val_loss: 0.3422 - val_acc: 0.7093\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1218 - acc: 0.7611 - val_loss: 0.3444 - val_acc: 0.7085\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1193 - acc: 0.7616 - val_loss: 0.3512 - val_acc: 0.7101\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1165 - acc: 0.7620 - val_loss: 0.3551 - val_acc: 0.7091\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1151 - acc: 0.7623 - val_loss: 0.3444 - val_acc: 0.7109\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1129 - acc: 0.7626 - val_loss: 0.3678 - val_acc: 0.7067\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1169 - acc: 0.7621 - val_loss: 0.3374 - val_acc: 0.7099\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1108 - acc: 0.7628 - val_loss: 0.3499 - val_acc: 0.7097\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1096 - acc: 0.7630 - val_loss: 0.3555 - val_acc: 0.7111\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1083 - acc: 0.7631 - val_loss: 0.3594 - val_acc: 0.7120\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1075 - acc: 0.7632 - val_loss: 0.3541 - val_acc: 0.7120\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1070 - acc: 0.7633 - val_loss: 0.3625 - val_acc: 0.7119\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1063 - acc: 0.7633 - val_loss: 0.3467 - val_acc: 0.7108\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1066 - acc: 0.7633 - val_loss: 0.3658 - val_acc: 0.7113\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1048 - acc: 0.7634 - val_loss: 0.3730 - val_acc: 0.7121\n",
      "17921/17921 [==============================] - 1s 68us/sample - loss: 0.7250 - acc: 0.6612\n",
      "35843/35843 [==============================] - 2s 69us/sample - loss: 0.1041 - acc: 0.7635\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 477us/sample - loss: 0.2893 - acc: 0.6906 - val_loss: 0.3833 - val_acc: 0.6748\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1811 - acc: 0.7234 - val_loss: 0.3707 - val_acc: 0.6853\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1642 - acc: 0.7275 - val_loss: 0.3649 - val_acc: 0.6856\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1530 - acc: 0.7300 - val_loss: 0.3643 - val_acc: 0.6857\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1469 - acc: 0.7313 - val_loss: 0.3594 - val_acc: 0.6904\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1423 - acc: 0.7322 - val_loss: 0.3769 - val_acc: 0.6828\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1405 - acc: 0.7326 - val_loss: 0.3758 - val_acc: 0.6906\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1357 - acc: 0.7335 - val_loss: 0.4011 - val_acc: 0.6802\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1346 - acc: 0.7337 - val_loss: 0.3620 - val_acc: 0.6894\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1303 - acc: 0.7345 - val_loss: 0.3852 - val_acc: 0.6834\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1291 - acc: 0.7346 - val_loss: 0.3796 - val_acc: 0.6852\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1271 - acc: 0.7350 - val_loss: 0.3802 - val_acc: 0.6880\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1249 - acc: 0.7352 - val_loss: 0.3825 - val_acc: 0.6874\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1244 - acc: 0.7353 - val_loss: 0.3797 - val_acc: 0.6898\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1225 - acc: 0.7355 - val_loss: 0.3738 - val_acc: 0.6898\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1210 - acc: 0.7357 - val_loss: 0.3910 - val_acc: 0.6899\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1208 - acc: 0.7357 - val_loss: 0.3822 - val_acc: 0.6879\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1196 - acc: 0.7358 - val_loss: 0.3817 - val_acc: 0.6897\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1187 - acc: 0.7358 - val_loss: 0.3776 - val_acc: 0.6906\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1198 - acc: 0.7358 - val_loss: 0.3858 - val_acc: 0.6909\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6331 - acc: 0.6724\n",
      "35843/35843 [==============================] - 2s 70us/sample - loss: 0.1182 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 473us/sample - loss: 0.2853 - acc: 0.7051 - val_loss: 0.4214 - val_acc: 0.6819\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1572 - acc: 0.7463 - val_loss: 0.3975 - val_acc: 0.6896\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1422 - acc: 0.7495 - val_loss: 0.3906 - val_acc: 0.6935\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1339 - acc: 0.7512 - val_loss: 0.3895 - val_acc: 0.6909\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1289 - acc: 0.7521 - val_loss: 0.3901 - val_acc: 0.6942\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1245 - acc: 0.7529 - val_loss: 0.4070 - val_acc: 0.6952\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1233 - acc: 0.7532 - val_loss: 0.3841 - val_acc: 0.6950\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1187 - acc: 0.7539 - val_loss: 0.3977 - val_acc: 0.6962\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1176 - acc: 0.7541 - val_loss: 0.3923 - val_acc: 0.6950\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1152 - acc: 0.7545 - val_loss: 0.4008 - val_acc: 0.6965\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1138 - acc: 0.7547 - val_loss: 0.3994 - val_acc: 0.6955\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1122 - acc: 0.7549 - val_loss: 0.4296 - val_acc: 0.6941\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1174 - acc: 0.7542 - val_loss: 0.4107 - val_acc: 0.6945\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1112 - acc: 0.7550 - val_loss: 0.4097 - val_acc: 0.6965\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1098 - acc: 0.7552 - val_loss: 0.4241 - val_acc: 0.6959\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1090 - acc: 0.7553 - val_loss: 0.4111 - val_acc: 0.6949\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1083 - acc: 0.7553 - val_loss: 0.4040 - val_acc: 0.6958\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1074 - acc: 0.7554 - val_loss: 0.4181 - val_acc: 0.6950\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1068 - acc: 0.7555 - val_loss: 0.4134 - val_acc: 0.6959\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 121us/sample - loss: 0.1064 - acc: 0.7555 - val_loss: 0.4222 - val_acc: 0.6967\n",
      "17922/17922 [==============================] - 1s 69us/sample - loss: 0.9768 - acc: 0.6104\n",
      "35842/35842 [==============================] - 2s 69us/sample - loss: 0.1073 - acc: 0.7556\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 473us/sample - loss: 0.2910 - acc: 0.7145 - val_loss: 0.3894 - val_acc: 0.6867\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1652 - acc: 0.7520 - val_loss: 0.3814 - val_acc: 0.6920\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1472 - acc: 0.7556 - val_loss: 0.3609 - val_acc: 0.7014\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1382 - acc: 0.7578 - val_loss: 0.3485 - val_acc: 0.7032\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1334 - acc: 0.7588 - val_loss: 0.3453 - val_acc: 0.7062\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1272 - acc: 0.7601 - val_loss: 0.3347 - val_acc: 0.7090\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1235 - acc: 0.7608 - val_loss: 0.3383 - val_acc: 0.7096\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1209 - acc: 0.7613 - val_loss: 0.3372 - val_acc: 0.7102\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1182 - acc: 0.7618 - val_loss: 0.3457 - val_acc: 0.7106\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1161 - acc: 0.7622 - val_loss: 0.3487 - val_acc: 0.7100\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1166 - acc: 0.7621 - val_loss: 0.3458 - val_acc: 0.7102\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1137 - acc: 0.7626 - val_loss: 0.3481 - val_acc: 0.7108\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1116 - acc: 0.7628 - val_loss: 0.3374 - val_acc: 0.7114\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1102 - acc: 0.7630 - val_loss: 0.3572 - val_acc: 0.7107\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1093 - acc: 0.7631 - val_loss: 0.3465 - val_acc: 0.7108\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1082 - acc: 0.7632 - val_loss: 0.3492 - val_acc: 0.7115\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1076 - acc: 0.7632 - val_loss: 0.3528 - val_acc: 0.7106\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1076 - acc: 0.7633 - val_loss: 0.3463 - val_acc: 0.7113\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1062 - acc: 0.7633 - val_loss: 0.3604 - val_acc: 0.7110\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1065 - acc: 0.7633 - val_loss: 0.3811 - val_acc: 0.7084\n",
      "17921/17921 [==============================] - 1s 70us/sample - loss: 0.7324 - acc: 0.6563\n",
      "35843/35843 [==============================] - 2s 69us/sample - loss: 0.1090 - acc: 0.7632\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 472us/sample - loss: 0.3078 - acc: 0.6841 - val_loss: 0.3781 - val_acc: 0.6809\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1824 - acc: 0.7233 - val_loss: 0.3749 - val_acc: 0.6829\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1664 - acc: 0.7271 - val_loss: 0.3645 - val_acc: 0.6866\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1561 - acc: 0.7295 - val_loss: 0.3694 - val_acc: 0.6804\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1510 - acc: 0.7307 - val_loss: 0.3830 - val_acc: 0.6796\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1473 - acc: 0.7314 - val_loss: 0.3587 - val_acc: 0.6873\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1410 - acc: 0.7326 - val_loss: 0.3718 - val_acc: 0.6848\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1393 - acc: 0.7330 - val_loss: 0.3579 - val_acc: 0.6893\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1346 - acc: 0.7338 - val_loss: 0.3779 - val_acc: 0.6872\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1325 - acc: 0.7342 - val_loss: 0.3643 - val_acc: 0.6913\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1303 - acc: 0.7346 - val_loss: 0.3575 - val_acc: 0.6912\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1284 - acc: 0.7348 - val_loss: 0.3717 - val_acc: 0.6882\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1267 - acc: 0.7351 - val_loss: 0.3685 - val_acc: 0.6902\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1256 - acc: 0.7352 - val_loss: 0.3723 - val_acc: 0.6928\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1263 - acc: 0.7351 - val_loss: 0.3695 - val_acc: 0.6923\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1240 - acc: 0.7354 - val_loss: 0.3714 - val_acc: 0.6890\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1223 - acc: 0.7356 - val_loss: 0.3844 - val_acc: 0.6862\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1226 - acc: 0.7356 - val_loss: 0.3791 - val_acc: 0.6902\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1207 - acc: 0.7357 - val_loss: 0.3853 - val_acc: 0.6876\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1199 - acc: 0.7358 - val_loss: 0.3826 - val_acc: 0.6888\n",
      "17921/17921 [==============================] - 1s 66us/sample - loss: 0.6278 - acc: 0.6682\n",
      "35843/35843 [==============================] - 3s 72us/sample - loss: 0.1200 - acc: 0.7359\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 481us/sample - loss: 0.2439 - acc: 0.7196 - val_loss: 0.4138 - val_acc: 0.6844\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1532 - acc: 0.7470 - val_loss: 0.3808 - val_acc: 0.6846\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1462 - acc: 0.7486 - val_loss: 0.4043 - val_acc: 0.6920\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1343 - acc: 0.7509 - val_loss: 0.3967 - val_acc: 0.6930\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1284 - acc: 0.7519 - val_loss: 0.3937 - val_acc: 0.6942\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1250 - acc: 0.7526 - val_loss: 0.3990 - val_acc: 0.6918\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1211 - acc: 0.7534 - val_loss: 0.3959 - val_acc: 0.6938\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1189 - acc: 0.7538 - val_loss: 0.4024 - val_acc: 0.6941\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1163 - acc: 0.7542 - val_loss: 0.4054 - val_acc: 0.6945\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1156 - acc: 0.7543 - val_loss: 0.4099 - val_acc: 0.6948\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1131 - acc: 0.7547 - val_loss: 0.4118 - val_acc: 0.6933\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1174 - acc: 0.7540 - val_loss: 0.4197 - val_acc: 0.6946\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1134 - acc: 0.7547 - val_loss: 0.4208 - val_acc: 0.6956\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1112 - acc: 0.7549 - val_loss: 0.4198 - val_acc: 0.6954\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1097 - acc: 0.7551 - val_loss: 0.4129 - val_acc: 0.6955\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 122us/sample - loss: 0.1088 - acc: 0.7552 - val_loss: 0.4193 - val_acc: 0.6942\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1124 - acc: 0.7548 - val_loss: 0.4185 - val_acc: 0.6971\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1089 - acc: 0.7552 - val_loss: 0.4139 - val_acc: 0.6954\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1075 - acc: 0.7553 - val_loss: 0.4533 - val_acc: 0.6929\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1142 - acc: 0.7545 - val_loss: 0.4123 - val_acc: 0.6945\n",
      "17922/17922 [==============================] - 1s 67us/sample - loss: 0.9097 - acc: 0.6124\n",
      "35842/35842 [==============================] - 3s 70us/sample - loss: 0.1085 - acc: 0.7552\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 483us/sample - loss: 0.2534 - acc: 0.7280 - val_loss: 0.4089 - val_acc: 0.6875\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1649 - acc: 0.7517 - val_loss: 0.3646 - val_acc: 0.6951\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1459 - acc: 0.7559 - val_loss: 0.3528 - val_acc: 0.7025\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1361 - acc: 0.7581 - val_loss: 0.3527 - val_acc: 0.7051\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1296 - acc: 0.7594 - val_loss: 0.3476 - val_acc: 0.7068\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1251 - acc: 0.7603 - val_loss: 0.3524 - val_acc: 0.7071\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1219 - acc: 0.7609 - val_loss: 0.3526 - val_acc: 0.7088\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1187 - acc: 0.7615 - val_loss: 0.3554 - val_acc: 0.7084\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1168 - acc: 0.7619 - val_loss: 0.3565 - val_acc: 0.7093\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1176 - acc: 0.7618 - val_loss: 0.3610 - val_acc: 0.7101\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1140 - acc: 0.7624 - val_loss: 0.3455 - val_acc: 0.7100\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1121 - acc: 0.7626 - val_loss: 0.3537 - val_acc: 0.7082\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1129 - acc: 0.7626 - val_loss: 0.3713 - val_acc: 0.7066\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1129 - acc: 0.7625 - val_loss: 0.3749 - val_acc: 0.7111\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1093 - acc: 0.7630 - val_loss: 0.3812 - val_acc: 0.7103\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1086 - acc: 0.7630 - val_loss: 0.3628 - val_acc: 0.7104\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1072 - acc: 0.7632 - val_loss: 0.3884 - val_acc: 0.7097\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1073 - acc: 0.7632 - val_loss: 0.3722 - val_acc: 0.7107\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1058 - acc: 0.7633 - val_loss: 0.3710 - val_acc: 0.7103\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1053 - acc: 0.7633 - val_loss: 0.4022 - val_acc: 0.7093\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.7622 - acc: 0.6589\n",
      "35843/35843 [==============================] - 2s 69us/sample - loss: 0.1075 - acc: 0.7632\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 502us/sample - loss: 0.2778 - acc: 0.6951 - val_loss: 0.4063 - val_acc: 0.6697\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1810 - acc: 0.7236 - val_loss: 0.3882 - val_acc: 0.6761\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1627 - acc: 0.7278 - val_loss: 0.3638 - val_acc: 0.6851\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1523 - acc: 0.7300 - val_loss: 0.3486 - val_acc: 0.6881\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1460 - acc: 0.7314 - val_loss: 0.3837 - val_acc: 0.6856\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1412 - acc: 0.7323 - val_loss: 0.3520 - val_acc: 0.6887\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1372 - acc: 0.7331 - val_loss: 0.3707 - val_acc: 0.6873\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1356 - acc: 0.7335 - val_loss: 0.3842 - val_acc: 0.6888\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1345 - acc: 0.7337 - val_loss: 0.3822 - val_acc: 0.6821\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1310 - acc: 0.7342 - val_loss: 0.3842 - val_acc: 0.6875\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1278 - acc: 0.7347 - val_loss: 0.3886 - val_acc: 0.6838\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1289 - acc: 0.7346 - val_loss: 0.3701 - val_acc: 0.6892\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1251 - acc: 0.7351 - val_loss: 0.3876 - val_acc: 0.6841\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1241 - acc: 0.7353 - val_loss: 0.3818 - val_acc: 0.6909\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1239 - acc: 0.7353 - val_loss: 0.3800 - val_acc: 0.6892\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1232 - acc: 0.7354 - val_loss: 0.3858 - val_acc: 0.6881\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1212 - acc: 0.7356 - val_loss: 0.3821 - val_acc: 0.6871\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1206 - acc: 0.7356 - val_loss: 0.3925 - val_acc: 0.6869\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1195 - acc: 0.7357 - val_loss: 0.3965 - val_acc: 0.6880\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1191 - acc: 0.7358 - val_loss: 0.3931 - val_acc: 0.6874\n",
      "17921/17921 [==============================] - 1s 69us/sample - loss: 0.6293 - acc: 0.6684\n",
      "35843/35843 [==============================] - 3s 70us/sample - loss: 0.1208 - acc: 0.7357\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 17s 475us/sample - loss: 0.2609 - acc: 0.7148 - val_loss: 0.4169 - val_acc: 0.6833\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1569 - acc: 0.7463 - val_loss: 0.4105 - val_acc: 0.6880\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1434 - acc: 0.7491 - val_loss: 0.3961 - val_acc: 0.6915\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1360 - acc: 0.7507 - val_loss: 0.3840 - val_acc: 0.6944\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1297 - acc: 0.7519 - val_loss: 0.3948 - val_acc: 0.6940\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1258 - acc: 0.7527 - val_loss: 0.4255 - val_acc: 0.6908\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1248 - acc: 0.7528 - val_loss: 0.3926 - val_acc: 0.6940\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1199 - acc: 0.7537 - val_loss: 0.3997 - val_acc: 0.6938\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1178 - acc: 0.7541 - val_loss: 0.3979 - val_acc: 0.6945\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 111us/sample - loss: 0.1159 - acc: 0.7544 - val_loss: 0.4232 - val_acc: 0.6937\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1182 - acc: 0.7540 - val_loss: 0.3976 - val_acc: 0.6949\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1134 - acc: 0.7547 - val_loss: 0.4114 - val_acc: 0.6940\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1123 - acc: 0.7549 - val_loss: 0.4036 - val_acc: 0.6962\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1114 - acc: 0.7550 - val_loss: 0.4046 - val_acc: 0.6931\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1140 - acc: 0.7547 - val_loss: 0.4120 - val_acc: 0.6948\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1116 - acc: 0.7550 - val_loss: 0.4171 - val_acc: 0.6937\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 112us/sample - loss: 0.1092 - acc: 0.7552 - val_loss: 0.4202 - val_acc: 0.6949\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 114us/sample - loss: 0.1088 - acc: 0.7553 - val_loss: 0.4266 - val_acc: 0.6969\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1076 - acc: 0.7554 - val_loss: 0.4231 - val_acc: 0.6912\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1094 - acc: 0.7553 - val_loss: 0.4177 - val_acc: 0.6951\n",
      "17922/17922 [==============================] - 1s 68us/sample - loss: 0.9618 - acc: 0.6079\n",
      "35842/35842 [==============================] - 2s 70us/sample - loss: 0.1067 - acc: 0.7555\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 491us/sample - loss: 0.2703 - acc: 0.7219 - val_loss: 0.3894 - val_acc: 0.6819\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1651 - acc: 0.7518 - val_loss: 0.3602 - val_acc: 0.6958\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1469 - acc: 0.7557 - val_loss: 0.3613 - val_acc: 0.6991\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1391 - acc: 0.7576 - val_loss: 0.3592 - val_acc: 0.7048\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1319 - acc: 0.7591 - val_loss: 0.3487 - val_acc: 0.7051\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1278 - acc: 0.7599 - val_loss: 0.3502 - val_acc: 0.7065\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1241 - acc: 0.7606 - val_loss: 0.3434 - val_acc: 0.7066\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1218 - acc: 0.7610 - val_loss: 0.3442 - val_acc: 0.7088\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1188 - acc: 0.7616 - val_loss: 0.3617 - val_acc: 0.7078\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1178 - acc: 0.7619 - val_loss: 0.3498 - val_acc: 0.7101\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1152 - acc: 0.7622 - val_loss: 0.3421 - val_acc: 0.7099\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1138 - acc: 0.7625 - val_loss: 0.3667 - val_acc: 0.7104\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1154 - acc: 0.7622 - val_loss: 0.3647 - val_acc: 0.7093\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1119 - acc: 0.7627 - val_loss: 0.3617 - val_acc: 0.7088\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1111 - acc: 0.7628 - val_loss: 0.3519 - val_acc: 0.7100\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1100 - acc: 0.7630 - val_loss: 0.3537 - val_acc: 0.7100\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1087 - acc: 0.7631 - val_loss: 0.3671 - val_acc: 0.7105\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1082 - acc: 0.7631 - val_loss: 0.3595 - val_acc: 0.7093\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1093 - acc: 0.7630 - val_loss: 0.3635 - val_acc: 0.7098\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1067 - acc: 0.7633 - val_loss: 0.3727 - val_acc: 0.7099\n",
      "17921/17921 [==============================] - 1s 68us/sample - loss: 0.7533 - acc: 0.6565\n",
      "35843/35843 [==============================] - 2s 69us/sample - loss: 0.1057 - acc: 0.7634\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 489us/sample - loss: 0.2903 - acc: 0.6911 - val_loss: 0.3913 - val_acc: 0.6791\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1823 - acc: 0.7232 - val_loss: 0.4032 - val_acc: 0.6715\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1669 - acc: 0.7269 - val_loss: 0.3795 - val_acc: 0.6845\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1576 - acc: 0.7291 - val_loss: 0.3580 - val_acc: 0.6890\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1503 - acc: 0.7307 - val_loss: 0.3719 - val_acc: 0.6891\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1452 - acc: 0.7317 - val_loss: 0.3720 - val_acc: 0.6867\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1410 - acc: 0.7325 - val_loss: 0.3802 - val_acc: 0.6819\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1404 - acc: 0.7326 - val_loss: 0.3604 - val_acc: 0.6921\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1367 - acc: 0.7334 - val_loss: 0.3770 - val_acc: 0.6857\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1343 - acc: 0.7338 - val_loss: 0.3753 - val_acc: 0.6846\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1359 - acc: 0.7337 - val_loss: 0.3738 - val_acc: 0.6879\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1303 - acc: 0.7345 - val_loss: 0.3710 - val_acc: 0.6879\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1281 - acc: 0.7349 - val_loss: 0.3818 - val_acc: 0.6857\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1265 - acc: 0.7351 - val_loss: 0.3918 - val_acc: 0.6880\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1288 - acc: 0.7349 - val_loss: 0.3783 - val_acc: 0.6876\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1251 - acc: 0.7353 - val_loss: 0.4007 - val_acc: 0.6812\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1260 - acc: 0.7352 - val_loss: 0.3748 - val_acc: 0.6870\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1229 - acc: 0.7355 - val_loss: 0.3807 - val_acc: 0.6901\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1223 - acc: 0.7356 - val_loss: 0.4117 - val_acc: 0.6833\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1243 - acc: 0.7353 - val_loss: 0.3809 - val_acc: 0.6887\n",
      "17921/17921 [==============================] - 1s 71us/sample - loss: 0.6254 - acc: 0.6649\n",
      "35843/35843 [==============================] - 3s 70us/sample - loss: 0.1210 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 18s 497us/sample - loss: 0.2843 - acc: 0.7048 - val_loss: 0.4188 - val_acc: 0.6815\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 113us/sample - loss: 0.1617 - acc: 0.7454 - val_loss: 0.3986 - val_acc: 0.6876\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1461 - acc: 0.7487 - val_loss: 0.3978 - val_acc: 0.6920\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1374 - acc: 0.7504 - val_loss: 0.4034 - val_acc: 0.6857\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1336 - acc: 0.7512 - val_loss: 0.3816 - val_acc: 0.6908\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1300 - acc: 0.7520 - val_loss: 0.3923 - val_acc: 0.6925\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1247 - acc: 0.7528 - val_loss: 0.3800 - val_acc: 0.6938\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 117us/sample - loss: 0.1224 - acc: 0.7533 - val_loss: 0.4012 - val_acc: 0.6897\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1264 - acc: 0.7526 - val_loss: 0.3945 - val_acc: 0.6914\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 119us/sample - loss: 0.1195 - acc: 0.7538 - val_loss: 0.3924 - val_acc: 0.6915\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1194 - acc: 0.7538 - val_loss: 0.3947 - val_acc: 0.6934\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 120us/sample - loss: 0.1168 - acc: 0.7542 - val_loss: 0.4001 - val_acc: 0.6945\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1147 - acc: 0.7546 - val_loss: 0.4059 - val_acc: 0.6953\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1139 - acc: 0.7547 - val_loss: 0.4006 - val_acc: 0.6941\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1129 - acc: 0.7548 - val_loss: 0.4008 - val_acc: 0.6953\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1119 - acc: 0.7549 - val_loss: 0.4014 - val_acc: 0.6958\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1108 - acc: 0.7551 - val_loss: 0.4092 - val_acc: 0.6941\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 116us/sample - loss: 0.1101 - acc: 0.7552 - val_loss: 0.4090 - val_acc: 0.6946\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 118us/sample - loss: 0.1098 - acc: 0.7552 - val_loss: 0.3999 - val_acc: 0.6957\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 115us/sample - loss: 0.1090 - acc: 0.7553 - val_loss: 0.4206 - val_acc: 0.6932\n",
      "17922/17922 [==============================] - 1s 69us/sample - loss: 0.9895 - acc: 0.6036\n",
      "35842/35842 [==============================] - 2s 69us/sample - loss: 0.1099 - acc: 0.7553\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 17s 487us/sample - loss: 0.2915 - acc: 0.7153 - val_loss: 0.4039 - val_acc: 0.6827\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1658 - acc: 0.7518 - val_loss: 0.3741 - val_acc: 0.6932\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1504 - acc: 0.7550 - val_loss: 0.3540 - val_acc: 0.7002\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1404 - acc: 0.7573 - val_loss: 0.3446 - val_acc: 0.7049\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1338 - acc: 0.7587 - val_loss: 0.3409 - val_acc: 0.7054\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1294 - acc: 0.7597 - val_loss: 0.3407 - val_acc: 0.7080\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1292 - acc: 0.7598 - val_loss: 0.3369 - val_acc: 0.7076\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1234 - acc: 0.7609 - val_loss: 0.3421 - val_acc: 0.7074\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 114us/sample - loss: 0.1207 - acc: 0.7614 - val_loss: 0.3529 - val_acc: 0.7092\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1205 - acc: 0.7615 - val_loss: 0.3512 - val_acc: 0.7092\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1189 - acc: 0.7618 - val_loss: 0.3519 - val_acc: 0.7098\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1162 - acc: 0.7622 - val_loss: 0.3538 - val_acc: 0.7092\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 116us/sample - loss: 0.1147 - acc: 0.7624 - val_loss: 0.3526 - val_acc: 0.7090\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1147 - acc: 0.7624 - val_loss: 0.3550 - val_acc: 0.7102\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1121 - acc: 0.7628 - val_loss: 0.3438 - val_acc: 0.7103\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 113us/sample - loss: 0.1118 - acc: 0.7628 - val_loss: 0.3467 - val_acc: 0.7101\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1104 - acc: 0.7629 - val_loss: 0.3484 - val_acc: 0.7104\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 117us/sample - loss: 0.1096 - acc: 0.7631 - val_loss: 0.3492 - val_acc: 0.7097\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1095 - acc: 0.7631 - val_loss: 0.3502 - val_acc: 0.7104\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 115us/sample - loss: 0.1082 - acc: 0.7632 - val_loss: 0.3557 - val_acc: 0.7091\n",
      "17921/17921 [==============================] - 1s 70us/sample - loss: 0.6835 - acc: 0.6553\n",
      "35843/35843 [==============================] - 2s 68us/sample - loss: 0.1099 - acc: 0.7632\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 498us/sample - loss: 0.3171 - acc: 0.6816 - val_loss: 0.3977 - val_acc: 0.6774\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1882 - acc: 0.7220 - val_loss: 0.3802 - val_acc: 0.6825\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1701 - acc: 0.7264 - val_loss: 0.3781 - val_acc: 0.6819\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1606 - acc: 0.7286 - val_loss: 0.3644 - val_acc: 0.6888\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1548 - acc: 0.7300 - val_loss: 0.3679 - val_acc: 0.6886\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1481 - acc: 0.7312 - val_loss: 0.3671 - val_acc: 0.6819\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1481 - acc: 0.7313 - val_loss: 0.3808 - val_acc: 0.6830\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1470 - acc: 0.7316 - val_loss: 0.3677 - val_acc: 0.6879\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1393 - acc: 0.7330 - val_loss: 0.3552 - val_acc: 0.6910\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1363 - acc: 0.7335 - val_loss: 0.3568 - val_acc: 0.6900\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1350 - acc: 0.7338 - val_loss: 0.3694 - val_acc: 0.6895\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1325 - acc: 0.7342 - val_loss: 0.3674 - val_acc: 0.6895\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1307 - acc: 0.7345 - val_loss: 0.3718 - val_acc: 0.6891\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1294 - acc: 0.7347 - val_loss: 0.3729 - val_acc: 0.6883\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1280 - acc: 0.7350 - val_loss: 0.3684 - val_acc: 0.6911\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1268 - acc: 0.7351 - val_loss: 0.3726 - val_acc: 0.6894\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 118us/sample - loss: 0.1256 - acc: 0.7353 - val_loss: 0.3769 - val_acc: 0.6895\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 121us/sample - loss: 0.1248 - acc: 0.7354 - val_loss: 0.3727 - val_acc: 0.6885\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 120us/sample - loss: 0.1238 - acc: 0.7355 - val_loss: 0.3785 - val_acc: 0.6901\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 119us/sample - loss: 0.1232 - acc: 0.7356 - val_loss: 0.3901 - val_acc: 0.6907\n",
      "17921/17921 [==============================] - 1s 72us/sample - loss: 0.6354 - acc: 0.6718\n",
      "35843/35843 [==============================] - 3s 72us/sample - loss: 0.1227 - acc: 0.7358\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 18s 498us/sample - loss: 0.2526 - acc: 0.7173 - val_loss: 0.4444 - val_acc: 0.6732\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1688 - acc: 0.7423 - val_loss: 0.4206 - val_acc: 0.6813\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1560 - acc: 0.7456 - val_loss: 0.4254 - val_acc: 0.6822\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1458 - acc: 0.7479 - val_loss: 0.4168 - val_acc: 0.6884\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1403 - acc: 0.7490 - val_loss: 0.4315 - val_acc: 0.6911\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.1355 - acc: 0.7501 - val_loss: 0.4100 - val_acc: 0.6847\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1370 - acc: 0.7497 - val_loss: 0.4785 - val_acc: 0.6846\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1437 - acc: 0.7482 - val_loss: 0.4331 - val_acc: 0.6891\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1308 - acc: 0.7509 - val_loss: 0.4496 - val_acc: 0.6917\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1291 - acc: 0.7514 - val_loss: 0.4373 - val_acc: 0.6910\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.1260 - acc: 0.7519 - val_loss: 0.4346 - val_acc: 0.6895\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1241 - acc: 0.7523 - val_loss: 0.4356 - val_acc: 0.6878\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1229 - acc: 0.7525 - val_loss: 0.4479 - val_acc: 0.6900\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1224 - acc: 0.7526 - val_loss: 0.4481 - val_acc: 0.6906\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1238 - acc: 0.7525 - val_loss: 0.4578 - val_acc: 0.6910\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1203 - acc: 0.7530 - val_loss: 0.4443 - val_acc: 0.6873\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1192 - acc: 0.7533 - val_loss: 0.4480 - val_acc: 0.6893\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1180 - acc: 0.7534 - val_loss: 0.4958 - val_acc: 0.6933\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1246 - acc: 0.7527 - val_loss: 0.4494 - val_acc: 0.6881\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1181 - acc: 0.7535 - val_loss: 0.4489 - val_acc: 0.6914\n",
      "17922/17922 [==============================] - 1s 74us/sample - loss: 1.0034 - acc: 0.6096\n",
      "35842/35842 [==============================] - 3s 74us/sample - loss: 0.1169 - acc: 0.7537\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 512us/sample - loss: 0.2628 - acc: 0.7236 - val_loss: 0.4063 - val_acc: 0.6838\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 126us/sample - loss: 0.1764 - acc: 0.7487 - val_loss: 0.4048 - val_acc: 0.6873\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1597 - acc: 0.7522 - val_loss: 0.3914 - val_acc: 0.6951\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1500 - acc: 0.7543 - val_loss: 0.4183 - val_acc: 0.6882\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1513 - acc: 0.7543 - val_loss: 0.3724 - val_acc: 0.6984\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1406 - acc: 0.7564 - val_loss: 0.3881 - val_acc: 0.7010\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1392 - acc: 0.7570 - val_loss: 0.3733 - val_acc: 0.7043\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1329 - acc: 0.7581 - val_loss: 0.3566 - val_acc: 0.7035\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1298 - acc: 0.7588 - val_loss: 0.3664 - val_acc: 0.7000\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1313 - acc: 0.7585 - val_loss: 0.3750 - val_acc: 0.7002\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1300 - acc: 0.7588 - val_loss: 0.3843 - val_acc: 0.7040\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1244 - acc: 0.7599 - val_loss: 0.3650 - val_acc: 0.7045\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1282 - acc: 0.7592 - val_loss: 0.3779 - val_acc: 0.7046\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1223 - acc: 0.7603 - val_loss: 0.3780 - val_acc: 0.7062\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1212 - acc: 0.7605 - val_loss: 0.3957 - val_acc: 0.7057\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1200 - acc: 0.7607 - val_loss: 0.3814 - val_acc: 0.7055\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1180 - acc: 0.7611 - val_loss: 0.3835 - val_acc: 0.7045\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1186 - acc: 0.7611 - val_loss: 0.4147 - val_acc: 0.7052\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1187 - acc: 0.7612 - val_loss: 0.3986 - val_acc: 0.7061\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1155 - acc: 0.7615 - val_loss: 0.4127 - val_acc: 0.7060\n",
      "17921/17921 [==============================] - 1s 74us/sample - loss: 0.8327 - acc: 0.6561\n",
      "35843/35843 [==============================] - 3s 74us/sample - loss: 0.1141 - acc: 0.7619\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 19s 525us/sample - loss: 0.2828 - acc: 0.6919 - val_loss: 0.4269 - val_acc: 0.6600\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1969 - acc: 0.7183 - val_loss: 0.4082 - val_acc: 0.6770\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1822 - acc: 0.7220 - val_loss: 0.3950 - val_acc: 0.6760\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1724 - acc: 0.7245 - val_loss: 0.4352 - val_acc: 0.6837\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1689 - acc: 0.7254 - val_loss: 0.3929 - val_acc: 0.6835\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1597 - acc: 0.7277 - val_loss: 0.3906 - val_acc: 0.6815\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1567 - acc: 0.7284 - val_loss: 0.3942 - val_acc: 0.6823\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1524 - acc: 0.7294 - val_loss: 0.3813 - val_acc: 0.6848\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1502 - acc: 0.7298 - val_loss: 0.3992 - val_acc: 0.6867\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1487 - acc: 0.7302 - val_loss: 0.3984 - val_acc: 0.6848\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1452 - acc: 0.7309 - val_loss: 0.3885 - val_acc: 0.6867\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1430 - acc: 0.7314 - val_loss: 0.3753 - val_acc: 0.6817\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1456 - acc: 0.7309 - val_loss: 0.4445 - val_acc: 0.6847\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1451 - acc: 0.7312 - val_loss: 0.3830 - val_acc: 0.6845\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1411 - acc: 0.7318 - val_loss: 0.3989 - val_acc: 0.6874\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1397 - acc: 0.7321 - val_loss: 0.4038 - val_acc: 0.6896\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1397 - acc: 0.7321 - val_loss: 0.3982 - val_acc: 0.6865\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1367 - acc: 0.7326 - val_loss: 0.3820 - val_acc: 0.6873\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1337 - acc: 0.7332 - val_loss: 0.3990 - val_acc: 0.6870\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1343 - acc: 0.7332 - val_loss: 0.3951 - val_acc: 0.6867\n",
      "17921/17921 [==============================] - 1s 74us/sample - loss: 0.6215 - acc: 0.6706\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1317 - acc: 0.7338\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 18s 510us/sample - loss: 0.2798 - acc: 0.7070 - val_loss: 0.4111 - val_acc: 0.6789\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1772 - acc: 0.7405 - val_loss: 0.4166 - val_acc: 0.6803\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1596 - acc: 0.7448 - val_loss: 0.4376 - val_acc: 0.6818\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1502 - acc: 0.7469 - val_loss: 0.4326 - val_acc: 0.6777\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1488 - acc: 0.7473 - val_loss: 0.4686 - val_acc: 0.6728\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1474 - acc: 0.7476 - val_loss: 0.4795 - val_acc: 0.6854\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1451 - acc: 0.7481 - val_loss: 0.4387 - val_acc: 0.6875\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1441 - acc: 0.7478 - val_loss: 0.4314 - val_acc: 0.6866\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1487 - acc: 0.7472 - val_loss: 0.4585 - val_acc: 0.6763\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1398 - acc: 0.7491 - val_loss: 0.4267 - val_acc: 0.6872\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.1345 - acc: 0.7502 - val_loss: 0.4387 - val_acc: 0.6866\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1329 - acc: 0.7506 - val_loss: 0.4507 - val_acc: 0.6869\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1358 - acc: 0.7499 - val_loss: 0.4445 - val_acc: 0.6880\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 126us/sample - loss: 0.1311 - acc: 0.7509 - val_loss: 0.4259 - val_acc: 0.6884\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1291 - acc: 0.7514 - val_loss: 0.4448 - val_acc: 0.6894\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1272 - acc: 0.7517 - val_loss: 0.4290 - val_acc: 0.6888\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1263 - acc: 0.7519 - val_loss: 0.4443 - val_acc: 0.6845\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1345 - acc: 0.7502 - val_loss: 0.4461 - val_acc: 0.6878\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1285 - acc: 0.7514 - val_loss: 0.4680 - val_acc: 0.6877\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1312 - acc: 0.7513 - val_loss: 0.4400 - val_acc: 0.6880\n",
      "17922/17922 [==============================] - 1s 77us/sample - loss: 0.9174 - acc: 0.6080\n",
      "35842/35842 [==============================] - 3s 76us/sample - loss: 0.1248 - acc: 0.7521\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 19s 520us/sample - loss: 0.2869 - acc: 0.7151 - val_loss: 0.4394 - val_acc: 0.6810\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1838 - acc: 0.7474 - val_loss: 0.4025 - val_acc: 0.6907\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1660 - acc: 0.7511 - val_loss: 0.3722 - val_acc: 0.6950\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1553 - acc: 0.7536 - val_loss: 0.4122 - val_acc: 0.6972\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1508 - acc: 0.7547 - val_loss: 0.4055 - val_acc: 0.6981\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1455 - acc: 0.7557 - val_loss: 0.3715 - val_acc: 0.6997\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1412 - acc: 0.7566 - val_loss: 0.3897 - val_acc: 0.7012\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1380 - acc: 0.7572 - val_loss: 0.4023 - val_acc: 0.7033\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1355 - acc: 0.7577 - val_loss: 0.3852 - val_acc: 0.7035\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1360 - acc: 0.7576 - val_loss: 0.3827 - val_acc: 0.7039\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1352 - acc: 0.7579 - val_loss: 0.4002 - val_acc: 0.7026\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1312 - acc: 0.7587 - val_loss: 0.3916 - val_acc: 0.7034\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1310 - acc: 0.7587 - val_loss: 0.3847 - val_acc: 0.7041\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1290 - acc: 0.7591 - val_loss: 0.3820 - val_acc: 0.7000\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1320 - acc: 0.7587 - val_loss: 0.4102 - val_acc: 0.7038\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1305 - acc: 0.7591 - val_loss: 0.4331 - val_acc: 0.7024\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1369 - acc: 0.7574 - val_loss: 0.3908 - val_acc: 0.7045\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1267 - acc: 0.7597 - val_loss: 0.4046 - val_acc: 0.7046\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1251 - acc: 0.7600 - val_loss: 0.3792 - val_acc: 0.7026\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1267 - acc: 0.7597 - val_loss: 0.3924 - val_acc: 0.7047\n",
      "17921/17921 [==============================] - 1s 74us/sample - loss: 0.7433 - acc: 0.6536\n",
      "35843/35843 [==============================] - 3s 74us/sample - loss: 0.1219 - acc: 0.7607\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 515us/sample - loss: 0.3061 - acc: 0.6852 - val_loss: 0.3978 - val_acc: 0.6696\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.2051 - acc: 0.7168 - val_loss: 0.4209 - val_acc: 0.6728\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1929 - acc: 0.7196 - val_loss: 0.3819 - val_acc: 0.6784\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1779 - acc: 0.7231 - val_loss: 0.3970 - val_acc: 0.6759\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1742 - acc: 0.7240 - val_loss: 0.3963 - val_acc: 0.6789\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1678 - acc: 0.7257 - val_loss: 0.3813 - val_acc: 0.6743\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1738 - acc: 0.7244 - val_loss: 0.3912 - val_acc: 0.6787\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1624 - acc: 0.7270 - val_loss: 0.3853 - val_acc: 0.6784\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1586 - acc: 0.7278 - val_loss: 0.4196 - val_acc: 0.6852\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1581 - acc: 0.7281 - val_loss: 0.3823 - val_acc: 0.6829\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1535 - acc: 0.7291 - val_loss: 0.3960 - val_acc: 0.6806\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1564 - acc: 0.7285 - val_loss: 0.4167 - val_acc: 0.6802\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1509 - acc: 0.7296 - val_loss: 0.4203 - val_acc: 0.6816\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1498 - acc: 0.7298 - val_loss: 0.3952 - val_acc: 0.6840\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1471 - acc: 0.7305 - val_loss: 0.4083 - val_acc: 0.6836\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1450 - acc: 0.7309 - val_loss: 0.3964 - val_acc: 0.6808\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1448 - acc: 0.7310 - val_loss: 0.3936 - val_acc: 0.6829\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1448 - acc: 0.7310 - val_loss: 0.3823 - val_acc: 0.6836\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1431 - acc: 0.7314 - val_loss: 0.4050 - val_acc: 0.6822\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1457 - acc: 0.7309 - val_loss: 0.4127 - val_acc: 0.6808\n",
      "17921/17921 [==============================] - 1s 74us/sample - loss: 0.6477 - acc: 0.6659\n",
      "35843/35843 [==============================] - 3s 74us/sample - loss: 0.1430 - acc: 0.7313\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 18s 508us/sample - loss: 0.3327 - acc: 0.6871 - val_loss: 0.4193 - val_acc: 0.6758\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1887 - acc: 0.7379 - val_loss: 0.4449 - val_acc: 0.6698\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1744 - acc: 0.7407 - val_loss: 0.4401 - val_acc: 0.6825\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1663 - acc: 0.7430 - val_loss: 0.4420 - val_acc: 0.6827\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1578 - acc: 0.7452 - val_loss: 0.4336 - val_acc: 0.6828\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1534 - acc: 0.7465 - val_loss: 0.4292 - val_acc: 0.6866\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1494 - acc: 0.7476 - val_loss: 0.4276 - val_acc: 0.6868\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1458 - acc: 0.7484 - val_loss: 0.4359 - val_acc: 0.6862\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1437 - acc: 0.7488 - val_loss: 0.4537 - val_acc: 0.6874\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1416 - acc: 0.7492 - val_loss: 0.4409 - val_acc: 0.6885\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1392 - acc: 0.7497 - val_loss: 0.4274 - val_acc: 0.6869\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1378 - acc: 0.7499 - val_loss: 0.4578 - val_acc: 0.6837\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1382 - acc: 0.7499 - val_loss: 0.4949 - val_acc: 0.6820\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1440 - acc: 0.7489 - val_loss: 0.4354 - val_acc: 0.6881\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1352 - acc: 0.7505 - val_loss: 0.4507 - val_acc: 0.6884\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1334 - acc: 0.7509 - val_loss: 0.4495 - val_acc: 0.6883\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1325 - acc: 0.7511 - val_loss: 0.4700 - val_acc: 0.6881\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1321 - acc: 0.7511 - val_loss: 0.4721 - val_acc: 0.6846\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1355 - acc: 0.7504 - val_loss: 0.4734 - val_acc: 0.6863\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1373 - acc: 0.7497 - val_loss: 0.4628 - val_acc: 0.6867\n",
      "17922/17922 [==============================] - 1s 73us/sample - loss: 0.9801 - acc: 0.6102\n",
      "35842/35842 [==============================] - 3s 73us/sample - loss: 0.1300 - acc: 0.7515\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 512us/sample - loss: 0.3235 - acc: 0.7029 - val_loss: 0.4368 - val_acc: 0.6784\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2007 - acc: 0.7439 - val_loss: 0.4186 - val_acc: 0.6820\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1771 - acc: 0.7487 - val_loss: 0.4134 - val_acc: 0.6844\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1754 - acc: 0.7489 - val_loss: 0.4030 - val_acc: 0.6876\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1646 - acc: 0.7512 - val_loss: 0.3925 - val_acc: 0.6878\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1597 - acc: 0.7522 - val_loss: 0.3868 - val_acc: 0.6922\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1589 - acc: 0.7523 - val_loss: 0.3995 - val_acc: 0.6912\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1543 - acc: 0.7533 - val_loss: 0.3964 - val_acc: 0.6906\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1515 - acc: 0.7540 - val_loss: 0.3910 - val_acc: 0.6926\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1492 - acc: 0.7546 - val_loss: 0.3724 - val_acc: 0.6938\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1490 - acc: 0.7547 - val_loss: 0.4161 - val_acc: 0.6918\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1505 - acc: 0.7545 - val_loss: 0.3787 - val_acc: 0.6955\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1433 - acc: 0.7558 - val_loss: 0.4066 - val_acc: 0.6956\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1446 - acc: 0.7558 - val_loss: 0.3903 - val_acc: 0.6952\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1417 - acc: 0.7562 - val_loss: 0.3927 - val_acc: 0.6966\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1441 - acc: 0.7557 - val_loss: 0.3695 - val_acc: 0.6990\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1392 - acc: 0.7567 - val_loss: 0.4102 - val_acc: 0.6944\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1448 - acc: 0.7558 - val_loss: 0.3817 - val_acc: 0.6987\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1383 - acc: 0.7569 - val_loss: 0.3929 - val_acc: 0.6991\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1381 - acc: 0.7571 - val_loss: 0.3898 - val_acc: 0.7003\n",
      "17921/17921 [==============================] - 1s 74us/sample - loss: 0.6920 - acc: 0.6506\n",
      "35843/35843 [==============================] - 3s 73us/sample - loss: 0.1325 - acc: 0.7583\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 18s 516us/sample - loss: 0.3329 - acc: 0.6765 - val_loss: 0.4105 - val_acc: 0.6663\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.2195 - acc: 0.7130 - val_loss: 0.4088 - val_acc: 0.6691\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.2011 - acc: 0.7173 - val_loss: 0.3786 - val_acc: 0.6816\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1881 - acc: 0.7204 - val_loss: 0.3865 - val_acc: 0.6808\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1829 - acc: 0.7219 - val_loss: 0.4285 - val_acc: 0.6694\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1843 - acc: 0.7218 - val_loss: 0.4048 - val_acc: 0.6761\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1724 - acc: 0.7245 - val_loss: 0.4001 - val_acc: 0.6764\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1687 - acc: 0.7252 - val_loss: 0.3959 - val_acc: 0.6727\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1702 - acc: 0.7250 - val_loss: 0.4132 - val_acc: 0.6703\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1693 - acc: 0.7252 - val_loss: 0.4126 - val_acc: 0.6714\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1716 - acc: 0.7245 - val_loss: 0.4172 - val_acc: 0.6736\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1682 - acc: 0.7257 - val_loss: 0.3930 - val_acc: 0.6810\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1618 - acc: 0.7270 - val_loss: 0.4414 - val_acc: 0.6718\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1611 - acc: 0.7272 - val_loss: 0.4309 - val_acc: 0.6786\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1639 - acc: 0.7268 - val_loss: 0.4081 - val_acc: 0.6831\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1586 - acc: 0.7278 - val_loss: 0.3948 - val_acc: 0.6802\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1610 - acc: 0.7273 - val_loss: 0.3960 - val_acc: 0.6804\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1573 - acc: 0.7281 - val_loss: 0.4069 - val_acc: 0.6807\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1556 - acc: 0.7285 - val_loss: 0.4459 - val_acc: 0.6744\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.1667 - acc: 0.7263 - val_loss: 0.4105 - val_acc: 0.6799\n",
      "17921/17921 [==============================] - 1s 75us/sample - loss: 0.6024 - acc: 0.6712\n",
      "35843/35843 [==============================] - 3s 76us/sample - loss: 0.1608 - acc: 0.7275\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 18s 514us/sample - loss: 0.2581 - acc: 0.7155 - val_loss: 0.4175 - val_acc: 0.6758\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1690 - acc: 0.7423 - val_loss: 0.4295 - val_acc: 0.6841\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1547 - acc: 0.7456 - val_loss: 0.4284 - val_acc: 0.6812\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1477 - acc: 0.7472 - val_loss: 0.4141 - val_acc: 0.6850\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1465 - acc: 0.7476 - val_loss: 0.4258 - val_acc: 0.6887\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1377 - acc: 0.7494 - val_loss: 0.4239 - val_acc: 0.6878\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1336 - acc: 0.7502 - val_loss: 0.4125 - val_acc: 0.6905\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1316 - acc: 0.7507 - val_loss: 0.4299 - val_acc: 0.6881\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1293 - acc: 0.7512 - val_loss: 0.4379 - val_acc: 0.6909\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1297 - acc: 0.7512 - val_loss: 0.4253 - val_acc: 0.6896\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1255 - acc: 0.7519 - val_loss: 0.4449 - val_acc: 0.6874\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.1314 - acc: 0.7510 - val_loss: 0.4327 - val_acc: 0.6906\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1250 - acc: 0.7521 - val_loss: 0.4448 - val_acc: 0.6910\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1232 - acc: 0.7523 - val_loss: 0.4356 - val_acc: 0.6897\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.1278 - acc: 0.7515 - val_loss: 0.4413 - val_acc: 0.6926\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1228 - acc: 0.7524 - val_loss: 0.4506 - val_acc: 0.6913\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1193 - acc: 0.7531 - val_loss: 0.4317 - val_acc: 0.6905\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1183 - acc: 0.7533 - val_loss: 0.4418 - val_acc: 0.6895\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1176 - acc: 0.7534 - val_loss: 0.4290 - val_acc: 0.6899\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1181 - acc: 0.7533 - val_loss: 0.4504 - val_acc: 0.6899\n",
      "17922/17922 [==============================] - 1s 76us/sample - loss: 1.0262 - acc: 0.6041\n",
      "35842/35842 [==============================] - 3s 75us/sample - loss: 0.1150 - acc: 0.7539\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 19s 531us/sample - loss: 0.2635 - acc: 0.7245 - val_loss: 0.3968 - val_acc: 0.6799\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1829 - acc: 0.7472 - val_loss: 0.3885 - val_acc: 0.6889\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1610 - acc: 0.7518 - val_loss: 0.3896 - val_acc: 0.6904\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1563 - acc: 0.7529 - val_loss: 0.3947 - val_acc: 0.6940\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.1495 - acc: 0.7546 - val_loss: 0.3701 - val_acc: 0.6985\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1419 - acc: 0.7561 - val_loss: 0.3970 - val_acc: 0.6999\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1401 - acc: 0.7566 - val_loss: 0.3779 - val_acc: 0.7002\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1357 - acc: 0.7576 - val_loss: 0.3763 - val_acc: 0.6998\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1354 - acc: 0.7576 - val_loss: 0.3665 - val_acc: 0.7037\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1298 - acc: 0.7587 - val_loss: 0.3575 - val_acc: 0.7029\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1288 - acc: 0.7590 - val_loss: 0.3793 - val_acc: 0.7036\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1272 - acc: 0.7592 - val_loss: 0.3911 - val_acc: 0.7039\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1262 - acc: 0.7596 - val_loss: 0.3569 - val_acc: 0.7030\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1261 - acc: 0.7596 - val_loss: 0.3653 - val_acc: 0.7054\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1243 - acc: 0.7600 - val_loss: 0.3752 - val_acc: 0.7050\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1220 - acc: 0.7604 - val_loss: 0.3646 - val_acc: 0.7060\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1210 - acc: 0.7607 - val_loss: 0.3765 - val_acc: 0.7058\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1184 - acc: 0.7611 - val_loss: 0.3770 - val_acc: 0.7064\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1177 - acc: 0.7612 - val_loss: 0.3737 - val_acc: 0.7060\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1173 - acc: 0.7613 - val_loss: 0.4031 - val_acc: 0.7068\n",
      "17921/17921 [==============================] - 1s 75us/sample - loss: 0.7922 - acc: 0.6551\n",
      "35843/35843 [==============================] - 3s 74us/sample - loss: 0.1192 - acc: 0.7615\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 19s 521us/sample - loss: 0.2878 - acc: 0.6905 - val_loss: 0.4051 - val_acc: 0.6708\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1949 - acc: 0.7188 - val_loss: 0.3998 - val_acc: 0.6749\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1831 - acc: 0.7216 - val_loss: 0.4059 - val_acc: 0.6768\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1734 - acc: 0.7239 - val_loss: 0.4041 - val_acc: 0.6791\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1647 - acc: 0.7261 - val_loss: 0.4029 - val_acc: 0.6814\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1609 - acc: 0.7272 - val_loss: 0.3836 - val_acc: 0.6834\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1556 - acc: 0.7283 - val_loss: 0.3963 - val_acc: 0.6864\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1529 - acc: 0.7291 - val_loss: 0.3725 - val_acc: 0.6836\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1568 - acc: 0.7285 - val_loss: 0.4060 - val_acc: 0.6783\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1586 - acc: 0.7281 - val_loss: 0.3822 - val_acc: 0.6864\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1466 - acc: 0.7305 - val_loss: 0.3830 - val_acc: 0.6860\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1442 - acc: 0.7311 - val_loss: 0.4140 - val_acc: 0.6841\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1415 - acc: 0.7316 - val_loss: 0.3907 - val_acc: 0.6870\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1401 - acc: 0.7319 - val_loss: 0.3948 - val_acc: 0.6867\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1383 - acc: 0.7323 - val_loss: 0.3823 - val_acc: 0.6848\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1373 - acc: 0.7324 - val_loss: 0.3855 - val_acc: 0.6888\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1389 - acc: 0.7322 - val_loss: 0.3782 - val_acc: 0.6903\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1407 - acc: 0.7318 - val_loss: 0.4097 - val_acc: 0.6854\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1351 - acc: 0.7329 - val_loss: 0.3893 - val_acc: 0.6861\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1336 - acc: 0.7333 - val_loss: 0.4059 - val_acc: 0.6846\n",
      "17921/17921 [==============================] - 1s 76us/sample - loss: 0.6368 - acc: 0.6691\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1329 - acc: 0.7334\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      " 3840/35842 [==>...........................] - ETA: 1:07 - loss: 0.5716 - acc: 0.5624"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1711 - acc: 0.7497 - val_loss: 0.3915 - val_acc: 0.6875\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1660 - acc: 0.7509 - val_loss: 0.3810 - val_acc: 0.6950\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1522 - acc: 0.7538 - val_loss: 0.3786 - val_acc: 0.6956\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1478 - acc: 0.7549 - val_loss: 0.3967 - val_acc: 0.6981\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1457 - acc: 0.7555 - val_loss: 0.3879 - val_acc: 0.6987\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1411 - acc: 0.7564 - val_loss: 0.3745 - val_acc: 0.7001\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1380 - acc: 0.7571 - val_loss: 0.3780 - val_acc: 0.7014\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1366 - acc: 0.7574 - val_loss: 0.3876 - val_acc: 0.7011\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1392 - acc: 0.7570 - val_loss: 0.3908 - val_acc: 0.7037\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1318 - acc: 0.7584 - val_loss: 0.3805 - val_acc: 0.7017\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1307 - acc: 0.7587 - val_loss: 0.3677 - val_acc: 0.7038\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1289 - acc: 0.7590 - val_loss: 0.3792 - val_acc: 0.7036\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1274 - acc: 0.7593 - val_loss: 0.3666 - val_acc: 0.7047\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1264 - acc: 0.7596 - val_loss: 0.3842 - val_acc: 0.7028\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1286 - acc: 0.7590 - val_loss: 0.3707 - val_acc: 0.7045\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1266 - acc: 0.7595 - val_loss: 0.3803 - val_acc: 0.7042\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1243 - acc: 0.7600 - val_loss: 0.4075 - val_acc: 0.7046\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1228 - acc: 0.7602 - val_loss: 0.3911 - val_acc: 0.7015\n",
      "17921/17921 [==============================] - 1s 78us/sample - loss: 0.7278 - acc: 0.6508\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1307 - acc: 0.7587\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 19s 523us/sample - loss: 0.3131 - acc: 0.6829 - val_loss: 0.3892 - val_acc: 0.6778\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.2099 - acc: 0.7157 - val_loss: 0.3946 - val_acc: 0.6709\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1980 - acc: 0.7187 - val_loss: 0.3802 - val_acc: 0.6793\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1797 - acc: 0.7229 - val_loss: 0.3913 - val_acc: 0.6824\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1729 - acc: 0.7245 - val_loss: 0.3822 - val_acc: 0.6828\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1677 - acc: 0.7257 - val_loss: 0.3952 - val_acc: 0.6755\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1751 - acc: 0.7241 - val_loss: 0.3847 - val_acc: 0.6817\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1624 - acc: 0.7269 - val_loss: 0.3688 - val_acc: 0.6827\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1589 - acc: 0.7279 - val_loss: 0.4082 - val_acc: 0.6837\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1569 - acc: 0.7282 - val_loss: 0.3884 - val_acc: 0.6825\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1538 - acc: 0.7290 - val_loss: 0.4128 - val_acc: 0.6827\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1524 - acc: 0.7294 - val_loss: 0.3877 - val_acc: 0.6841\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1506 - acc: 0.7297 - val_loss: 0.4051 - val_acc: 0.6808\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1522 - acc: 0.7295 - val_loss: 0.3922 - val_acc: 0.6806\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1482 - acc: 0.7303 - val_loss: 0.3986 - val_acc: 0.6795\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1519 - acc: 0.7296 - val_loss: 0.4194 - val_acc: 0.6826\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1525 - acc: 0.7294 - val_loss: 0.4182 - val_acc: 0.6872\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1476 - acc: 0.7305 - val_loss: 0.3979 - val_acc: 0.6864\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1519 - acc: 0.7297 - val_loss: 0.4155 - val_acc: 0.6797\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.1533 - acc: 0.7291 - val_loss: 0.4135 - val_acc: 0.6822\n",
      "17921/17921 [==============================] - 1s 76us/sample - loss: 0.6389 - acc: 0.6688\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1465 - acc: 0.7311\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 20s 547us/sample - loss: 0.3216 - acc: 0.6934 - val_loss: 0.4292 - val_acc: 0.6728\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1860 - acc: 0.7392 - val_loss: 0.4319 - val_acc: 0.6801\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1703 - acc: 0.7424 - val_loss: 0.4422 - val_acc: 0.6820\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1623 - acc: 0.7442 - val_loss: 0.4299 - val_acc: 0.6833\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1567 - acc: 0.7457 - val_loss: 0.4354 - val_acc: 0.6830\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 126us/sample - loss: 0.1535 - acc: 0.7467 - val_loss: 0.4348 - val_acc: 0.6818\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1481 - acc: 0.7479 - val_loss: 0.4533 - val_acc: 0.6816\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1468 - acc: 0.7481 - val_loss: 0.4235 - val_acc: 0.6834\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1473 - acc: 0.7479 - val_loss: 0.4633 - val_acc: 0.6877\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1472 - acc: 0.7481 - val_loss: 0.4304 - val_acc: 0.6899\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1422 - acc: 0.7489 - val_loss: 0.4286 - val_acc: 0.6855\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1384 - acc: 0.7497 - val_loss: 0.4417 - val_acc: 0.6864\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1376 - acc: 0.7500 - val_loss: 0.4448 - val_acc: 0.6880\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1361 - acc: 0.7502 - val_loss: 0.4530 - val_acc: 0.6876\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1355 - acc: 0.7503 - val_loss: 0.4511 - val_acc: 0.6858\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1353 - acc: 0.7504 - val_loss: 0.4322 - val_acc: 0.6874\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1335 - acc: 0.7507 - val_loss: 0.4470 - val_acc: 0.6869\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.1334 - acc: 0.7508 - val_loss: 0.4552 - val_acc: 0.6886\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1322 - acc: 0.7510 - val_loss: 0.4413 - val_acc: 0.6860\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1324 - acc: 0.7510 - val_loss: 0.4637 - val_acc: 0.6869\n",
      "17922/17922 [==============================] - 1s 75us/sample - loss: 0.9835 - acc: 0.6084\n",
      "35842/35842 [==============================] - 3s 75us/sample - loss: 0.1305 - acc: 0.7517\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 19s 534us/sample - loss: 0.3215 - acc: 0.7045 - val_loss: 0.4459 - val_acc: 0.6648\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.2015 - acc: 0.7431 - val_loss: 0.4082 - val_acc: 0.6849\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1783 - acc: 0.7481 - val_loss: 0.3846 - val_acc: 0.6911\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1690 - acc: 0.7504 - val_loss: 0.3798 - val_acc: 0.6931\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1614 - acc: 0.7519 - val_loss: 0.3840 - val_acc: 0.6941\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1572 - acc: 0.7529 - val_loss: 0.3787 - val_acc: 0.6970\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.1609 - acc: 0.7519 - val_loss: 0.3740 - val_acc: 0.6975\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1511 - acc: 0.7541 - val_loss: 0.3802 - val_acc: 0.6954\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1486 - acc: 0.7546 - val_loss: 0.3836 - val_acc: 0.6969\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1460 - acc: 0.7550 - val_loss: 0.3817 - val_acc: 0.6988\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1496 - acc: 0.7543 - val_loss: 0.3793 - val_acc: 0.6942\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1459 - acc: 0.7551 - val_loss: 0.3993 - val_acc: 0.6957\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1434 - acc: 0.7556 - val_loss: 0.3937 - val_acc: 0.6976\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1423 - acc: 0.7559 - val_loss: 0.3800 - val_acc: 0.6969\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1414 - acc: 0.7561 - val_loss: 0.4178 - val_acc: 0.6980\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1398 - acc: 0.7565 - val_loss: 0.3930 - val_acc: 0.6982\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1395 - acc: 0.7565 - val_loss: 0.3906 - val_acc: 0.6990\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1378 - acc: 0.7569 - val_loss: 0.4180 - val_acc: 0.6984\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1376 - acc: 0.7570 - val_loss: 0.4085 - val_acc: 0.6984\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1360 - acc: 0.7573 - val_loss: 0.3898 - val_acc: 0.7003\n",
      "17921/17921 [==============================] - 1s 77us/sample - loss: 0.7237 - acc: 0.6507\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.1324 - acc: 0.7582\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 552us/sample - loss: 0.3459 - acc: 0.6700 - val_loss: 0.4004 - val_acc: 0.6743\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.2220 - acc: 0.7125 - val_loss: 0.4050 - val_acc: 0.6694\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.2040 - acc: 0.7167 - val_loss: 0.3894 - val_acc: 0.6774\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1887 - acc: 0.7205 - val_loss: 0.3915 - val_acc: 0.6745\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1801 - acc: 0.7226 - val_loss: 0.4075 - val_acc: 0.6673\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1832 - acc: 0.7216 - val_loss: 0.3904 - val_acc: 0.6788\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1781 - acc: 0.7230 - val_loss: 0.3802 - val_acc: 0.6797\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1718 - acc: 0.7246 - val_loss: 0.4022 - val_acc: 0.6789\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1710 - acc: 0.7249 - val_loss: 0.4173 - val_acc: 0.6826\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1678 - acc: 0.7258 - val_loss: 0.4001 - val_acc: 0.6774\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1700 - acc: 0.7252 - val_loss: 0.4556 - val_acc: 0.6739\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1701 - acc: 0.7256 - val_loss: 0.4021 - val_acc: 0.6801\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1662 - acc: 0.7264 - val_loss: 0.3982 - val_acc: 0.6800\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1619 - acc: 0.7272 - val_loss: 0.4037 - val_acc: 0.6810\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1609 - acc: 0.7275 - val_loss: 0.4104 - val_acc: 0.6799\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1595 - acc: 0.7278 - val_loss: 0.4378 - val_acc: 0.6805\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1600 - acc: 0.7279 - val_loss: 0.3991 - val_acc: 0.6804\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1565 - acc: 0.7285 - val_loss: 0.4003 - val_acc: 0.6791\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1555 - acc: 0.7287 - val_loss: 0.4319 - val_acc: 0.6786\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1562 - acc: 0.7286 - val_loss: 0.4000 - val_acc: 0.6805\n",
      "17921/17921 [==============================] - 1s 75us/sample - loss: 0.5740 - acc: 0.6736\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1608 - acc: 0.7272\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      " 3840/35842 [==>...........................] - ETA: 1:10 - loss: 0.5250 - acc: 0.5965"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1186 - acc: 0.7532 - val_loss: 0.4354 - val_acc: 0.6900\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1177 - acc: 0.7535 - val_loss: 0.4293 - val_acc: 0.6916\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.1161 - acc: 0.7537 - val_loss: 0.4480 - val_acc: 0.6805\n",
      "17922/17922 [==============================] - 1s 78us/sample - loss: 0.9915 - acc: 0.5897\n",
      "35842/35842 [==============================] - 3s 77us/sample - loss: 0.1237 - acc: 0.7518\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 546us/sample - loss: 0.2638 - acc: 0.7236 - val_loss: 0.3953 - val_acc: 0.6812\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1755 - acc: 0.7488 - val_loss: 0.3902 - val_acc: 0.6911\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1606 - acc: 0.7519 - val_loss: 0.3861 - val_acc: 0.6896\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1593 - acc: 0.7523 - val_loss: 0.3992 - val_acc: 0.6944\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1475 - acc: 0.7548 - val_loss: 0.3890 - val_acc: 0.6997\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.1425 - acc: 0.7560 - val_loss: 0.3853 - val_acc: 0.7011\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1393 - acc: 0.7567 - val_loss: 0.3600 - val_acc: 0.7020\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1349 - acc: 0.7577 - val_loss: 0.3766 - val_acc: 0.7021\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1358 - acc: 0.7576 - val_loss: 0.3615 - val_acc: 0.7034\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1342 - acc: 0.7580 - val_loss: 0.3875 - val_acc: 0.7036\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1295 - acc: 0.7588 - val_loss: 0.3851 - val_acc: 0.7044\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1269 - acc: 0.7593 - val_loss: 0.3713 - val_acc: 0.7053\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1253 - acc: 0.7596 - val_loss: 0.3875 - val_acc: 0.7037\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.1280 - acc: 0.7591 - val_loss: 0.3907 - val_acc: 0.7044\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1237 - acc: 0.7600 - val_loss: 0.4223 - val_acc: 0.7007\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1271 - acc: 0.7593 - val_loss: 0.3996 - val_acc: 0.7054\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1212 - acc: 0.7604 - val_loss: 0.3798 - val_acc: 0.7048\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1223 - acc: 0.7603 - val_loss: 0.4114 - val_acc: 0.7056\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1193 - acc: 0.7609 - val_loss: 0.3942 - val_acc: 0.7058\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1183 - acc: 0.7610 - val_loss: 0.4273 - val_acc: 0.7039\n",
      "17921/17921 [==============================] - 1s 77us/sample - loss: 0.8186 - acc: 0.6538\n",
      "35843/35843 [==============================] - 3s 78us/sample - loss: 0.1206 - acc: 0.7606\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 558us/sample - loss: 0.2899 - acc: 0.6900 - val_loss: 0.4044 - val_acc: 0.6715\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1970 - acc: 0.7182 - val_loss: 0.3823 - val_acc: 0.6734\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1808 - acc: 0.7222 - val_loss: 0.3796 - val_acc: 0.6826\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1708 - acc: 0.7246 - val_loss: 0.3873 - val_acc: 0.6810\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1651 - acc: 0.7261 - val_loss: 0.3851 - val_acc: 0.6865\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1612 - acc: 0.7271 - val_loss: 0.3887 - val_acc: 0.6837\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1667 - acc: 0.7261 - val_loss: 0.4054 - val_acc: 0.6875\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1563 - acc: 0.7282 - val_loss: 0.4101 - val_acc: 0.6819\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1510 - acc: 0.7294 - val_loss: 0.3986 - val_acc: 0.6882\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1490 - acc: 0.7300 - val_loss: 0.3847 - val_acc: 0.6829\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1462 - acc: 0.7305 - val_loss: 0.3986 - val_acc: 0.6815\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1442 - acc: 0.7310 - val_loss: 0.3916 - val_acc: 0.6835\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1461 - acc: 0.7307 - val_loss: 0.4001 - val_acc: 0.6860\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1406 - acc: 0.7318 - val_loss: 0.3937 - val_acc: 0.6882\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1393 - acc: 0.7321 - val_loss: 0.3748 - val_acc: 0.6884\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1433 - acc: 0.7311 - val_loss: 0.3912 - val_acc: 0.6851\n",
      "Epoch 17/20\n",
      "12032/35843 [=========>....................] - ETA: 2s - loss: 0.1381 - acc: 0.7322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.1398 - acc: 0.7565 - val_loss: 0.3811 - val_acc: 0.6973\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1380 - acc: 0.7570 - val_loss: 0.4061 - val_acc: 0.6996\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1364 - acc: 0.7573 - val_loss: 0.3960 - val_acc: 0.6991\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1392 - acc: 0.7568 - val_loss: 0.3718 - val_acc: 0.7013\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1329 - acc: 0.7581 - val_loss: 0.3812 - val_acc: 0.6977\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1408 - acc: 0.7567 - val_loss: 0.3993 - val_acc: 0.7010\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1319 - acc: 0.7586 - val_loss: 0.3791 - val_acc: 0.7014\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1289 - acc: 0.7591 - val_loss: 0.3712 - val_acc: 0.7051\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1275 - acc: 0.7594 - val_loss: 0.3800 - val_acc: 0.7042\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1261 - acc: 0.7596 - val_loss: 0.3944 - val_acc: 0.7017\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1319 - acc: 0.7585 - val_loss: 0.4244 - val_acc: 0.6976\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1328 - acc: 0.7584 - val_loss: 0.4166 - val_acc: 0.7032\n",
      "17921/17921 [==============================] - 1s 78us/sample - loss: 0.7671 - acc: 0.6547\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.1273 - acc: 0.7598\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 562us/sample - loss: 0.3090 - acc: 0.6845 - val_loss: 0.4017 - val_acc: 0.6740\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.2078 - acc: 0.7160 - val_loss: 0.3940 - val_acc: 0.6731\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1984 - acc: 0.7183 - val_loss: 0.3874 - val_acc: 0.6788\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1829 - acc: 0.7218 - val_loss: 0.3944 - val_acc: 0.6777\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1755 - acc: 0.7237 - val_loss: 0.4548 - val_acc: 0.6723\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1767 - acc: 0.7237 - val_loss: 0.3807 - val_acc: 0.6807\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1666 - acc: 0.7260 - val_loss: 0.4075 - val_acc: 0.6839\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1688 - acc: 0.7257 - val_loss: 0.3839 - val_acc: 0.6813\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1610 - acc: 0.7273 - val_loss: 0.3815 - val_acc: 0.6802\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1576 - acc: 0.7281 - val_loss: 0.3866 - val_acc: 0.6794\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1563 - acc: 0.7284 - val_loss: 0.3899 - val_acc: 0.6794\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1536 - acc: 0.7291 - val_loss: 0.3929 - val_acc: 0.6787\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1523 - acc: 0.7294 - val_loss: 0.3770 - val_acc: 0.6825\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1500 - acc: 0.7299 - val_loss: 0.3922 - val_acc: 0.6826\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1495 - acc: 0.7300 - val_loss: 0.3931 - val_acc: 0.6782\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1543 - acc: 0.7288 - val_loss: 0.3896 - val_acc: 0.6816\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1475 - acc: 0.7305 - val_loss: 0.3940 - val_acc: 0.6856\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1476 - acc: 0.7306 - val_loss: 0.3938 - val_acc: 0.6809\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1449 - acc: 0.7310 - val_loss: 0.3873 - val_acc: 0.6853\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1490 - acc: 0.7302 - val_loss: 0.4000 - val_acc: 0.6822\n",
      "17921/17921 [==============================] - 1s 73us/sample - loss: 0.6127 - acc: 0.6658\n",
      "35843/35843 [==============================] - 3s 70us/sample - loss: 0.1425 - acc: 0.7316\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 20s 565us/sample - loss: 0.3148 - acc: 0.6970 - val_loss: 0.4384 - val_acc: 0.6729\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1921 - acc: 0.7376 - val_loss: 0.4130 - val_acc: 0.6747\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.1732 - acc: 0.7416 - val_loss: 0.4201 - val_acc: 0.6780\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1656 - acc: 0.7434 - val_loss: 0.4534 - val_acc: 0.6821\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1620 - acc: 0.7442 - val_loss: 0.4267 - val_acc: 0.6864\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1575 - acc: 0.7453 - val_loss: 0.4208 - val_acc: 0.6858\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1524 - acc: 0.7466 - val_loss: 0.4216 - val_acc: 0.6865\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1475 - acc: 0.7476 - val_loss: 0.4786 - val_acc: 0.6801\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1548 - acc: 0.7465 - val_loss: 0.4357 - val_acc: 0.6859\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1439 - acc: 0.7482 - val_loss: 0.4436 - val_acc: 0.6843\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1414 - acc: 0.7488 - val_loss: 0.4388 - val_acc: 0.6881\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1403 - acc: 0.7490 - val_loss: 0.4258 - val_acc: 0.6887\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1383 - acc: 0.7494 - val_loss: 0.4371 - val_acc: 0.6875\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1383 - acc: 0.7494 - val_loss: 0.4433 - val_acc: 0.6863\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1362 - acc: 0.7498 - val_loss: 0.4507 - val_acc: 0.6894\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1362 - acc: 0.7499 - val_loss: 0.4580 - val_acc: 0.6865\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1341 - acc: 0.7502 - val_loss: 0.4304 - val_acc: 0.6864\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1333 - acc: 0.7504 - val_loss: 0.4493 - val_acc: 0.6868\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1325 - acc: 0.7505 - val_loss: 0.4330 - val_acc: 0.6859\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1317 - acc: 0.7507 - val_loss: 0.4583 - val_acc: 0.6881\n",
      "17922/17922 [==============================] - 1s 77us/sample - loss: 0.9700 - acc: 0.6085\n",
      "35842/35842 [==============================] - 3s 76us/sample - loss: 0.1296 - acc: 0.7513\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 21s 577us/sample - loss: 0.3261 - acc: 0.7022 - val_loss: 0.4147 - val_acc: 0.6734\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.2001 - acc: 0.7440 - val_loss: 0.4342 - val_acc: 0.6833\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1820 - acc: 0.7476 - val_loss: 0.3848 - val_acc: 0.6878\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1743 - acc: 0.7492 - val_loss: 0.3989 - val_acc: 0.6896\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1646 - acc: 0.7511 - val_loss: 0.4053 - val_acc: 0.6892\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1599 - acc: 0.7523 - val_loss: 0.4004 - val_acc: 0.6885\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1555 - acc: 0.7532 - val_loss: 0.4022 - val_acc: 0.6927\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1555 - acc: 0.7534 - val_loss: 0.3923 - val_acc: 0.6937\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1508 - acc: 0.7543 - val_loss: 0.3968 - val_acc: 0.6953\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1471 - acc: 0.7551 - val_loss: 0.4458 - val_acc: 0.6944\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1488 - acc: 0.7549 - val_loss: 0.4012 - val_acc: 0.6942\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1480 - acc: 0.7549 - val_loss: 0.3790 - val_acc: 0.6973\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1431 - acc: 0.7560 - val_loss: 0.3987 - val_acc: 0.6980\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1458 - acc: 0.7555 - val_loss: 0.4150 - val_acc: 0.6955\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1401 - acc: 0.7567 - val_loss: 0.4047 - val_acc: 0.6985\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1402 - acc: 0.7567 - val_loss: 0.3940 - val_acc: 0.6973\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1384 - acc: 0.7571 - val_loss: 0.4108 - val_acc: 0.6971\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1377 - acc: 0.7573 - val_loss: 0.4017 - val_acc: 0.7009\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1367 - acc: 0.7575 - val_loss: 0.4173 - val_acc: 0.6986\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1357 - acc: 0.7579 - val_loss: 0.4241 - val_acc: 0.6992\n",
      "17921/17921 [==============================] - 1s 77us/sample - loss: 0.7619 - acc: 0.6525\n",
      "35843/35843 [==============================] - 3s 76us/sample - loss: 0.1387 - acc: 0.7580\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 562us/sample - loss: 0.3467 - acc: 0.6699 - val_loss: 0.3974 - val_acc: 0.6728\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.2214 - acc: 0.7128 - val_loss: 0.4079 - val_acc: 0.6723\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.2030 - acc: 0.7167 - val_loss: 0.3976 - val_acc: 0.6713\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1953 - acc: 0.7185 - val_loss: 0.3956 - val_acc: 0.6749\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.1852 - acc: 0.7212 - val_loss: 0.4067 - val_acc: 0.6765\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1789 - acc: 0.7227 - val_loss: 0.4084 - val_acc: 0.6784\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1752 - acc: 0.7238 - val_loss: 0.3974 - val_acc: 0.6748\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1710 - acc: 0.7246 - val_loss: 0.4054 - val_acc: 0.6759\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1763 - acc: 0.7238 - val_loss: 0.4438 - val_acc: 0.6717\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1784 - acc: 0.7231 - val_loss: 0.4036 - val_acc: 0.6788\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1679 - acc: 0.7256 - val_loss: 0.4091 - val_acc: 0.6686\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1752 - acc: 0.7240 - val_loss: 0.3933 - val_acc: 0.6811\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1669 - acc: 0.7259 - val_loss: 0.4135 - val_acc: 0.6805\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1681 - acc: 0.7260 - val_loss: 0.3935 - val_acc: 0.6827\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1627 - acc: 0.7269 - val_loss: 0.4022 - val_acc: 0.6804\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1617 - acc: 0.7272 - val_loss: 0.3882 - val_acc: 0.6811\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1611 - acc: 0.7273 - val_loss: 0.4084 - val_acc: 0.6783\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1587 - acc: 0.7278 - val_loss: 0.4049 - val_acc: 0.6776\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1616 - acc: 0.7272 - val_loss: 0.4144 - val_acc: 0.6814\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1579 - acc: 0.7281 - val_loss: 0.4109 - val_acc: 0.6776\n",
      "17921/17921 [==============================] - 1s 77us/sample - loss: 0.5973 - acc: 0.6654\n",
      "35843/35843 [==============================] - 3s 76us/sample - loss: 0.1581 - acc: 0.7279\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 21s 576us/sample - loss: 0.2573 - acc: 0.7159 - val_loss: 0.4150 - val_acc: 0.6760\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1679 - acc: 0.7426 - val_loss: 0.4153 - val_acc: 0.6784\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1548 - acc: 0.7456 - val_loss: 0.4219 - val_acc: 0.6856\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1473 - acc: 0.7472 - val_loss: 0.4278 - val_acc: 0.6866\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1414 - acc: 0.7487 - val_loss: 0.4277 - val_acc: 0.6866\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1392 - acc: 0.7493 - val_loss: 0.4266 - val_acc: 0.6871\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1434 - acc: 0.7486 - val_loss: 0.4411 - val_acc: 0.6905\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1335 - acc: 0.7504 - val_loss: 0.4744 - val_acc: 0.6859\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1340 - acc: 0.7505 - val_loss: 0.4339 - val_acc: 0.6895\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1287 - acc: 0.7513 - val_loss: 0.4388 - val_acc: 0.6898\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1266 - acc: 0.7517 - val_loss: 0.4592 - val_acc: 0.6893\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1255 - acc: 0.7519 - val_loss: 0.4669 - val_acc: 0.6910\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1335 - acc: 0.7506 - val_loss: 0.4416 - val_acc: 0.6834\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 126us/sample - loss: 0.1327 - acc: 0.7505 - val_loss: 0.4200 - val_acc: 0.6886\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1251 - acc: 0.7520 - val_loss: 0.4407 - val_acc: 0.6917\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1226 - acc: 0.7525 - val_loss: 0.4439 - val_acc: 0.6908\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1218 - acc: 0.7526 - val_loss: 0.4493 - val_acc: 0.6913\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1197 - acc: 0.7529 - val_loss: 0.4415 - val_acc: 0.6909\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1188 - acc: 0.7532 - val_loss: 0.4434 - val_acc: 0.6915\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1180 - acc: 0.7534 - val_loss: 0.4330 - val_acc: 0.6912\n",
      "17922/17922 [==============================] - 1s 77us/sample - loss: 0.9496 - acc: 0.6077\n",
      "35842/35842 [==============================] - 3s 78us/sample - loss: 0.1172 - acc: 0.7536\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 564us/sample - loss: 0.2676 - acc: 0.7235 - val_loss: 0.4000 - val_acc: 0.6840\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1783 - acc: 0.7482 - val_loss: 0.3877 - val_acc: 0.6924\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1611 - acc: 0.7518 - val_loss: 0.3760 - val_acc: 0.6931\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1512 - acc: 0.7540 - val_loss: 0.3823 - val_acc: 0.6960\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1464 - acc: 0.7551 - val_loss: 0.3755 - val_acc: 0.7000\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1411 - acc: 0.7564 - val_loss: 0.3736 - val_acc: 0.7009\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1363 - acc: 0.7574 - val_loss: 0.3838 - val_acc: 0.7031\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1340 - acc: 0.7579 - val_loss: 0.3647 - val_acc: 0.7029\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1321 - acc: 0.7583 - val_loss: 0.3554 - val_acc: 0.6998\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.1402 - acc: 0.7569 - val_loss: 0.3894 - val_acc: 0.7018\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.1316 - acc: 0.7586 - val_loss: 0.3773 - val_acc: 0.7036\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1347 - acc: 0.7576 - val_loss: 0.3618 - val_acc: 0.7044\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1285 - acc: 0.7591 - val_loss: 0.3789 - val_acc: 0.7050\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1238 - acc: 0.7600 - val_loss: 0.3825 - val_acc: 0.7053\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1232 - acc: 0.7602 - val_loss: 0.3791 - val_acc: 0.7038\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1256 - acc: 0.7594 - val_loss: 0.3938 - val_acc: 0.7054\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1207 - acc: 0.7606 - val_loss: 0.3761 - val_acc: 0.7052\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1211 - acc: 0.7606 - val_loss: 0.3827 - val_acc: 0.7058\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1185 - acc: 0.7610 - val_loss: 0.3875 - val_acc: 0.7022\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1220 - acc: 0.7604 - val_loss: 0.3981 - val_acc: 0.7067\n",
      "17921/17921 [==============================] - 1s 78us/sample - loss: 0.7854 - acc: 0.6560\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.1177 - acc: 0.7614\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 601us/sample - loss: 0.2879 - acc: 0.6907 - val_loss: 0.3960 - val_acc: 0.6759\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1989 - acc: 0.7179 - val_loss: 0.3995 - val_acc: 0.6764\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1827 - acc: 0.7219 - val_loss: 0.3810 - val_acc: 0.6800\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1755 - acc: 0.7237 - val_loss: 0.3969 - val_acc: 0.6804\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1664 - acc: 0.7259 - val_loss: 0.3831 - val_acc: 0.6814\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1616 - acc: 0.7270 - val_loss: 0.4096 - val_acc: 0.6745\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1608 - acc: 0.7273 - val_loss: 0.3932 - val_acc: 0.6859\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1545 - acc: 0.7287 - val_loss: 0.3932 - val_acc: 0.6806\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1516 - acc: 0.7293 - val_loss: 0.4053 - val_acc: 0.6859\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1482 - acc: 0.7301 - val_loss: 0.3830 - val_acc: 0.6862\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1458 - acc: 0.7306 - val_loss: 0.3826 - val_acc: 0.6879\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1453 - acc: 0.7308 - val_loss: 0.3897 - val_acc: 0.6842\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1417 - acc: 0.7315 - val_loss: 0.3922 - val_acc: 0.6844\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1405 - acc: 0.7318 - val_loss: 0.3978 - val_acc: 0.6841\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1397 - acc: 0.7320 - val_loss: 0.4083 - val_acc: 0.6836\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1421 - acc: 0.7315 - val_loss: 0.3948 - val_acc: 0.6860\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1376 - acc: 0.7324 - val_loss: 0.3777 - val_acc: 0.6853\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1390 - acc: 0.7322 - val_loss: 0.4390 - val_acc: 0.6811\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1354 - acc: 0.7328 - val_loss: 0.3958 - val_acc: 0.6877\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1335 - acc: 0.7333 - val_loss: 0.4290 - val_acc: 0.6832\n",
      "17921/17921 [==============================] - 1s 77us/sample - loss: 0.6957 - acc: 0.6624\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.1337 - acc: 0.7334\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 20s 567us/sample - loss: 0.2867 - acc: 0.7060 - val_loss: 0.4247 - val_acc: 0.6753\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 123us/sample - loss: 0.1778 - acc: 0.7403 - val_loss: 0.4518 - val_acc: 0.6786\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1653 - acc: 0.7429 - val_loss: 0.4247 - val_acc: 0.6829\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1557 - acc: 0.7454 - val_loss: 0.4366 - val_acc: 0.6854\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1487 - acc: 0.7471 - val_loss: 0.4525 - val_acc: 0.6848\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1482 - acc: 0.7472 - val_loss: 0.4304 - val_acc: 0.6788\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1447 - acc: 0.7482 - val_loss: 0.4381 - val_acc: 0.6839\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1410 - acc: 0.7488 - val_loss: 0.4769 - val_acc: 0.6770\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1429 - acc: 0.7485 - val_loss: 0.4276 - val_acc: 0.6870\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1356 - acc: 0.7500 - val_loss: 0.4134 - val_acc: 0.6842\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1382 - acc: 0.7494 - val_loss: 0.4476 - val_acc: 0.6858\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1372 - acc: 0.7499 - val_loss: 0.4356 - val_acc: 0.6882\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1314 - acc: 0.7509 - val_loss: 0.4316 - val_acc: 0.6881\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1322 - acc: 0.7508 - val_loss: 0.4393 - val_acc: 0.6899\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1305 - acc: 0.7512 - val_loss: 0.4241 - val_acc: 0.6867\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1280 - acc: 0.7517 - val_loss: 0.4424 - val_acc: 0.6886\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1270 - acc: 0.7518 - val_loss: 0.4433 - val_acc: 0.6889\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1258 - acc: 0.7520 - val_loss: 0.4609 - val_acc: 0.6878\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1290 - acc: 0.7515 - val_loss: 0.4287 - val_acc: 0.6882\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1250 - acc: 0.7522 - val_loss: 0.4423 - val_acc: 0.6891\n",
      "17922/17922 [==============================] - 1s 77us/sample - loss: 0.9284 - acc: 0.6123\n",
      "35842/35842 [==============================] - 3s 75us/sample - loss: 0.1247 - acc: 0.7524\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 20s 572us/sample - loss: 0.2892 - acc: 0.7161 - val_loss: 0.4157 - val_acc: 0.6784\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1845 - acc: 0.7470 - val_loss: 0.4067 - val_acc: 0.6874\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1681 - acc: 0.7505 - val_loss: 0.3996 - val_acc: 0.6910\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1620 - acc: 0.7519 - val_loss: 0.3887 - val_acc: 0.6952\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1530 - acc: 0.7539 - val_loss: 0.3716 - val_acc: 0.6963\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1467 - acc: 0.7552 - val_loss: 0.3856 - val_acc: 0.6988\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1430 - acc: 0.7561 - val_loss: 0.3786 - val_acc: 0.7015\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1395 - acc: 0.7568 - val_loss: 0.3770 - val_acc: 0.7000\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 122us/sample - loss: 0.1375 - acc: 0.7573 - val_loss: 0.3682 - val_acc: 0.7030\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1351 - acc: 0.7578 - val_loss: 0.4230 - val_acc: 0.6999\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1423 - acc: 0.7564 - val_loss: 0.3705 - val_acc: 0.7033\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1328 - acc: 0.7582 - val_loss: 0.3734 - val_acc: 0.7046\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1301 - acc: 0.7588 - val_loss: 0.3890 - val_acc: 0.7024\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1306 - acc: 0.7588 - val_loss: 0.3698 - val_acc: 0.7037\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1271 - acc: 0.7594 - val_loss: 0.3675 - val_acc: 0.7048\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1261 - acc: 0.7596 - val_loss: 0.3885 - val_acc: 0.7048\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1250 - acc: 0.7599 - val_loss: 0.4036 - val_acc: 0.7041\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1284 - acc: 0.7594 - val_loss: 0.3791 - val_acc: 0.7042\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1247 - acc: 0.7600 - val_loss: 0.3816 - val_acc: 0.7026\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1262 - acc: 0.7596 - val_loss: 0.4095 - val_acc: 0.7018\n",
      "17921/17921 [==============================] - 1s 78us/sample - loss: 0.7368 - acc: 0.6541\n",
      "35843/35843 [==============================] - 3s 76us/sample - loss: 0.1300 - acc: 0.7592\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 21s 582us/sample - loss: 0.3129 - acc: 0.6834 - val_loss: 0.3951 - val_acc: 0.6707\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.2089 - acc: 0.7156 - val_loss: 0.3985 - val_acc: 0.6781\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1924 - acc: 0.7193 - val_loss: 0.3954 - val_acc: 0.6755\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1824 - acc: 0.7218 - val_loss: 0.3947 - val_acc: 0.6808\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1749 - acc: 0.7237 - val_loss: 0.3863 - val_acc: 0.6805\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1686 - acc: 0.7252 - val_loss: 0.3882 - val_acc: 0.6837\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1676 - acc: 0.7257 - val_loss: 0.3861 - val_acc: 0.6830\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1662 - acc: 0.7260 - val_loss: 0.4218 - val_acc: 0.6671\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1766 - acc: 0.7240 - val_loss: 0.3894 - val_acc: 0.6827\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1593 - acc: 0.7278 - val_loss: 0.4083 - val_acc: 0.6808\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1569 - acc: 0.7284 - val_loss: 0.3899 - val_acc: 0.6805\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1542 - acc: 0.7289 - val_loss: 0.3961 - val_acc: 0.6801\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1522 - acc: 0.7294 - val_loss: 0.3888 - val_acc: 0.6839\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1502 - acc: 0.7298 - val_loss: 0.4140 - val_acc: 0.6830\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1545 - acc: 0.7291 - val_loss: 0.4037 - val_acc: 0.6794\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1483 - acc: 0.7303 - val_loss: 0.4038 - val_acc: 0.6847\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1475 - acc: 0.7305 - val_loss: 0.3988 - val_acc: 0.6846\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1456 - acc: 0.7308 - val_loss: 0.4074 - val_acc: 0.6816\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1462 - acc: 0.7308 - val_loss: 0.3931 - val_acc: 0.6852\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1434 - acc: 0.7313 - val_loss: 0.3922 - val_acc: 0.6835\n",
      "17921/17921 [==============================] - 1s 75us/sample - loss: 0.6032 - acc: 0.6674\n",
      "35843/35843 [==============================] - 3s 77us/sample - loss: 0.1416 - acc: 0.7318\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 21s 588us/sample - loss: 0.3115 - acc: 0.6990 - val_loss: 0.4583 - val_acc: 0.6760\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1942 - acc: 0.7372 - val_loss: 0.4271 - val_acc: 0.6724\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1753 - acc: 0.7409 - val_loss: 0.4181 - val_acc: 0.6760\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1685 - acc: 0.7423 - val_loss: 0.4345 - val_acc: 0.6824\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1693 - acc: 0.7421 - val_loss: 0.4406 - val_acc: 0.6795\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1605 - acc: 0.7442 - val_loss: 0.4208 - val_acc: 0.6804\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1569 - acc: 0.7450 - val_loss: 0.4389 - val_acc: 0.6824\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1535 - acc: 0.7459 - val_loss: 0.4430 - val_acc: 0.6801\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1514 - acc: 0.7465 - val_loss: 0.4409 - val_acc: 0.6774\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1503 - acc: 0.7468 - val_loss: 0.4409 - val_acc: 0.6780\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1488 - acc: 0.7471 - val_loss: 0.4380 - val_acc: 0.6836\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1461 - acc: 0.7479 - val_loss: 0.4545 - val_acc: 0.6852\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1512 - acc: 0.7469 - val_loss: 0.4321 - val_acc: 0.6851\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1536 - acc: 0.7458 - val_loss: 0.4432 - val_acc: 0.6850\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 127us/sample - loss: 0.1464 - acc: 0.7479 - val_loss: 0.4402 - val_acc: 0.6842\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 128us/sample - loss: 0.1427 - acc: 0.7488 - val_loss: 0.4396 - val_acc: 0.6862\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 4s 125us/sample - loss: 0.1414 - acc: 0.7491 - val_loss: 0.4509 - val_acc: 0.6870\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1404 - acc: 0.7494 - val_loss: 0.4319 - val_acc: 0.6866\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 126us/sample - loss: 0.1370 - acc: 0.7500 - val_loss: 0.4554 - val_acc: 0.6857\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 4s 124us/sample - loss: 0.1357 - acc: 0.7503 - val_loss: 0.4572 - val_acc: 0.6771\n",
      "17922/17922 [==============================] - 1s 77us/sample - loss: 0.9426 - acc: 0.5935\n",
      "35842/35842 [==============================] - 3s 75us/sample - loss: 0.1431 - acc: 0.7481\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 21s 587us/sample - loss: 0.3276 - acc: 0.7028 - val_loss: 0.4197 - val_acc: 0.6761\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.2028 - acc: 0.7433 - val_loss: 0.4171 - val_acc: 0.6854\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1773 - acc: 0.7486 - val_loss: 0.3974 - val_acc: 0.6892\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1672 - acc: 0.7509 - val_loss: 0.3754 - val_acc: 0.6951\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1610 - acc: 0.7522 - val_loss: 0.3818 - val_acc: 0.6946\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1579 - acc: 0.7529 - val_loss: 0.3920 - val_acc: 0.6911\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1609 - acc: 0.7524 - val_loss: 0.3844 - val_acc: 0.6985\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1573 - acc: 0.7530 - val_loss: 0.3972 - val_acc: 0.6939\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1524 - acc: 0.7539 - val_loss: 0.3834 - val_acc: 0.6959\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1477 - acc: 0.7549 - val_loss: 0.3909 - val_acc: 0.6968\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1450 - acc: 0.7556 - val_loss: 0.3874 - val_acc: 0.6973\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1443 - acc: 0.7558 - val_loss: 0.3727 - val_acc: 0.7003\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 123us/sample - loss: 0.1418 - acc: 0.7563 - val_loss: 0.3886 - val_acc: 0.7000\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1408 - acc: 0.7565 - val_loss: 0.3759 - val_acc: 0.7009\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1399 - acc: 0.7568 - val_loss: 0.3912 - val_acc: 0.6950\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1481 - acc: 0.7550 - val_loss: 0.3910 - val_acc: 0.6984\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1398 - acc: 0.7567 - val_loss: 0.4072 - val_acc: 0.6979\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1474 - acc: 0.7550 - val_loss: 0.4381 - val_acc: 0.6897\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1475 - acc: 0.7550 - val_loss: 0.4128 - val_acc: 0.6970\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1427 - acc: 0.7561 - val_loss: 0.4018 - val_acc: 0.6981\n",
      "17921/17921 [==============================] - 1s 76us/sample - loss: 0.7100 - acc: 0.6501\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1402 - acc: 0.7568\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 600us/sample - loss: 0.3626 - acc: 0.6644 - val_loss: 0.4251 - val_acc: 0.6628\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.2161 - acc: 0.7142 - val_loss: 0.3933 - val_acc: 0.6780\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1960 - acc: 0.7187 - val_loss: 0.3830 - val_acc: 0.6744\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 129us/sample - loss: 0.1858 - acc: 0.7212 - val_loss: 0.3917 - val_acc: 0.6742\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.1801 - acc: 0.7226 - val_loss: 0.3790 - val_acc: 0.6770\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1819 - acc: 0.7221 - val_loss: 0.3898 - val_acc: 0.6776\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 128us/sample - loss: 0.1730 - acc: 0.7242 - val_loss: 0.3830 - val_acc: 0.6785\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1704 - acc: 0.7248 - val_loss: 0.3889 - val_acc: 0.6767\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 127us/sample - loss: 0.1695 - acc: 0.7251 - val_loss: 0.3936 - val_acc: 0.6787\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 130us/sample - loss: 0.1670 - acc: 0.7257 - val_loss: 0.4090 - val_acc: 0.6738\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1760 - acc: 0.7232 - val_loss: 0.4000 - val_acc: 0.6770\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1689 - acc: 0.7253 - val_loss: 0.4063 - val_acc: 0.6786\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1664 - acc: 0.7261 - val_loss: 0.4114 - val_acc: 0.6755\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1645 - acc: 0.7265 - val_loss: 0.3943 - val_acc: 0.6841\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 126us/sample - loss: 0.1630 - acc: 0.7268 - val_loss: 0.3940 - val_acc: 0.6775\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 4s 126us/sample - loss: 0.1720 - acc: 0.7249 - val_loss: 0.4865 - val_acc: 0.6821\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1689 - acc: 0.7263 - val_loss: 0.4001 - val_acc: 0.6795\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1660 - acc: 0.7266 - val_loss: 0.4054 - val_acc: 0.6786\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 4s 124us/sample - loss: 0.1694 - acc: 0.7254 - val_loss: 0.4006 - val_acc: 0.6778\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 4s 125us/sample - loss: 0.1630 - acc: 0.7270 - val_loss: 0.4003 - val_acc: 0.6808\n",
      "17921/17921 [==============================] - 1s 76us/sample - loss: 0.5841 - acc: 0.6699\n",
      "35843/35843 [==============================] - 3s 75us/sample - loss: 0.1618 - acc: 0.7277\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 21s 582us/sample - loss: 0.2745 - acc: 0.7124 - val_loss: 0.4511 - val_acc: 0.6632\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2295 - acc: 0.7288 - val_loss: 0.4554 - val_acc: 0.6646\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2277 - acc: 0.7290 - val_loss: 0.4502 - val_acc: 0.6660\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 141us/sample - loss: 0.2261 - acc: 0.7294 - val_loss: 0.4569 - val_acc: 0.6592\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2272 - acc: 0.7290 - val_loss: 0.4860 - val_acc: 0.6689\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2256 - acc: 0.7295 - val_loss: 0.4581 - val_acc: 0.6641\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2244 - acc: 0.7297 - val_loss: 0.4612 - val_acc: 0.6676\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2230 - acc: 0.7300 - val_loss: 0.4580 - val_acc: 0.6653\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2219 - acc: 0.7303 - val_loss: 0.4682 - val_acc: 0.6727\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 139us/sample - loss: 0.2238 - acc: 0.7298 - val_loss: 0.4654 - val_acc: 0.6722\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2240 - acc: 0.7298 - val_loss: 0.4769 - val_acc: 0.6688\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 138us/sample - loss: 0.2229 - acc: 0.7300 - val_loss: 0.4578 - val_acc: 0.6705\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2220 - acc: 0.7302 - val_loss: 0.4670 - val_acc: 0.6687\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2222 - acc: 0.7301 - val_loss: 0.4711 - val_acc: 0.6673\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2212 - acc: 0.7303 - val_loss: 0.4600 - val_acc: 0.6672\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2217 - acc: 0.7302 - val_loss: 0.4661 - val_acc: 0.6647\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 139us/sample - loss: 0.2223 - acc: 0.7301 - val_loss: 0.4633 - val_acc: 0.6701\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 141us/sample - loss: 0.2203 - acc: 0.7305 - val_loss: 0.4665 - val_acc: 0.6655\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2224 - acc: 0.7300 - val_loss: 0.4637 - val_acc: 0.6695\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2209 - acc: 0.7304 - val_loss: 0.4691 - val_acc: 0.6629\n",
      "17922/17922 [==============================] - 1s 80us/sample - loss: 0.7733 - acc: 0.6140\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.2219 - acc: 0.7289\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 611us/sample - loss: 0.2776 - acc: 0.7224 - val_loss: 0.4469 - val_acc: 0.6783\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2340 - acc: 0.7368 - val_loss: 0.4208 - val_acc: 0.6788\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2301 - acc: 0.7377 - val_loss: 0.4320 - val_acc: 0.6573\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2336 - acc: 0.7364 - val_loss: 0.4270 - val_acc: 0.6604\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2312 - acc: 0.7372 - val_loss: 0.4282 - val_acc: 0.6764\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2270 - acc: 0.7381 - val_loss: 0.4250 - val_acc: 0.6718\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2277 - acc: 0.7380 - val_loss: 0.4350 - val_acc: 0.6782\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2260 - acc: 0.7383 - val_loss: 0.4464 - val_acc: 0.6744\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2297 - acc: 0.7374 - val_loss: 0.4350 - val_acc: 0.6758\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2276 - acc: 0.7380 - val_loss: 0.4299 - val_acc: 0.6767\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2262 - acc: 0.7383 - val_loss: 0.4502 - val_acc: 0.6791\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2271 - acc: 0.7381 - val_loss: 0.4348 - val_acc: 0.6708\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2251 - acc: 0.7384 - val_loss: 0.4280 - val_acc: 0.6746\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2246 - acc: 0.7386 - val_loss: 0.4337 - val_acc: 0.6747\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2284 - acc: 0.7376 - val_loss: 0.4282 - val_acc: 0.6726\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2257 - acc: 0.7384 - val_loss: 0.4433 - val_acc: 0.6694\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2267 - acc: 0.7380 - val_loss: 0.4297 - val_acc: 0.6736\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2254 - acc: 0.7383 - val_loss: 0.4196 - val_acc: 0.6765\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 140us/sample - loss: 0.2254 - acc: 0.7383 - val_loss: 0.4336 - val_acc: 0.6761\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2252 - acc: 0.7384 - val_loss: 0.4396 - val_acc: 0.6743\n",
      "17921/17921 [==============================] - 1s 81us/sample - loss: 0.6549 - acc: 0.6302\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.2233 - acc: 0.7383\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 601us/sample - loss: 0.3100 - acc: 0.6868 - val_loss: 0.4251 - val_acc: 0.6646\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 139us/sample - loss: 0.2656 - acc: 0.7027 - val_loss: 0.4149 - val_acc: 0.6666\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 140us/sample - loss: 0.2623 - acc: 0.7034 - val_loss: 0.4479 - val_acc: 0.6481\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2636 - acc: 0.7029 - val_loss: 0.4278 - val_acc: 0.6678\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2594 - acc: 0.7040 - val_loss: 0.4500 - val_acc: 0.6625\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2613 - acc: 0.7035 - val_loss: 0.4188 - val_acc: 0.6705\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2596 - acc: 0.7038 - val_loss: 0.4236 - val_acc: 0.6686\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2590 - acc: 0.7040 - val_loss: 0.4405 - val_acc: 0.6611\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2599 - acc: 0.7037 - val_loss: 0.4449 - val_acc: 0.6641\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2600 - acc: 0.7037 - val_loss: 0.4333 - val_acc: 0.6617\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 142us/sample - loss: 0.2576 - acc: 0.7043 - val_loss: 0.4482 - val_acc: 0.6580\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2596 - acc: 0.7037 - val_loss: 0.4242 - val_acc: 0.6625\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2567 - acc: 0.7046 - val_loss: 0.4256 - val_acc: 0.6697\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 145us/sample - loss: 0.2566 - acc: 0.7046 - val_loss: 0.4346 - val_acc: 0.6664\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2572 - acc: 0.7044 - val_loss: 0.4250 - val_acc: 0.6651\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2559 - acc: 0.7048 - val_loss: 0.4345 - val_acc: 0.6639\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2583 - acc: 0.7042 - val_loss: 0.4187 - val_acc: 0.6692\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2572 - acc: 0.7045 - val_loss: 0.4213 - val_acc: 0.6633\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2573 - acc: 0.7044 - val_loss: 0.4222 - val_acc: 0.6693\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2567 - acc: 0.7046 - val_loss: 0.4218 - val_acc: 0.6650\n",
      "17921/17921 [==============================] - 1s 80us/sample - loss: 0.5144 - acc: 0.6807\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.2565 - acc: 0.7044\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 21s 595us/sample - loss: 0.2904 - acc: 0.7062 - val_loss: 0.4627 - val_acc: 0.6547\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2326 - acc: 0.7280 - val_loss: 0.4654 - val_acc: 0.6638\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2269 - acc: 0.7293 - val_loss: 0.4448 - val_acc: 0.6593\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2305 - acc: 0.7283 - val_loss: 0.4513 - val_acc: 0.6622\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2282 - acc: 0.7290 - val_loss: 0.4586 - val_acc: 0.6608\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2254 - acc: 0.7295 - val_loss: 0.4422 - val_acc: 0.6667\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2254 - acc: 0.7295 - val_loss: 0.4518 - val_acc: 0.6662\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2227 - acc: 0.7302 - val_loss: 0.4580 - val_acc: 0.6652\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2245 - acc: 0.7297 - val_loss: 0.4705 - val_acc: 0.6709\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2269 - acc: 0.7291 - val_loss: 0.4632 - val_acc: 0.6679\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2217 - acc: 0.7303 - val_loss: 0.4598 - val_acc: 0.6632\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2236 - acc: 0.7298 - val_loss: 0.4616 - val_acc: 0.6698\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2214 - acc: 0.7304 - val_loss: 0.4610 - val_acc: 0.6698\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2217 - acc: 0.7304 - val_loss: 0.4636 - val_acc: 0.6630\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2249 - acc: 0.7295 - val_loss: 0.4591 - val_acc: 0.6706\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2209 - acc: 0.7305 - val_loss: 0.4598 - val_acc: 0.6677\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2213 - acc: 0.7304 - val_loss: 0.4608 - val_acc: 0.6561\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2257 - acc: 0.7291 - val_loss: 0.4636 - val_acc: 0.6641\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2234 - acc: 0.7298 - val_loss: 0.4610 - val_acc: 0.6665\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2201 - acc: 0.7307 - val_loss: 0.4654 - val_acc: 0.6628\n",
      "17922/17922 [==============================] - 1s 81us/sample - loss: 0.7499 - acc: 0.6164\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.2294 - acc: 0.7268\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 21s 598us/sample - loss: 0.2926 - acc: 0.7171 - val_loss: 0.4541 - val_acc: 0.6685\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2352 - acc: 0.7366 - val_loss: 0.4383 - val_acc: 0.6743\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2308 - acc: 0.7378 - val_loss: 0.4333 - val_acc: 0.6750\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2288 - acc: 0.7379 - val_loss: 0.4331 - val_acc: 0.6765\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2281 - acc: 0.7382 - val_loss: 0.4293 - val_acc: 0.6715\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2289 - acc: 0.7379 - val_loss: 0.4515 - val_acc: 0.6736\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2269 - acc: 0.7383 - val_loss: 0.4267 - val_acc: 0.6761\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2291 - acc: 0.7378 - val_loss: 0.4423 - val_acc: 0.6761\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2258 - acc: 0.7385 - val_loss: 0.4417 - val_acc: 0.6789\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2315 - acc: 0.7374 - val_loss: 0.4301 - val_acc: 0.6746\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2262 - acc: 0.7383 - val_loss: 0.4226 - val_acc: 0.6744\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2291 - acc: 0.7377 - val_loss: 0.4243 - val_acc: 0.6755\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2254 - acc: 0.7385 - val_loss: 0.4362 - val_acc: 0.6771\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2273 - acc: 0.7381 - val_loss: 0.4335 - val_acc: 0.6765\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2253 - acc: 0.7385 - val_loss: 0.4475 - val_acc: 0.6670\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2293 - acc: 0.7376 - val_loss: 0.4353 - val_acc: 0.6767\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2246 - acc: 0.7386 - val_loss: 0.4386 - val_acc: 0.6771\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2238 - acc: 0.7388 - val_loss: 0.4415 - val_acc: 0.6778\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2240 - acc: 0.7388 - val_loss: 0.4254 - val_acc: 0.6763\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2242 - acc: 0.7386 - val_loss: 0.4318 - val_acc: 0.6786\n",
      "17921/17921 [==============================] - 1s 80us/sample - loss: 0.6429 - acc: 0.6346\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.2250 - acc: 0.7398\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 608us/sample - loss: 0.3224 - acc: 0.6820 - val_loss: 0.4251 - val_acc: 0.6721\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2657 - acc: 0.7028 - val_loss: 0.4268 - val_acc: 0.6594\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2628 - acc: 0.7035 - val_loss: 0.4466 - val_acc: 0.6565\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2648 - acc: 0.7028 - val_loss: 0.4349 - val_acc: 0.6579\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2614 - acc: 0.7036 - val_loss: 0.4240 - val_acc: 0.6714\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2604 - acc: 0.7040 - val_loss: 0.4406 - val_acc: 0.6486\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2631 - acc: 0.7031 - val_loss: 0.4253 - val_acc: 0.6613\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2600 - acc: 0.7040 - val_loss: 0.4394 - val_acc: 0.6514\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2609 - acc: 0.7035 - val_loss: 0.4158 - val_acc: 0.6713\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2583 - acc: 0.7044 - val_loss: 0.4245 - val_acc: 0.6651\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2567 - acc: 0.7048 - val_loss: 0.4174 - val_acc: 0.6694\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2586 - acc: 0.7042 - val_loss: 0.4287 - val_acc: 0.6653\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2571 - acc: 0.7046 - val_loss: 0.4227 - val_acc: 0.6690\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2567 - acc: 0.7047 - val_loss: 0.4244 - val_acc: 0.6687\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2571 - acc: 0.7046 - val_loss: 0.4241 - val_acc: 0.6635\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2576 - acc: 0.7044 - val_loss: 0.4177 - val_acc: 0.6697\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2565 - acc: 0.7047 - val_loss: 0.4352 - val_acc: 0.6637\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2559 - acc: 0.7049 - val_loss: 0.4339 - val_acc: 0.6684\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2561 - acc: 0.7048 - val_loss: 0.4314 - val_acc: 0.6606\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2566 - acc: 0.7046 - val_loss: 0.4206 - val_acc: 0.6699\n",
      "17921/17921 [==============================] - 1s 80us/sample - loss: 0.5128 - acc: 0.6869\n",
      "35843/35843 [==============================] - 3s 83us/sample - loss: 0.2562 - acc: 0.7056\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 22s 605us/sample - loss: 0.3123 - acc: 0.6986 - val_loss: 0.4434 - val_acc: 0.6679\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2291 - acc: 0.7290 - val_loss: 0.4689 - val_acc: 0.6707\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 138us/sample - loss: 0.2270 - acc: 0.7295 - val_loss: 0.4651 - val_acc: 0.6714\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2277 - acc: 0.7290 - val_loss: 0.4734 - val_acc: 0.6523\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2303 - acc: 0.7283 - val_loss: 0.4509 - val_acc: 0.6689\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 139us/sample - loss: 0.2239 - acc: 0.7301 - val_loss: 0.4587 - val_acc: 0.6650\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2247 - acc: 0.7299 - val_loss: 0.4825 - val_acc: 0.6729\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2258 - acc: 0.7296 - val_loss: 0.4731 - val_acc: 0.6689\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2278 - acc: 0.7292 - val_loss: 0.4526 - val_acc: 0.6701\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2239 - acc: 0.7299 - val_loss: 0.4531 - val_acc: 0.6654\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2223 - acc: 0.7303 - val_loss: 0.4591 - val_acc: 0.6676\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2222 - acc: 0.7304 - val_loss: 0.4554 - val_acc: 0.6596\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2290 - acc: 0.7285 - val_loss: 0.4647 - val_acc: 0.6653\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2227 - acc: 0.7302 - val_loss: 0.4585 - val_acc: 0.6690\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2224 - acc: 0.7303 - val_loss: 0.4689 - val_acc: 0.6676\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2226 - acc: 0.7302 - val_loss: 0.4722 - val_acc: 0.6655\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2216 - acc: 0.7304 - val_loss: 0.4741 - val_acc: 0.6730\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2280 - acc: 0.7288 - val_loss: 0.4623 - val_acc: 0.6688\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2222 - acc: 0.7303 - val_loss: 0.4826 - val_acc: 0.6681\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2256 - acc: 0.7292 - val_loss: 0.4603 - val_acc: 0.6686\n",
      "17922/17922 [==============================] - 1s 79us/sample - loss: 0.7512 - acc: 0.6241\n",
      "35842/35842 [==============================] - 3s 80us/sample - loss: 0.2210 - acc: 0.7301\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 21s 599us/sample - loss: 0.3112 - acc: 0.7101 - val_loss: 0.4516 - val_acc: 0.6730\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2349 - acc: 0.7371 - val_loss: 0.4300 - val_acc: 0.6683\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2327 - acc: 0.7371 - val_loss: 0.4416 - val_acc: 0.6755\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2299 - acc: 0.7379 - val_loss: 0.4227 - val_acc: 0.6675\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2306 - acc: 0.7375 - val_loss: 0.4350 - val_acc: 0.6705\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2316 - acc: 0.7371 - val_loss: 0.4339 - val_acc: 0.6744\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2280 - acc: 0.7383 - val_loss: 0.4404 - val_acc: 0.6688\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2280 - acc: 0.7381 - val_loss: 0.4279 - val_acc: 0.6768\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2275 - acc: 0.7382 - val_loss: 0.4307 - val_acc: 0.6684\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2270 - acc: 0.7383 - val_loss: 0.4284 - val_acc: 0.6762\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2290 - acc: 0.7379 - val_loss: 0.4338 - val_acc: 0.6771\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2259 - acc: 0.7386 - val_loss: 0.4346 - val_acc: 0.6785\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2276 - acc: 0.7382 - val_loss: 0.4320 - val_acc: 0.6740\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2255 - acc: 0.7386 - val_loss: 0.4357 - val_acc: 0.6769\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2255 - acc: 0.7386 - val_loss: 0.4370 - val_acc: 0.6760\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2243 - acc: 0.7389 - val_loss: 0.4363 - val_acc: 0.6728\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2244 - acc: 0.7388 - val_loss: 0.4326 - val_acc: 0.6743\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2249 - acc: 0.7388 - val_loss: 0.4504 - val_acc: 0.6792\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2257 - acc: 0.7385 - val_loss: 0.4357 - val_acc: 0.6766\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2240 - acc: 0.7389 - val_loss: 0.4267 - val_acc: 0.6774\n",
      "17921/17921 [==============================] - 1s 81us/sample - loss: 0.6237 - acc: 0.6319\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.2233 - acc: 0.7394\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 602us/sample - loss: 0.3457 - acc: 0.6722 - val_loss: 0.4293 - val_acc: 0.6695\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2650 - acc: 0.7031 - val_loss: 0.4187 - val_acc: 0.6609\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2644 - acc: 0.7031 - val_loss: 0.4307 - val_acc: 0.6722\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2623 - acc: 0.7036 - val_loss: 0.4205 - val_acc: 0.6621\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2626 - acc: 0.7035 - val_loss: 0.4169 - val_acc: 0.6661\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2601 - acc: 0.7042 - val_loss: 0.4217 - val_acc: 0.6596\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2609 - acc: 0.7039 - val_loss: 0.4207 - val_acc: 0.6704\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2585 - acc: 0.7045 - val_loss: 0.4259 - val_acc: 0.6654\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2592 - acc: 0.7044 - val_loss: 0.4304 - val_acc: 0.6569\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2597 - acc: 0.7042 - val_loss: 0.4345 - val_acc: 0.6683\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2585 - acc: 0.7045 - val_loss: 0.4227 - val_acc: 0.6644\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2574 - acc: 0.7047 - val_loss: 0.4223 - val_acc: 0.6632\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2576 - acc: 0.7047 - val_loss: 0.4344 - val_acc: 0.6582\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2609 - acc: 0.7037 - val_loss: 0.4331 - val_acc: 0.6646\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2569 - acc: 0.7048 - val_loss: 0.4305 - val_acc: 0.6727\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2587 - acc: 0.7045 - val_loss: 0.4300 - val_acc: 0.6601\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2575 - acc: 0.7045 - val_loss: 0.4321 - val_acc: 0.6580\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2573 - acc: 0.7046 - val_loss: 0.4256 - val_acc: 0.6681\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2558 - acc: 0.7050 - val_loss: 0.4265 - val_acc: 0.6654\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2570 - acc: 0.7046 - val_loss: 0.4344 - val_acc: 0.6656\n",
      "17921/17921 [==============================] - 1s 82us/sample - loss: 0.5304 - acc: 0.6841\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.2576 - acc: 0.7048\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 22s 623us/sample - loss: 0.2767 - acc: 0.7118 - val_loss: 0.4655 - val_acc: 0.6681\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2293 - acc: 0.7288 - val_loss: 0.4580 - val_acc: 0.6679\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2260 - acc: 0.7295 - val_loss: 0.4559 - val_acc: 0.6633\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2259 - acc: 0.7294 - val_loss: 0.4871 - val_acc: 0.6694\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2259 - acc: 0.7294 - val_loss: 0.4604 - val_acc: 0.6666\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2232 - acc: 0.7299 - val_loss: 0.4592 - val_acc: 0.6681\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2248 - acc: 0.7295 - val_loss: 0.4711 - val_acc: 0.6719\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2239 - acc: 0.7298 - val_loss: 0.4656 - val_acc: 0.6676\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2221 - acc: 0.7302 - val_loss: 0.4517 - val_acc: 0.6585\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2239 - acc: 0.7297 - val_loss: 0.4665 - val_acc: 0.6649\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2250 - acc: 0.7292 - val_loss: 0.4670 - val_acc: 0.6590\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2260 - acc: 0.7290 - val_loss: 0.4731 - val_acc: 0.6690\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2219 - acc: 0.7302 - val_loss: 0.4585 - val_acc: 0.6680\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2208 - acc: 0.7305 - val_loss: 0.4900 - val_acc: 0.6715\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2237 - acc: 0.7298 - val_loss: 0.4712 - val_acc: 0.6613\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2244 - acc: 0.7293 - val_loss: 0.4651 - val_acc: 0.6648\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2252 - acc: 0.7293 - val_loss: 0.4576 - val_acc: 0.6629\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2226 - acc: 0.7301 - val_loss: 0.4641 - val_acc: 0.6643\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2234 - acc: 0.7299 - val_loss: 0.4618 - val_acc: 0.6695\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2234 - acc: 0.7298 - val_loss: 0.4666 - val_acc: 0.6671\n",
      "17922/17922 [==============================] - 1s 80us/sample - loss: 0.7683 - acc: 0.6207\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.2217 - acc: 0.7304\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 621us/sample - loss: 0.2779 - acc: 0.7221 - val_loss: 0.4429 - val_acc: 0.6730\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2332 - acc: 0.7372 - val_loss: 0.4288 - val_acc: 0.6683\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2309 - acc: 0.7373 - val_loss: 0.4910 - val_acc: 0.6788\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2338 - acc: 0.7369 - val_loss: 0.4488 - val_acc: 0.6737\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2281 - acc: 0.7381 - val_loss: 0.4192 - val_acc: 0.6761\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2266 - acc: 0.7383 - val_loss: 0.4458 - val_acc: 0.6771\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2264 - acc: 0.7382 - val_loss: 0.4495 - val_acc: 0.6667\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2311 - acc: 0.7371 - val_loss: 0.4256 - val_acc: 0.6784\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2261 - acc: 0.7382 - val_loss: 0.4272 - val_acc: 0.6744\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2265 - acc: 0.7382 - val_loss: 0.4239 - val_acc: 0.6722\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2268 - acc: 0.7381 - val_loss: 0.4329 - val_acc: 0.6775\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2253 - acc: 0.7384 - val_loss: 0.4539 - val_acc: 0.6743\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2251 - acc: 0.7384 - val_loss: 0.4327 - val_acc: 0.6754\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2261 - acc: 0.7384 - val_loss: 0.4293 - val_acc: 0.6766\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2251 - acc: 0.7384 - val_loss: 0.4316 - val_acc: 0.6761\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2250 - acc: 0.7384 - val_loss: 0.4398 - val_acc: 0.6776\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2242 - acc: 0.7386 - val_loss: 0.4321 - val_acc: 0.6749\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2240 - acc: 0.7387 - val_loss: 0.4392 - val_acc: 0.6766\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2243 - acc: 0.7386 - val_loss: 0.4479 - val_acc: 0.6712\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2253 - acc: 0.7383 - val_loss: 0.4401 - val_acc: 0.6746\n",
      "17921/17921 [==============================] - 1s 82us/sample - loss: 0.6470 - acc: 0.6306\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.2247 - acc: 0.7385\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 631us/sample - loss: 0.3097 - acc: 0.6868 - val_loss: 0.4417 - val_acc: 0.6652\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2653 - acc: 0.7028 - val_loss: 0.4429 - val_acc: 0.6466\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2645 - acc: 0.7028 - val_loss: 0.4380 - val_acc: 0.6643\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2605 - acc: 0.7038 - val_loss: 0.4333 - val_acc: 0.6610\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2608 - acc: 0.7036 - val_loss: 0.4390 - val_acc: 0.6586\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2608 - acc: 0.7036 - val_loss: 0.4367 - val_acc: 0.6522\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2619 - acc: 0.7033 - val_loss: 0.4289 - val_acc: 0.6662\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2597 - acc: 0.7040 - val_loss: 0.4228 - val_acc: 0.6666\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2591 - acc: 0.7040 - val_loss: 0.4354 - val_acc: 0.6580\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2580 - acc: 0.7043 - val_loss: 0.4416 - val_acc: 0.6685\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2574 - acc: 0.7045 - val_loss: 0.4273 - val_acc: 0.6622\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2573 - acc: 0.7044 - val_loss: 0.4296 - val_acc: 0.6670\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2567 - acc: 0.7045 - val_loss: 0.4272 - val_acc: 0.6635\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2590 - acc: 0.7040 - val_loss: 0.4285 - val_acc: 0.6666\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2568 - acc: 0.7046 - val_loss: 0.4238 - val_acc: 0.6712\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2575 - acc: 0.7044 - val_loss: 0.4186 - val_acc: 0.6680\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2564 - acc: 0.7046 - val_loss: 0.4235 - val_acc: 0.6658\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2555 - acc: 0.7048 - val_loss: 0.4231 - val_acc: 0.6638\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2575 - acc: 0.7043 - val_loss: 0.4323 - val_acc: 0.6614\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2564 - acc: 0.7046 - val_loss: 0.4331 - val_acc: 0.6662\n",
      "17921/17921 [==============================] - 1s 82us/sample - loss: 0.5320 - acc: 0.6833\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.2553 - acc: 0.7049\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 22s 624us/sample - loss: 0.2923 - acc: 0.7061 - val_loss: 0.4496 - val_acc: 0.6631\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2292 - acc: 0.7289 - val_loss: 0.4548 - val_acc: 0.6573\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2283 - acc: 0.7291 - val_loss: 0.4428 - val_acc: 0.6603\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 141us/sample - loss: 0.2275 - acc: 0.7290 - val_loss: 0.4527 - val_acc: 0.6679\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2247 - acc: 0.7298 - val_loss: 0.4625 - val_acc: 0.6668\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2240 - acc: 0.7298 - val_loss: 0.4588 - val_acc: 0.6659\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2236 - acc: 0.7299 - val_loss: 0.4543 - val_acc: 0.6681\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2227 - acc: 0.7302 - val_loss: 0.4603 - val_acc: 0.6675\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2281 - acc: 0.7286 - val_loss: 0.4678 - val_acc: 0.6732\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2274 - acc: 0.7289 - val_loss: 0.4578 - val_acc: 0.6691\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2221 - acc: 0.7303 - val_loss: 0.4580 - val_acc: 0.6669\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2227 - acc: 0.7302 - val_loss: 0.4606 - val_acc: 0.6718\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2219 - acc: 0.7303 - val_loss: 0.4621 - val_acc: 0.6612\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2268 - acc: 0.7289 - val_loss: 0.4584 - val_acc: 0.6693\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 138us/sample - loss: 0.2213 - acc: 0.7304 - val_loss: 0.4676 - val_acc: 0.6628\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2227 - acc: 0.7299 - val_loss: 0.4524 - val_acc: 0.6688\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2225 - acc: 0.7301 - val_loss: 0.4563 - val_acc: 0.6632\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2221 - acc: 0.7301 - val_loss: 0.4712 - val_acc: 0.6701\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2212 - acc: 0.7305 - val_loss: 0.4695 - val_acc: 0.6648\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2237 - acc: 0.7295 - val_loss: 0.4711 - val_acc: 0.6682\n",
      "17922/17922 [==============================] - 1s 80us/sample - loss: 0.7869 - acc: 0.6189\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.2233 - acc: 0.7307\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 631us/sample - loss: 0.2934 - acc: 0.7166 - val_loss: 0.4629 - val_acc: 0.6682\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2356 - acc: 0.7366 - val_loss: 0.4360 - val_acc: 0.6732\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2307 - acc: 0.7377 - val_loss: 0.4208 - val_acc: 0.6765\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2283 - acc: 0.7382 - val_loss: 0.4407 - val_acc: 0.6755\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2284 - acc: 0.7381 - val_loss: 0.4321 - val_acc: 0.6755\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2275 - acc: 0.7382 - val_loss: 0.4290 - val_acc: 0.6755\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2264 - acc: 0.7384 - val_loss: 0.4248 - val_acc: 0.6788\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2272 - acc: 0.7383 - val_loss: 0.4299 - val_acc: 0.6772\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2261 - acc: 0.7384 - val_loss: 0.4313 - val_acc: 0.6692\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2259 - acc: 0.7384 - val_loss: 0.4475 - val_acc: 0.6792\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2274 - acc: 0.7381 - val_loss: 0.4179 - val_acc: 0.6731\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2296 - acc: 0.7374 - val_loss: 0.4277 - val_acc: 0.6712\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2274 - acc: 0.7379 - val_loss: 0.4406 - val_acc: 0.6776\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2251 - acc: 0.7386 - val_loss: 0.4282 - val_acc: 0.6676\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2282 - acc: 0.7379 - val_loss: 0.4317 - val_acc: 0.6760\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2247 - acc: 0.7387 - val_loss: 0.4330 - val_acc: 0.6736\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2241 - acc: 0.7387 - val_loss: 0.4311 - val_acc: 0.6757\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2241 - acc: 0.7388 - val_loss: 0.4383 - val_acc: 0.6783\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2239 - acc: 0.7388 - val_loss: 0.4405 - val_acc: 0.6780\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 139us/sample - loss: 0.2240 - acc: 0.7387 - val_loss: 0.4434 - val_acc: 0.6790\n",
      "17921/17921 [==============================] - 2s 85us/sample - loss: 0.6662 - acc: 0.6372\n",
      "35843/35843 [==============================] - 3s 84us/sample - loss: 0.2278 - acc: 0.7396\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 22s 626us/sample - loss: 0.3232 - acc: 0.6826 - val_loss: 0.4272 - val_acc: 0.6587\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2655 - acc: 0.7029 - val_loss: 0.4371 - val_acc: 0.6425\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2675 - acc: 0.7019 - val_loss: 0.4205 - val_acc: 0.6664\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2604 - acc: 0.7041 - val_loss: 0.4185 - val_acc: 0.6662\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2601 - acc: 0.7041 - val_loss: 0.4255 - val_acc: 0.6690\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2594 - acc: 0.7043 - val_loss: 0.4327 - val_acc: 0.6581\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2588 - acc: 0.7043 - val_loss: 0.4264 - val_acc: 0.6644\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2580 - acc: 0.7044 - val_loss: 0.4534 - val_acc: 0.6584\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2612 - acc: 0.7035 - val_loss: 0.4260 - val_acc: 0.6614\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2586 - acc: 0.7042 - val_loss: 0.4247 - val_acc: 0.6658\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2571 - acc: 0.7047 - val_loss: 0.4257 - val_acc: 0.6675\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2573 - acc: 0.7046 - val_loss: 0.4248 - val_acc: 0.6651\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2573 - acc: 0.7046 - val_loss: 0.4439 - val_acc: 0.6583\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2603 - acc: 0.7036 - val_loss: 0.4341 - val_acc: 0.6649\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2590 - acc: 0.7040 - val_loss: 0.4379 - val_acc: 0.6608\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 140us/sample - loss: 0.2561 - acc: 0.7048 - val_loss: 0.4267 - val_acc: 0.6670\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2563 - acc: 0.7048 - val_loss: 0.4249 - val_acc: 0.6676\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2566 - acc: 0.7046 - val_loss: 0.4279 - val_acc: 0.6693\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2561 - acc: 0.7048 - val_loss: 0.4319 - val_acc: 0.6633\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2563 - acc: 0.7047 - val_loss: 0.4266 - val_acc: 0.6691\n",
      "17921/17921 [==============================] - 1s 82us/sample - loss: 0.5115 - acc: 0.6891\n",
      "35843/35843 [==============================] - 3s 81us/sample - loss: 0.2557 - acc: 0.7053\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 23s 636us/sample - loss: 0.3129 - acc: 0.6987 - val_loss: 0.4564 - val_acc: 0.6625\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2316 - acc: 0.7282 - val_loss: 0.4518 - val_acc: 0.6639\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2266 - acc: 0.7294 - val_loss: 0.4381 - val_acc: 0.6678\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2248 - acc: 0.7299 - val_loss: 0.4755 - val_acc: 0.6674\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2276 - acc: 0.7291 - val_loss: 0.4488 - val_acc: 0.6688\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2243 - acc: 0.7299 - val_loss: 0.4498 - val_acc: 0.6621\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2277 - acc: 0.7290 - val_loss: 0.4558 - val_acc: 0.6709\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2233 - acc: 0.7302 - val_loss: 0.4504 - val_acc: 0.6669\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2239 - acc: 0.7299 - val_loss: 0.4513 - val_acc: 0.6609\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2248 - acc: 0.7298 - val_loss: 0.4550 - val_acc: 0.6581\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2243 - acc: 0.7296 - val_loss: 0.4671 - val_acc: 0.6702\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 139us/sample - loss: 0.2250 - acc: 0.7295 - val_loss: 0.4498 - val_acc: 0.6589\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2247 - acc: 0.7296 - val_loss: 0.4659 - val_acc: 0.6693\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2209 - acc: 0.7307 - val_loss: 0.4620 - val_acc: 0.6682\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2207 - acc: 0.7307 - val_loss: 0.4880 - val_acc: 0.6690\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2260 - acc: 0.7293 - val_loss: 0.4501 - val_acc: 0.6630\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2236 - acc: 0.7299 - val_loss: 0.4730 - val_acc: 0.6694\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2229 - acc: 0.7302 - val_loss: 0.4693 - val_acc: 0.6697\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2224 - acc: 0.7303 - val_loss: 0.4714 - val_acc: 0.6699\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2208 - acc: 0.7307 - val_loss: 0.4660 - val_acc: 0.6676\n",
      "17922/17922 [==============================] - 1s 81us/sample - loss: 0.7582 - acc: 0.6261\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.2215 - acc: 0.7298\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 635us/sample - loss: 0.3154 - acc: 0.7091 - val_loss: 0.4537 - val_acc: 0.6676\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2370 - acc: 0.7366 - val_loss: 0.4630 - val_acc: 0.6718\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2332 - acc: 0.7373 - val_loss: 0.4351 - val_acc: 0.6719\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2320 - acc: 0.7375 - val_loss: 0.4235 - val_acc: 0.6660\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2309 - acc: 0.7374 - val_loss: 0.4328 - val_acc: 0.6738\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2300 - acc: 0.7379 - val_loss: 0.4208 - val_acc: 0.6751\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2271 - acc: 0.7384 - val_loss: 0.4267 - val_acc: 0.6773\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 141us/sample - loss: 0.2262 - acc: 0.7386 - val_loss: 0.4369 - val_acc: 0.6771\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 141us/sample - loss: 0.2271 - acc: 0.7384 - val_loss: 0.4341 - val_acc: 0.6712\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 138us/sample - loss: 0.2288 - acc: 0.7379 - val_loss: 0.4225 - val_acc: 0.6750\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2258 - acc: 0.7386 - val_loss: 0.4292 - val_acc: 0.6779\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2256 - acc: 0.7386 - val_loss: 0.4397 - val_acc: 0.6770\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2267 - acc: 0.7385 - val_loss: 0.4354 - val_acc: 0.6731\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2257 - acc: 0.7385 - val_loss: 0.4208 - val_acc: 0.6743\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2256 - acc: 0.7386 - val_loss: 0.4265 - val_acc: 0.6738\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2255 - acc: 0.7385 - val_loss: 0.4332 - val_acc: 0.6760\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2250 - acc: 0.7386 - val_loss: 0.4396 - val_acc: 0.6782\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2247 - acc: 0.7388 - val_loss: 0.4272 - val_acc: 0.6766\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2241 - acc: 0.7389 - val_loss: 0.4295 - val_acc: 0.6738\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2260 - acc: 0.7383 - val_loss: 0.4361 - val_acc: 0.6768\n",
      "17921/17921 [==============================] - 1s 83us/sample - loss: 0.6575 - acc: 0.6296\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.2232 - acc: 0.7398\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 630us/sample - loss: 0.3456 - acc: 0.6726 - val_loss: 0.4225 - val_acc: 0.6604\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2653 - acc: 0.7030 - val_loss: 0.4212 - val_acc: 0.6721\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2628 - acc: 0.7036 - val_loss: 0.4209 - val_acc: 0.6647\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2638 - acc: 0.7032 - val_loss: 0.4278 - val_acc: 0.6646\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2606 - acc: 0.7041 - val_loss: 0.4133 - val_acc: 0.6686\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2596 - acc: 0.7043 - val_loss: 0.4129 - val_acc: 0.6732\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2601 - acc: 0.7043 - val_loss: 0.4105 - val_acc: 0.6698\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2589 - acc: 0.7046 - val_loss: 0.4341 - val_acc: 0.6597\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2588 - acc: 0.7044 - val_loss: 0.4233 - val_acc: 0.6710\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2593 - acc: 0.7042 - val_loss: 0.4181 - val_acc: 0.6671\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2595 - acc: 0.7043 - val_loss: 0.4302 - val_acc: 0.6607\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2595 - acc: 0.7042 - val_loss: 0.4323 - val_acc: 0.6597\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2580 - acc: 0.7045 - val_loss: 0.4226 - val_acc: 0.6676\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2562 - acc: 0.7050 - val_loss: 0.4276 - val_acc: 0.6645\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 141us/sample - loss: 0.2563 - acc: 0.7050 - val_loss: 0.4354 - val_acc: 0.6632\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2595 - acc: 0.7041 - val_loss: 0.4342 - val_acc: 0.6551\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2584 - acc: 0.7044 - val_loss: 0.4252 - val_acc: 0.6622\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2565 - acc: 0.7048 - val_loss: 0.4307 - val_acc: 0.6690\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2563 - acc: 0.7049 - val_loss: 0.4229 - val_acc: 0.6716\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2567 - acc: 0.7048 - val_loss: 0.4357 - val_acc: 0.6680\n",
      "17921/17921 [==============================] - 1s 81us/sample - loss: 0.5251 - acc: 0.6907\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.2593 - acc: 0.7046\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 23s 633us/sample - loss: 0.2778 - acc: 0.7117 - val_loss: 0.4580 - val_acc: 0.6655\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2307 - acc: 0.7285 - val_loss: 0.4560 - val_acc: 0.6647\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2267 - acc: 0.7294 - val_loss: 0.4527 - val_acc: 0.6644\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2249 - acc: 0.7297 - val_loss: 0.4563 - val_acc: 0.6680\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2246 - acc: 0.7297 - val_loss: 0.4646 - val_acc: 0.6624\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2262 - acc: 0.7292 - val_loss: 0.4575 - val_acc: 0.6603\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 138us/sample - loss: 0.2250 - acc: 0.7295 - val_loss: 0.4677 - val_acc: 0.6678\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 141us/sample - loss: 0.2229 - acc: 0.7300 - val_loss: 0.4711 - val_acc: 0.6683\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2223 - acc: 0.7301 - val_loss: 0.4611 - val_acc: 0.6684\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2242 - acc: 0.7297 - val_loss: 0.4537 - val_acc: 0.6679\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 139us/sample - loss: 0.2222 - acc: 0.7301 - val_loss: 0.4557 - val_acc: 0.6698\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2211 - acc: 0.7305 - val_loss: 0.4583 - val_acc: 0.6696\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2219 - acc: 0.7302 - val_loss: 0.4673 - val_acc: 0.6677\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 147us/sample - loss: 0.2204 - acc: 0.7306 - val_loss: 0.4691 - val_acc: 0.6594\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2231 - acc: 0.7298 - val_loss: 0.4604 - val_acc: 0.6634\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2211 - acc: 0.7304 - val_loss: 0.4675 - val_acc: 0.6636\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2222 - acc: 0.7300 - val_loss: 0.4580 - val_acc: 0.6684\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2204 - acc: 0.7305 - val_loss: 0.4623 - val_acc: 0.6685\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2208 - acc: 0.7304 - val_loss: 0.4604 - val_acc: 0.6690\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2220 - acc: 0.7301 - val_loss: 0.4651 - val_acc: 0.6673\n",
      "17922/17922 [==============================] - 1s 82us/sample - loss: 0.7695 - acc: 0.6183\n",
      "35842/35842 [==============================] - 3s 82us/sample - loss: 0.2199 - acc: 0.7302\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 631us/sample - loss: 0.2797 - acc: 0.7219 - val_loss: 0.4480 - val_acc: 0.6751\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2337 - acc: 0.7371 - val_loss: 0.4471 - val_acc: 0.6735\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2301 - acc: 0.7378 - val_loss: 0.4353 - val_acc: 0.6617\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2318 - acc: 0.7371 - val_loss: 0.4414 - val_acc: 0.6760\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2280 - acc: 0.7380 - val_loss: 0.4312 - val_acc: 0.6766\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2277 - acc: 0.7380 - val_loss: 0.4324 - val_acc: 0.6754\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2265 - acc: 0.7383 - val_loss: 0.4378 - val_acc: 0.6773\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2279 - acc: 0.7379 - val_loss: 0.4253 - val_acc: 0.6764\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2252 - acc: 0.7385 - val_loss: 0.4301 - val_acc: 0.6719\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2288 - acc: 0.7375 - val_loss: 0.4343 - val_acc: 0.6744\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 141us/sample - loss: 0.2258 - acc: 0.7384 - val_loss: 0.4345 - val_acc: 0.6753\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2257 - acc: 0.7383 - val_loss: 0.4343 - val_acc: 0.6696\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2292 - acc: 0.7376 - val_loss: 0.4465 - val_acc: 0.6762\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 143us/sample - loss: 0.2304 - acc: 0.7374 - val_loss: 0.4432 - val_acc: 0.6720\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2252 - acc: 0.7385 - val_loss: 0.4419 - val_acc: 0.6671\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2270 - acc: 0.7379 - val_loss: 0.4240 - val_acc: 0.6773\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2239 - acc: 0.7387 - val_loss: 0.4327 - val_acc: 0.6784\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2249 - acc: 0.7385 - val_loss: 0.4293 - val_acc: 0.6781\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2238 - acc: 0.7388 - val_loss: 0.4319 - val_acc: 0.6764\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2234 - acc: 0.7389 - val_loss: 0.4332 - val_acc: 0.6732\n",
      "17921/17921 [==============================] - 1s 81us/sample - loss: 0.6459 - acc: 0.6283\n",
      "35843/35843 [==============================] - 3s 80us/sample - loss: 0.2270 - acc: 0.7367\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 649us/sample - loss: 0.3105 - acc: 0.6866 - val_loss: 0.4250 - val_acc: 0.6602\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2648 - acc: 0.7030 - val_loss: 0.4383 - val_acc: 0.6721\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2625 - acc: 0.7034 - val_loss: 0.4221 - val_acc: 0.6611\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 139us/sample - loss: 0.2609 - acc: 0.7037 - val_loss: 0.4277 - val_acc: 0.6654\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 142us/sample - loss: 0.2599 - acc: 0.7039 - val_loss: 0.4233 - val_acc: 0.6711\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2594 - acc: 0.7041 - val_loss: 0.4230 - val_acc: 0.6665\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2590 - acc: 0.7041 - val_loss: 0.4212 - val_acc: 0.6655\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2592 - acc: 0.7040 - val_loss: 0.4366 - val_acc: 0.6547\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2593 - acc: 0.7040 - val_loss: 0.4294 - val_acc: 0.6587\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2595 - acc: 0.7039 - val_loss: 0.4314 - val_acc: 0.6716\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2583 - acc: 0.7042 - val_loss: 0.4305 - val_acc: 0.6643\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2570 - acc: 0.7045 - val_loss: 0.4264 - val_acc: 0.6656\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2564 - acc: 0.7047 - val_loss: 0.4269 - val_acc: 0.6662\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2576 - acc: 0.7044 - val_loss: 0.4249 - val_acc: 0.6645\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 137us/sample - loss: 0.2569 - acc: 0.7045 - val_loss: 0.4228 - val_acc: 0.6658\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2567 - acc: 0.7046 - val_loss: 0.4276 - val_acc: 0.6699\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2571 - acc: 0.7045 - val_loss: 0.4244 - val_acc: 0.6662\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2560 - acc: 0.7047 - val_loss: 0.4299 - val_acc: 0.6634\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2563 - acc: 0.7047 - val_loss: 0.4270 - val_acc: 0.6640\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2553 - acc: 0.7048 - val_loss: 0.4222 - val_acc: 0.6700\n",
      "17921/17921 [==============================] - 2s 84us/sample - loss: 0.5077 - acc: 0.6911\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.2560 - acc: 0.7059\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 23s 640us/sample - loss: 0.2922 - acc: 0.7060 - val_loss: 0.4503 - val_acc: 0.6733\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 129us/sample - loss: 0.2294 - acc: 0.7289 - val_loss: 0.4507 - val_acc: 0.6715\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2271 - acc: 0.7293 - val_loss: 0.4497 - val_acc: 0.6684\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 137us/sample - loss: 0.2276 - acc: 0.7290 - val_loss: 0.4409 - val_acc: 0.6662\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 136us/sample - loss: 0.2272 - acc: 0.7290 - val_loss: 0.4670 - val_acc: 0.6716\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2238 - acc: 0.7299 - val_loss: 0.4626 - val_acc: 0.6576\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2307 - acc: 0.7283 - val_loss: 0.4558 - val_acc: 0.6666\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2230 - acc: 0.7302 - val_loss: 0.4565 - val_acc: 0.6622\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2251 - acc: 0.7296 - val_loss: 0.4623 - val_acc: 0.6681\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2221 - acc: 0.7304 - val_loss: 0.4466 - val_acc: 0.6620\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2245 - acc: 0.7298 - val_loss: 0.4651 - val_acc: 0.6697\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 130us/sample - loss: 0.2220 - acc: 0.7303 - val_loss: 0.4872 - val_acc: 0.6696\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2291 - acc: 0.7285 - val_loss: 0.4632 - val_acc: 0.6697\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2232 - acc: 0.7300 - val_loss: 0.4728 - val_acc: 0.6542\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2250 - acc: 0.7293 - val_loss: 0.4564 - val_acc: 0.6651\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2267 - acc: 0.7290 - val_loss: 0.4667 - val_acc: 0.6651\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2213 - acc: 0.7304 - val_loss: 0.4580 - val_acc: 0.6687\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2203 - acc: 0.7306 - val_loss: 0.4601 - val_acc: 0.6679\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2204 - acc: 0.7306 - val_loss: 0.4656 - val_acc: 0.6694\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2203 - acc: 0.7306 - val_loss: 0.4734 - val_acc: 0.6686\n",
      "17922/17922 [==============================] - 1s 80us/sample - loss: 0.7882 - acc: 0.6229\n",
      "35842/35842 [==============================] - 3s 79us/sample - loss: 0.2236 - acc: 0.7298\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 642us/sample - loss: 0.2948 - acc: 0.7161 - val_loss: 0.4426 - val_acc: 0.6714\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2335 - acc: 0.7373 - val_loss: 0.4435 - val_acc: 0.6781\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2313 - acc: 0.7374 - val_loss: 0.4231 - val_acc: 0.6728\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2288 - acc: 0.7379 - val_loss: 0.4307 - val_acc: 0.6764\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2274 - acc: 0.7383 - val_loss: 0.4290 - val_acc: 0.6724\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2283 - acc: 0.7380 - val_loss: 0.4265 - val_acc: 0.6763\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2272 - acc: 0.7382 - val_loss: 0.4397 - val_acc: 0.6780\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2268 - acc: 0.7383 - val_loss: 0.4236 - val_acc: 0.6755\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2284 - acc: 0.7378 - val_loss: 0.4630 - val_acc: 0.6719\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2283 - acc: 0.7381 - val_loss: 0.4379 - val_acc: 0.6785\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2252 - acc: 0.7386 - val_loss: 0.4235 - val_acc: 0.6723\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2262 - acc: 0.7383 - val_loss: 0.4298 - val_acc: 0.6749\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2245 - acc: 0.7386 - val_loss: 0.4400 - val_acc: 0.6767\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2251 - acc: 0.7386 - val_loss: 0.4444 - val_acc: 0.6759\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2248 - acc: 0.7387 - val_loss: 0.4398 - val_acc: 0.6774\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2247 - acc: 0.7387 - val_loss: 0.4477 - val_acc: 0.6760\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2260 - acc: 0.7382 - val_loss: 0.4250 - val_acc: 0.6774\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2243 - acc: 0.7388 - val_loss: 0.4350 - val_acc: 0.6740\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2245 - acc: 0.7387 - val_loss: 0.4318 - val_acc: 0.6784\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 133us/sample - loss: 0.2240 - acc: 0.7387 - val_loss: 0.4353 - val_acc: 0.6673\n",
      "17921/17921 [==============================] - 1s 79us/sample - loss: 0.6460 - acc: 0.6194\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.2346 - acc: 0.7324\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 649us/sample - loss: 0.3250 - acc: 0.6806 - val_loss: 0.4319 - val_acc: 0.6674\n",
      "Epoch 2/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2652 - acc: 0.7029 - val_loss: 0.4338 - val_acc: 0.6738\n",
      "Epoch 3/20\n",
      "35843/35843 [==============================] - 5s 146us/sample - loss: 0.2632 - acc: 0.7034 - val_loss: 0.4172 - val_acc: 0.6617\n",
      "Epoch 4/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2605 - acc: 0.7040 - val_loss: 0.4224 - val_acc: 0.6576\n",
      "Epoch 5/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2607 - acc: 0.7040 - val_loss: 0.4277 - val_acc: 0.6610\n",
      "Epoch 6/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2597 - acc: 0.7040 - val_loss: 0.4258 - val_acc: 0.6657\n",
      "Epoch 7/20\n",
      "35843/35843 [==============================] - 5s 143us/sample - loss: 0.2597 - acc: 0.7041 - val_loss: 0.4270 - val_acc: 0.6685\n",
      "Epoch 8/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2586 - acc: 0.7044 - val_loss: 0.4240 - val_acc: 0.6631\n",
      "Epoch 9/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2591 - acc: 0.7041 - val_loss: 0.4220 - val_acc: 0.6700\n",
      "Epoch 10/20\n",
      "35843/35843 [==============================] - 5s 131us/sample - loss: 0.2570 - acc: 0.7047 - val_loss: 0.4290 - val_acc: 0.6583\n",
      "Epoch 11/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2576 - acc: 0.7044 - val_loss: 0.4270 - val_acc: 0.6633\n",
      "Epoch 12/20\n",
      "35843/35843 [==============================] - 5s 136us/sample - loss: 0.2576 - acc: 0.7045 - val_loss: 0.4211 - val_acc: 0.6683\n",
      "Epoch 13/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2567 - acc: 0.7046 - val_loss: 0.4240 - val_acc: 0.6622\n",
      "Epoch 14/20\n",
      "35843/35843 [==============================] - 5s 140us/sample - loss: 0.2568 - acc: 0.7046 - val_loss: 0.4259 - val_acc: 0.6607\n",
      "Epoch 15/20\n",
      "35843/35843 [==============================] - 5s 134us/sample - loss: 0.2566 - acc: 0.7047 - val_loss: 0.4418 - val_acc: 0.6518\n",
      "Epoch 16/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2577 - acc: 0.7044 - val_loss: 0.4239 - val_acc: 0.6661\n",
      "Epoch 17/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2568 - acc: 0.7046 - val_loss: 0.4241 - val_acc: 0.6644\n",
      "Epoch 18/20\n",
      "35843/35843 [==============================] - 5s 135us/sample - loss: 0.2570 - acc: 0.7046 - val_loss: 0.4319 - val_acc: 0.6681\n",
      "Epoch 19/20\n",
      "35843/35843 [==============================] - 5s 132us/sample - loss: 0.2574 - acc: 0.7045 - val_loss: 0.4207 - val_acc: 0.6702\n",
      "Epoch 20/20\n",
      "35843/35843 [==============================] - 5s 141us/sample - loss: 0.2555 - acc: 0.7051 - val_loss: 0.4174 - val_acc: 0.6718\n",
      "17921/17921 [==============================] - 1s 81us/sample - loss: 0.5052 - acc: 0.6913\n",
      "35843/35843 [==============================] - 3s 82us/sample - loss: 0.2569 - acc: 0.7057\n",
      "Train on 35842 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35842/35842 [==============================] - 23s 632us/sample - loss: 0.3132 - acc: 0.6996 - val_loss: 0.4585 - val_acc: 0.6683\n",
      "Epoch 2/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2290 - acc: 0.7291 - val_loss: 0.4608 - val_acc: 0.6619\n",
      "Epoch 3/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2271 - acc: 0.7294 - val_loss: 0.4431 - val_acc: 0.6689\n",
      "Epoch 4/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2251 - acc: 0.7298 - val_loss: 0.4554 - val_acc: 0.6664\n",
      "Epoch 5/20\n",
      "35842/35842 [==============================] - 5s 134us/sample - loss: 0.2267 - acc: 0.7294 - val_loss: 0.4422 - val_acc: 0.6662\n",
      "Epoch 6/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2235 - acc: 0.7302 - val_loss: 0.4600 - val_acc: 0.6691\n",
      "Epoch 7/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2261 - acc: 0.7295 - val_loss: 0.4655 - val_acc: 0.6689\n",
      "Epoch 8/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2330 - acc: 0.7270 - val_loss: 0.4767 - val_acc: 0.6669\n",
      "Epoch 9/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2241 - acc: 0.7299 - val_loss: 0.4587 - val_acc: 0.6655\n",
      "Epoch 10/20\n",
      "35842/35842 [==============================] - 5s 133us/sample - loss: 0.2235 - acc: 0.7301 - val_loss: 0.4503 - val_acc: 0.6682\n",
      "Epoch 11/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2218 - acc: 0.7305 - val_loss: 0.4662 - val_acc: 0.6700\n",
      "Epoch 12/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2215 - acc: 0.7306 - val_loss: 0.4617 - val_acc: 0.6620\n",
      "Epoch 13/20\n",
      "35842/35842 [==============================] - 5s 131us/sample - loss: 0.2232 - acc: 0.7301 - val_loss: 0.4624 - val_acc: 0.6681\n",
      "Epoch 14/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2216 - acc: 0.7306 - val_loss: 0.4567 - val_acc: 0.6570\n",
      "Epoch 15/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2249 - acc: 0.7296 - val_loss: 0.4576 - val_acc: 0.6667\n",
      "Epoch 16/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2212 - acc: 0.7306 - val_loss: 0.4648 - val_acc: 0.6685\n",
      "Epoch 17/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2217 - acc: 0.7305 - val_loss: 0.4623 - val_acc: 0.6684\n",
      "Epoch 18/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2202 - acc: 0.7309 - val_loss: 0.4633 - val_acc: 0.6676\n",
      "Epoch 19/20\n",
      "35842/35842 [==============================] - 5s 135us/sample - loss: 0.2208 - acc: 0.7306 - val_loss: 0.4687 - val_acc: 0.6667\n",
      "Epoch 20/20\n",
      "35842/35842 [==============================] - 5s 132us/sample - loss: 0.2212 - acc: 0.7305 - val_loss: 0.4644 - val_acc: 0.6700\n",
      "17922/17922 [==============================] - 1s 83us/sample - loss: 0.7599 - acc: 0.6303\n",
      "35842/35842 [==============================] - 3s 81us/sample - loss: 0.2230 - acc: 0.7298\n",
      "Train on 35843 samples, validate on 6308 samples\n",
      "Epoch 1/20\n",
      "35843/35843 [==============================] - 23s 649us/sample - loss: 0.3185 - acc: 0.7071 - val_loss: 0.4488 - val_acc: 0.6726\n"
     ]
    }
   ],
   "source": [
    "# GridSearch for best configuration.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def autoencoder_model(activation1, activation2, activation3, optimizer, unit1, unit2, unit3):\n",
    "    input_img = keras.Input(shape=(784,))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = layers.Dense(unit1, activation=activation1, activity_regularizer=regularizers.l1(10e-8))(input_img)\n",
    "    encoded = layers.Dense(unit2, activation=activation2, activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "    #encoded = layers.Dense(64, activation='relu',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "    encoded = layers.Dense(unit3, activation=activation3, activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "    # This model maps an input to its reconstruction\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "    autoencoder.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"acc\"])\n",
    "    return autoencoder\n",
    "\n",
    "data_shape = (X_test.shape[0], 784)\n",
    "\n",
    "model = KerasClassifier(build_fn = autoencoder_model, batch_size=256, epochs=20)\n",
    "#now write out all the parameters you want to try out for the grid search\n",
    "activation1 = ['relu', 'linear']\n",
    "activation2 = ['relu', 'linear']\n",
    "activation3 = ['relu', 'linear']\n",
    "\n",
    "#units = [784, 512, 384, 256, 128, 64, 32,]\n",
    "unit1 = [784, 512]\n",
    "unit2 = [384, 256]\n",
    "unit3 = [128, 64, 32]\n",
    "optimizer = ['Adam']\n",
    "param_grid = dict(activation1=activation1,activation2=activation2,activation3=activation3,\n",
    "                  optimizer=optimizer, unit1=unit1, unit2=unit2, unit3=unit3)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "result = grid.fit(train_X, train_ground, validation_data=(valid_X,valid_ground))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (result.best_score_, result.best_params_))\n",
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named talos",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-882d1e23de14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtalos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named talos"
     ]
    }
   ],
   "source": [
    "# GridSearch for best configuration.\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import talos\n",
    "\n",
    "\n",
    "def autoencoder_model(params):\n",
    "    input_img = keras.Input(shape=(784,))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = layers.Dense(params['unit1'], activation=params['activation'], activity_regularizer=regularizers.l1(10e-8))(input_img)\n",
    "    encoded = layers.Dense(params['unit2'], activation=params['activation'], activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "    encoded = layers.Dense(params['unit3'], activation=params['activation'], activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "    # This model maps an input to its reconstruction\n",
    "    autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "    autoencoder.compile(optimizer=params['optimizer'], loss='binary_crossentropy', metrics=[\"loss\"])\n",
    "    out = autoencoder.fit(train_X, train_ground,\n",
    "                batch_size=params['batch_size'],\n",
    "                epochs=params['epochs'],\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X,valid_ground))\n",
    "    return out, autoencoder\n",
    "\n",
    "p = {\n",
    "    'activation':['relu', 'linear'],\n",
    "    'optimizer': ['Nadam', 'Adam'],\n",
    "    'losses': ['binary_crossentropy'],\n",
    "    'batch_size': [128,256,512],\n",
    "    'epochs': [10,20],\n",
    "    'unit1':[784, 512],\n",
    "    'unit2': [384, 256],\n",
    "    'unit3': [128, 64],\n",
    "    }\n",
    "\n",
    "p = {\n",
    "    'activation':['relu', 'linear'],\n",
    "    'optimizer': ['Adam'],\n",
    "    'losses': ['binary_crossentropy'],\n",
    "    'batch_size': [512],\n",
    "    'epochs': [10],\n",
    "    'unit1':[784,],\n",
    "    'unit2': [384,],\n",
    "    'unit3': [128],\n",
    "    }\n",
    "\n",
    "scan_object = talos.Scan(x, y, model=autoencoder_model, params=p, experiment_name='autoencode', fraction_limit=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Collecting tensorflow==1.13.2\n",
      "  Using cached tensorflow-1.13.2-cp27-cp27mu-manylinux1_x86_64.whl (92.6 MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.16.6)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (0.13.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (3.14.0)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Using cached tensorboard-1.13.1-py2-none-any.whl (3.2 MB)\n",
      "Requirement already satisfied: enum34>=1.1.6 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.1.10)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.0.8)\n",
      "Requirement already satisfied: backports.weakref>=1.0rc1 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.0.post1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.16.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (3.0.5)\n",
      "Requirement already satisfied: wheel in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (0.36.2)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Using cached tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorflow==1.13.2) (1.39.0)\n",
      "Requirement already satisfied: futures>=3.1.1; python_version < \"3\" in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.1)\n",
      "Requirement already satisfied: h5py in /home/peng/cooperating/venv/lib/python2.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
      "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /home/peng/cooperating/venv/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.13.2) (1.0.2)\n",
      "Requirement already satisfied: setuptools>=36 in /home/peng/cooperating/venv/lib/python2.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (44.1.1)\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# measure performance data, all data go into exit 0\n",
    "autoencoder.save('_models/autoencoder/autoencoder_kmnist.h5')\n",
    "model1 = models.load_model('_models/autoencoder/autoencoder_kmnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU used in 1 seconds: 15.2\n",
      "('accuracy is ', array([0.9154]))\n",
      "('branchyNet time is ', array([7.906358]))\n",
      "('the distribution of exit number is ', [array([9127,  873])])\n",
      "CPU used in 23 seconds: 98.3\n",
      "\n",
      "\n",
      "('total time(s) is ', array([9.0730629]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 27.6\n",
      "('accuracy is ', array([0.91469999]))\n",
      "('branchyNet time is ', array([3.45277166]))\n",
      "('the distribution of exit number is ', [array([10000,     0])])\n",
      "CPU used in 18 seconds: 98.4\n",
      "\n",
      "\n",
      "('total time(s) is ', array([4.15174675]))\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.91469999]), array([4.15174675]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_perf_and_time(X_test, Y_test, data_shape)\n",
    "\n",
    "# measure performance data,all data go into different exits\n",
    "measure_perf_and_time(X_test, Y_test, data_shape, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 760x380 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 760x380 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 760x380 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 760x380 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAFcCAYAAAAqFm1cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAA6cAAAOnAB65f4yQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi41LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvSM8oowAADd9JREFUeJzt3U+IVXUfx/FzZxxT/BMWUTM4VBCDUEJBj1HQtsVAi2oRpBQkCC4KWrSorfugTYtoaUEQrYIWZRkkEbqYCKJ/kA9mQTNq6NSYdrnPIij695yf3nvmnPu5r9dSvvzu16te3x7G+fUGg0EFAACpptpeAAAAmiR4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiLah7QX+n16vd01VVbv/8sPLVVX1W1gH6Kbpqqpu+MuPfToYDH5pY5kEPnuBAmP12dvp4K1++8A93vYSwNj5T1VVJ9peYoz57AWuRmc/e31JAwAA0QQvAADRBC8AANG6/jW8y20vAFfqkUceqZ357rvvis766KOPhl1nUvnsGI737yqdOnWq7RU6YX5+vu0VaEdnPzu6Hrz+RzBjZ8uWLbUzmzZtWodNJprPjuF4/67Szp07214B2tTZzw5f0gAAQDTBCwBANMELAEA0wQsAQLSu/6c1GrR169aiudXV1YY3GQ8vvvhi0dyTTz5ZO7O2tlZ01l133VU7c/r06aKzgOb1er3amcFgsA6bXLnHHnusaO61116rnWnj51jy3jO5POEFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgWq+rN75UVVX1er2dVVWdanuPVLfddlvRXMmNbEtLS8Ou06pnnnmmdubMmTNFZ7355pu1MydPniw66/nnn6+defnll4vOmjDzg8Hg27aXGFc+e5vV1b93R3lTmZvWJlZnP3s94QUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAg2oa2F6A9X3/9ddHckSNHamcefPDBorN+/vnnorlRKf1G5E888UTtzJ133jnsOr/75ptviuYWFhZG9ppA8ybhUolRvl7J+7W4uDjsOuAJLwAA2QQvAADRBC8AANEELwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANHctEatc+fO1c5ce+21RWeV3LQ2PT1ddNbs7GztzOrqatFZx44dK5oblU8++aRo7vjx4w1vAtCe9b4BjsnlCS8AANEELwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANEELwAA0Vw8McFmZmaK5i5fvlw78/333w+7zu/uvffeornHH3+8dmbfvn1FZz333HNFc6NSevHEnj17amdef/31YdcBgGie8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABDNTWsT7IUXXiiae+WVVxre5M8+/PDDormbb765dmbv3r1FZ509e7ZoblSuu+66orktW7Y0vAlQYjAYtL0CHVP6e6LX6zW8CSU84QUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmosnQk1N1f9bZnFxseisp556ath1fldykcJPP/1UdNa7775bO7OyslJ01qVLl4rmRqX09ZaXlxveBHjppZfaXuGqudSgPd778eIJLwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANHctBbq6aefrp159dVX12GTP5uenh7ZWSU3spXcOFdVVfX+++8Pu84VmZ+fL5rbuHFjw5sABw8ebHsFoGGe8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQzcUTofbu3Vs7c+jQoZG93vbt24vmFhYWamdOnDhRdNbq6mrtzK5du4rOKrnEYpROnjxZNLdjx45mF4Fgg8Gg7RWG0uv12l4BYnjCCwBANMELAEA0wQsAQDTBCwBANMELAEA0wQsAQDTBCwBANMELAEA0wQsAQDQ3rY2ZmZmZormbbrqpdubtt98edp3f3X///UVz77333shes8R636BWanl5uWhu06ZNDW8CwNUovcnPjXnd4AkvAADRBC8AANEELwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANFcPDFmHnjggaK5o0eP1s5cvnx5yG3+cOnSpaK5tbW1kb3mOPv444+L5ubm5hreBCBD6UUQTCZPeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIjmprUxs7CwUDS3tLQ0stfcunVr7cyePXuKznrnnXeGXSfCHXfcUTQ3NeXfpDCpSm4O6/V667BJcybhdrRR/hzH/de7Tf42BQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCaiyfGzH333Vc098Ybb4zsNR9++OHamSNHjozs9SbBrbfeWjQ3Ozvb8CYwnibhwoISbVxqUHIhTr/fH3Yd/sEkXEbSFE94AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiOamtQ7ZsWNH7cxDDz1UdNZbb71VOzMzM1N01rPPPls7s7i4WHQWvzl//nzR3Pz8fMObAPymq7fXtXFzWFffixKlu0/ajWye8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQzcUTHVJyycD09HTRWT/++GPtzMGDB4vOWlhYGMnr8YeLFy8WzZX+ekOKcf6G/1W1/t/M3/vVjFHuNe6/Rik84QUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCam9Y6ZOPGjSM7a/fu3bUzjz76aNFZ+/fvr525cOFC0Vn8pvRmujNnzjS8CVDCjWBXpqvvVxtK3gu3sTXPE14AAKIJXgAAogleAACiCV4AAKIJXgAAogleAACiCV4AAKIJXgAAorl4okM+//zz2pnPPvus6KwDBw7UznzwwQdFZx0+fLhojtFbWVlpewWIt23btrZX6ASXRbSn3+8XzU1PT4/sNUsuu0j6PeEJLwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANEELwAA0QQvAADRBC8AANHctNYha2trtTPHjh0rOuv222+vnTl06FDRWYzehg1lf/TuueeehjeB9VNys1MbVldX216BCVf6d0JX/wyNA094AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIjm4okO6ff7tTMHDhxYh01oWuk3ur/xxhsb3gRy9Xq9tleAIqdPn257hXie8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABDNTWvQglOnThXNXX/99bUz27dvLzrr/PnzRXNwpQaDQdsrwFibm5tre4V4nvACABBN8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABBN8AIAEM3FE9CCCxcuFM199dVXtTNTU/7dyuT54osv2l4BGCP+pgQAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCam9agBZs3by6au/vuu2tnbrnllqKzlpaWiuZgHOzatavtFWCs9Xq9tldYV57wAgAQTfACABBN8AIAEE3wAgAQTfACABBN8AIAEE3wAgAQTfACABDNxRPQgh9++KFo7ty5c7Uz+/btKzrLxROMg0n7ZvjA+vCEFwCAaIIXAIBoghcAgGiCFwCAaIIXAIBoghcAgGiCFwCAaIIXAIBoghcAgGhuWoMWnD17tmjuyy+/rJ05fPjwsOvAUNyOBv9uMBis6+v58/jPPOEFACCa4AUAIJrgBQAgmuAFACCa4AUAIJrgBQAgmuAFACCa4AUAIJqLJ6AFpd8YfG5urnZmdna26KylpaWiOQDqTU2t/zNDl0pcPU94AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiCZ4AQCIJngBAIgmeAEAiOamNWjB5s2bi+Z+/fXX2pmjR48OuQ0AV6rf74/sLDeoNc8TXgAAogleAACiCV4AAKIJXgAAogleAACiCV4AAKIJXgAAogleAACiuXgCWrBt27aiuf3799fOrK2tDbsOAA1xqUQ3eMILAEA0wQsAQDTBCwBANMELAEA0wQsAQDTBCwBANMELAEC0rn8f3um2F4Am9Pv9ormLFy82vEksnx3D8f5BjW+//bbtFbqos58dXQ/eG9peAJqwsrIy0jn+5oaqqv7b9hJjzGcv1Jifn297hS7q7GevL2kAACCa4AUAIJrgBQAgWm8wGLS9w7/q9XrXVFW1+y8/vFxVVdn/+AEmwXT19685/XQwGPzSxjIJfPYCBcbqs7fTwQsAAMPyJQ0AAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAEQTvAAARBO8AABEE7wAAET7H4yo1u/qazzHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 760x380 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 760x380 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_test_data_x = X_test.reshape(data_shape)\n",
    "decoded_imgs = autoencoder.predict(b_test_data_x)\n",
    "for i in range(10):\n",
    "    if y_test_data[i] == 1:\n",
    "        \n",
    "        plt.figure(figsize=(2, 1), dpi=380)\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        plt.imshow(X_test[i].reshape(28, 28))\n",
    "\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)     \n",
    "        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(512, activation='relu',activity_regularizer=regularizers.l1(10e-8))(input_img)\n",
    "encoded = layers.Dense(384, activation='relu',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "encoded = layers.Dense(256, activation='relu',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "encoded = layers.Dense(64, activation='linear',activity_regularizer=regularizers.l1(10e-8))(encoded)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(train_X, train_ground,\n",
    "                epochs=15,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X,valid_ground))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('**************current input number is ', 0)\n",
      "('accuracy is ', array([0.80299999]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.6042778491973877)\n",
      "('total time(s) is ', 1.738996982574463)\n",
      "CPU used in 1 seconds: 95.3\n",
      "('**************current input number is ', 1)\n",
      "('accuracy is ', array([0.626]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.4546191692352295)\n",
      "('total time(s) is ', 1.6026389598846436)\n",
      "CPU used in 1 seconds: 95.1\n",
      "('**************current input number is ', 2)\n",
      "('accuracy is ', array([0.759]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.4206349849700928)\n",
      "('total time(s) is ', 1.5498600006103516)\n",
      "CPU used in 1 seconds: 96.9\n",
      "('**************current input number is ', 3)\n",
      "('accuracy is ', array([0.90399999]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.509070873260498)\n",
      "('total time(s) is ', 1.6410198211669922)\n",
      "CPU used in 1 seconds: 95.4\n",
      "('**************current input number is ', 4)\n",
      "('accuracy is ', array([0.736]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.8589451313018799)\n",
      "('total time(s) is ', 1.9981579780578613)\n",
      "CPU used in 1 seconds: 95.4\n",
      "('**************current input number is ', 5)\n",
      "('accuracy is ', array([0.91999999]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.8635869026184082)\n",
      "('total time(s) is ', 1.9991309642791748)\n",
      "CPU used in 1 seconds: 96.9\n",
      "('**************current input number is ', 6)\n",
      "('accuracy is ', array([0.87599999]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.6478569507598877)\n",
      "('total time(s) is ', 1.7784819602966309)\n",
      "CPU used in 1 seconds: 96.1\n",
      "('**************current input number is ', 7)\n",
      "('accuracy is ', array([0.81899999]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.507930040359497)\n",
      "('total time(s) is ', 1.6377699375152588)\n",
      "CPU used in 1 seconds: 95.4\n",
      "('**************current input number is ', 8)\n",
      "('accuracy is ', array([0.89099999]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.9756710529327393)\n",
      "('total time(s) is ', 2.115756034851074)\n",
      "CPU used in 2 seconds: 97.2\n",
      "('**************current input number is ', 9)\n",
      "('accuracy is ', array([0.805]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "('branchynet running time(s) is ', 1.671555995941162)\n",
      "('total time(s) is ', 1.8031129837036133)\n",
      "CPU used in 1 seconds: 96.7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"**************current input number is \", i)\n",
    "    X_test_tmp = X_test[Y_test==i]\n",
    "    Y_test_tmp = Y_test[Y_test==i]\n",
    "\n",
    "        data_tmp_shape = (X_test_tmp.shape[0], 784)\n",
    "\n",
    "    measure_perf_and_time(X_test_tmp, Y_test_tmp, data_tmp_shape, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('accuracy is ', array([0.9226]))\n",
    "('the distribution of exit number is ', [array([7532, 2468])])\n",
    "('branchynet running time(s) i ', 28.145965099334717)\n",
    "('total time(s) is ', 29.104530811309814)\n",
    "CPU used in 29 seconds: 96.0\n",
    "('accuracy is ', array([0.92199999]))\n",
    "('the distribution of exit number is ', [array([10000,     0])])\n",
    "('branchynet running time(s) is ', 18.179352045059204)\n",
    "('total time(s) is ', 18.97394895553589)\n",
    "CPU used in 18 seconds: 95.2\n",
    "    \n",
    "('accuracy is ', array([0.92499999]))\n",
    "('the distribution of exit number is ', [array([10000,     0])])\n",
    "('branchynet running time(s) is ', 18.498135089874268)\n",
    "('total time(s) is ', 19.237569093704224)\n",
    "CPU used in 19 seconds: 95.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# the next experiment for convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU used in 0 seconds: 16.4\n",
      "('accuracy is ', array([0.92499999]))\n",
      "('branchyNet time is ', array([3.95138192]))\n",
      "('the distribution of exit number is ', [array([10000,     0])])\n",
      "CPU used in 19 seconds: 98.8\n",
      "\n",
      "\n",
      "('total time(s) is ', array([4.83758998]))\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92499999])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure performance data,all data go into different exits\n",
    "measure_perf_and_time(X_test, Y_test, (-1, 784), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"**************current input number is \", i)\n",
    "    X_test_tmp = X_test[Y_test==i]\n",
    "    Y_test_tmp = Y_test[Y_test==i]\n",
    "\n",
    "    data_tmp_shape = (X_test_tmp.shape[0], 28, 28, 1)\n",
    "\n",
    "    measure_perf_and_time(X_test_tmp, Y_test_tmp, data_tmp_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"**************current input number is \", i)\n",
    "    X_test_tmp = X_test[Y_test==i]\n",
    "    Y_test_tmp = Y_test[Y_test==i]\n",
    "\n",
    "    data_tmp_shape = (X_test_tmp.shape[0], 28, 28, 1)\n",
    "\n",
    "    measure_perf_and_time(X_test_tmp, Y_test_tmp, data_tmp_shape, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy is ', array([0.9205]))\n",
      "('branchyNet time is ', array([13.51258516]))\n",
      "('the distribution of exit number is ', [array([6308, 3692])])\n",
      "CPU used in 29 seconds: 98.8\n",
      "\n",
      "\n",
      "('accuracy is ', array([0.87389999]))\n",
      "('branchyNet time is ', array([3.50764632]))\n",
      "('the distribution of exit number is ', [array([10000,     0])])\n",
      "CPU used in 18 seconds: 98.8\n",
      "\n",
      "\n",
      "('LeNet accuracy is ', 0.9139999926686287)\n",
      "('LeNet time is ', 18.59014630317688)\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 21.1\n",
      "('accuracy is ', array([0.92499999]))\n",
      "('branchyNet time is ', array([3.58754134]))\n",
      "('the distribution of exit number is ', [array([10000,     0])])\n",
      "CPU used in 19 seconds: 98.7\n",
      "\n",
      "\n",
      "('total time(s) is ', array([4.36111736]))\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.92499999])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# measure branchyNet\n",
    "measure_performance_branchynet(X_test, Y_test)\n",
    "# measure BranchyNet with early exit\n",
    "measure_performance_branchynet(X_test, Y_test, 2)\n",
    "# measure LeNet\n",
    "measure_performance_LeNet(X_test, Y_test)\n",
    "# measure performance data,all data go into different exits\n",
    "measure_perf_and_time(X_test, Y_test, (-1, 784), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU used in 0 seconds: 21.8\n",
      "('accuracy is ', array([0.92099999]))\n",
      "('branchyNet time is ', array([0.41268969]))\n",
      "('the distribution of exit number is ', [array([1000,    0])])\n",
      "CPU used in 2 seconds: 98.8\n",
      "\n",
      "\n",
      "('total time(s) is ', array([0.52258873]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 56.9\n",
      "('accuracy is ', array([0.92099999]))\n",
      "('branchyNet time is ', array([0.81262112]))\n",
      "('the distribution of exit number is ', [array([2000,    0])])\n",
      "CPU used in 4 seconds: 99.2\n",
      "\n",
      "\n",
      "('total time(s) is ', array([0.98759127]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 46.2\n",
      "('accuracy is ', array([0.92666666]))\n",
      "('branchyNet time is ', array([1.16618729]))\n",
      "('the distribution of exit number is ', [array([3000,    0])])\n",
      "CPU used in 5 seconds: 99.0\n",
      "\n",
      "\n",
      "('total time(s) is ', array([1.42730117]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 38.3\n",
      "('accuracy is ', array([0.92449999]))\n",
      "('branchyNet time is ', array([1.4631114]))\n",
      "('the distribution of exit number is ', [array([4000,    0])])\n",
      "CPU used in 7 seconds: 98.8\n",
      "\n",
      "\n",
      "('total time(s) is ', array([1.78609347]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 31.6\n",
      "('accuracy is ', array([0.92539999]))\n",
      "('branchyNet time is ', array([1.82529211]))\n",
      "('the distribution of exit number is ', [array([5000,    0])])\n",
      "CPU used in 9 seconds: 99.0\n",
      "\n",
      "\n",
      "('total time(s) is ', array([2.22638297]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 25.9\n",
      "('accuracy is ', array([0.92449999]))\n",
      "('branchyNet time is ', array([2.12045383]))\n",
      "('the distribution of exit number is ', [array([6000,    0])])\n",
      "CPU used in 11 seconds: 98.7\n",
      "\n",
      "\n",
      "('total time(s) is ', array([2.61047363]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 26.1\n",
      "('accuracy is ', array([0.92657142]))\n",
      "('branchyNet time is ', array([2.77665901]))\n",
      "('the distribution of exit number is ', [array([7000,    0])])\n",
      "CPU used in 13 seconds: 98.0\n",
      "\n",
      "\n",
      "('total time(s) is ', array([3.37958097]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 16.5\n",
      "('accuracy is ', array([0.92599999]))\n",
      "('branchyNet time is ', array([2.85064268]))\n",
      "('the distribution of exit number is ', [array([8000,    0])])\n",
      "CPU used in 14 seconds: 98.7\n",
      "\n",
      "\n",
      "('total time(s) is ', array([3.4996717]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 18.6\n",
      "('accuracy is ', array([0.92533333]))\n",
      "('branchyNet time is ', array([3.18902111]))\n",
      "('the distribution of exit number is ', [array([9000,    0])])\n",
      "CPU used in 15 seconds: 98.6\n",
      "\n",
      "\n",
      "('total time(s) is ', array([3.9477911]))\n",
      "\n",
      "\n",
      "CPU used in 0 seconds: 21.9\n",
      "('accuracy is ', array([0.92499999]))\n",
      "('branchyNet time is ', array([3.56774664]))\n",
      "('the distribution of exit number is ', [array([10000,     0])])\n",
      "CPU used in 18 seconds: 98.6\n",
      "\n",
      "\n",
      "('total time(s) is ', array([4.33525562]))\n",
      "\n",
      "\n",
      "[0.9209999912977218, 0.920999992787838, 0.9266666597127915, 0.9244999925792218, 0.9253999927043914, 0.9244999930262565, 0.9265714218786785, 0.9259999930113554, 0.9253333262602488, 0.9249999929070473]\n",
      "[0.5225887298583984, 0.9875912666320801, 1.4273011684417725, 1.7860934734344482, 2.2263829708099365, 2.6104736328125, 3.3795809745788574, 3.499671697616577, 3.94779109954834, 4.3352556228637695]\n"
     ]
    }
   ],
   "source": [
    "# scale analysis, use part of training data \n",
    "    \n",
    "def scale_analysis(percentile=0.1):\n",
    "\n",
    "    X_test_part = []\n",
    "    Y_test_part = []\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        X_test_part.append(X_test[Y_test == i][0:int(1000*percentile), :])\n",
    "        Y_test_part.append(Y_test[Y_test == i][0:int(1000*percentile)])\n",
    "    \n",
    "    X_test_part = np.concatenate(X_test_part)\n",
    "    Y_test_part = np.concatenate(Y_test_part).reshape(-1, )\n",
    "\n",
    "    return measure_perf_and_time(X_test_part, Y_test_part, (-1, 784), 2)\n",
    "    \n",
    "acc = []\n",
    "run_time = []\n",
    "for i in range(1, 11, 1):\n",
    "    _acc, _time = scale_analysis(i/10.0)\n",
    "    acc.append(_acc)\n",
    "    run_time.append(_time)\n",
    "print(list(map(lambda x: x[0], acc)))\n",
    "print(list(map(lambda x: x[0], run_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy is ', array([0.922]))\n",
      "('branchyNet time is ', array([1.64907861]))\n",
      "('the distribution of exit number is ', [array([635, 365])])\n",
      "CPU used in 3 seconds: 99.2\n",
      "\n",
      "\n",
      "('running time(s) is ', array([1.64907861]))\n",
      "('accuracy is ', array([0.922]))\n",
      "('branchyNet time is ', array([2.51473546]))\n",
      "('the distribution of exit number is ', [array([1248,  752])])\n",
      "CPU used in 5 seconds: 98.3\n",
      "\n",
      "\n",
      "('running time(s) is ', array([2.51473546]))\n",
      "('accuracy is ', array([0.92066667]))\n",
      "('branchyNet time is ', array([4.63913298]))\n",
      "('the distribution of exit number is ', [array([1877, 1123])])\n",
      "CPU used in 9 seconds: 99.1\n",
      "\n",
      "\n",
      "('running time(s) is ', array([4.63913298]))\n",
      "('accuracy is ', array([0.921]))\n",
      "('branchyNet time is ', array([5.16554618]))\n",
      "('the distribution of exit number is ', [array([2524, 1476])])\n",
      "CPU used in 11 seconds: 98.7\n",
      "\n",
      "\n",
      "('running time(s) is ', array([5.16554618]))\n",
      "('accuracy is ', array([0.9204]))\n",
      "('branchyNet time is ', array([6.56226373]))\n",
      "('the distribution of exit number is ', [array([3165, 1835])])\n",
      "CPU used in 14 seconds: 98.7\n",
      "\n",
      "\n",
      "('running time(s) is ', array([6.56226373]))\n",
      "('accuracy is ', array([0.91966667]))\n",
      "('branchyNet time is ', array([8.05596066]))\n",
      "('the distribution of exit number is ', [array([3785, 2215])])\n",
      "CPU used in 17 seconds: 98.6\n",
      "\n",
      "\n",
      "('running time(s) is ', array([8.05596066]))\n",
      "('accuracy is ', array([0.921]))\n",
      "('branchyNet time is ', array([9.00857973]))\n",
      "('the distribution of exit number is ', [array([4403, 2597])])\n",
      "CPU used in 19 seconds: 98.7\n",
      "\n",
      "\n",
      "('running time(s) is ', array([9.00857973]))\n",
      "('accuracy is ', array([0.9205]))\n",
      "('branchyNet time is ', array([10.52942181]))\n",
      "('the distribution of exit number is ', [array([5032, 2968])])\n",
      "CPU used in 23 seconds: 98.8\n",
      "\n",
      "\n",
      "('running time(s) is ', array([10.52942181]))\n",
      "('accuracy is ', array([0.92066667]))\n",
      "('branchyNet time is ', array([13.09361982]))\n",
      "('the distribution of exit number is ', [array([5672, 3328])])\n",
      "CPU used in 28 seconds: 99.0\n",
      "\n",
      "\n",
      "('running time(s) is ', array([13.09361982]))\n",
      "('accuracy is ', array([0.9205]))\n",
      "('branchyNet time is ', array([13.90948701]))\n",
      "('the distribution of exit number is ', [array([6308, 3692])])\n",
      "CPU used in 30 seconds: 98.8\n",
      "\n",
      "\n",
      "('running time(s) is ', array([13.90948701]))\n",
      "[0.9220000015795231, 0.9220000016987324, 0.9206666682014862, 0.921000001501292, 0.9204000015944243, 0.9196666682586074, 0.9210000015646219, 0.9205000015776604, 0.9206666681170463, 0.9205000014811754]\n",
      "[1.649078607559204, 2.514735460281372, 4.639132976531982, 5.165546178817749, 6.5622637271881095, 8.055960655212402, 9.008579730987549, 10.52942180633545, 13.09361982345581, 13.909487009048462]\n"
     ]
    }
   ],
   "source": [
    "# scale analysis, use part of training data \n",
    "    \n",
    "def scale_analysis(percentile=0.1):\n",
    "\n",
    "    X_test_part = []\n",
    "    Y_test_part = []\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        X_test_part.append(X_test[Y_test == i][0:int(1000*percentile), :])\n",
    "        Y_test_part.append(Y_test[Y_test == i][0:int(1000*percentile)])\n",
    "    \n",
    "    X_test_part = np.concatenate(X_test_part)\n",
    "    Y_test_part = np.concatenate(Y_test_part).reshape(-1, )\n",
    "\n",
    "    acc, diff = measure_performance_branchynet(X_test_part, Y_test_part)\n",
    "    print(\"running time(s) is \", diff)\n",
    "    return acc,  diff\n",
    "\n",
    "acc = []\n",
    "run_time = []\n",
    "for i in range(1, 11, 1):\n",
    "    _acc, _time = scale_analysis(i/10.0)\n",
    "    acc.append(_acc)\n",
    "    run_time.append(_time)\n",
    "print(list(map(lambda x: x[0], acc)))\n",
    "print(list(map(lambda x: x[0], run_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model 5-9 for data \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine more data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
